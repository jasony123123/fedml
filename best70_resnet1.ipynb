{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "best70-resnet1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXKgF0bZSdfl",
        "outputId": "94025548-e77e-4228-82a4-560c4154626f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pytorch-cifar100'...\n",
            "remote: Enumerating objects: 1188, done.\u001b[K\n",
            "remote: Total 1188 (delta 0), reused 0 (delta 0), pack-reused 1188\u001b[K\n",
            "Receiving objects: 100% (1188/1188), 530.69 KiB | 4.91 MiB/s, done.\n",
            "Resolving deltas: 100% (753/753), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/weiaicunzai/pytorch-cifar100"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd pytorch-cifar100/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sN_U80xqSfcG",
        "outputId": "0260149e-9ad8-4d34-faab-15530d6b8eae"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pytorch-cifar100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py -net resnet18 -gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kz5BZ_bpSkpP",
        "outputId": "c4dae1b8-e933-41f6-a0e3-93a9dcf4af1e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Training Epoch: 61 [39680/50000]\tLoss: 0.4277\tLR: 0.020000\n",
            "Training Epoch: 61 [39808/50000]\tLoss: 0.5687\tLR: 0.020000\n",
            "Training Epoch: 61 [39936/50000]\tLoss: 0.7400\tLR: 0.020000\n",
            "Training Epoch: 61 [40064/50000]\tLoss: 0.3910\tLR: 0.020000\n",
            "Training Epoch: 61 [40192/50000]\tLoss: 0.5050\tLR: 0.020000\n",
            "Training Epoch: 61 [40320/50000]\tLoss: 0.6201\tLR: 0.020000\n",
            "Training Epoch: 61 [40448/50000]\tLoss: 0.5635\tLR: 0.020000\n",
            "Training Epoch: 61 [40576/50000]\tLoss: 0.5070\tLR: 0.020000\n",
            "Training Epoch: 61 [40704/50000]\tLoss: 0.6643\tLR: 0.020000\n",
            "Training Epoch: 61 [40832/50000]\tLoss: 0.6136\tLR: 0.020000\n",
            "Training Epoch: 61 [40960/50000]\tLoss: 0.4980\tLR: 0.020000\n",
            "Training Epoch: 61 [41088/50000]\tLoss: 0.6417\tLR: 0.020000\n",
            "Training Epoch: 61 [41216/50000]\tLoss: 0.5458\tLR: 0.020000\n",
            "Training Epoch: 61 [41344/50000]\tLoss: 0.4460\tLR: 0.020000\n",
            "Training Epoch: 61 [41472/50000]\tLoss: 0.4982\tLR: 0.020000\n",
            "Training Epoch: 61 [41600/50000]\tLoss: 0.4147\tLR: 0.020000\n",
            "Training Epoch: 61 [41728/50000]\tLoss: 0.5082\tLR: 0.020000\n",
            "Training Epoch: 61 [41856/50000]\tLoss: 0.4809\tLR: 0.020000\n",
            "Training Epoch: 61 [41984/50000]\tLoss: 0.4907\tLR: 0.020000\n",
            "Training Epoch: 61 [42112/50000]\tLoss: 0.7666\tLR: 0.020000\n",
            "Training Epoch: 61 [42240/50000]\tLoss: 0.6293\tLR: 0.020000\n",
            "Training Epoch: 61 [42368/50000]\tLoss: 0.7668\tLR: 0.020000\n",
            "Training Epoch: 61 [42496/50000]\tLoss: 0.5898\tLR: 0.020000\n",
            "Training Epoch: 61 [42624/50000]\tLoss: 0.4799\tLR: 0.020000\n",
            "Training Epoch: 61 [42752/50000]\tLoss: 0.6317\tLR: 0.020000\n",
            "Training Epoch: 61 [42880/50000]\tLoss: 0.5756\tLR: 0.020000\n",
            "Training Epoch: 61 [43008/50000]\tLoss: 0.5105\tLR: 0.020000\n",
            "Training Epoch: 61 [43136/50000]\tLoss: 0.4805\tLR: 0.020000\n",
            "Training Epoch: 61 [43264/50000]\tLoss: 0.4836\tLR: 0.020000\n",
            "Training Epoch: 61 [43392/50000]\tLoss: 0.5308\tLR: 0.020000\n",
            "Training Epoch: 61 [43520/50000]\tLoss: 0.5099\tLR: 0.020000\n",
            "Training Epoch: 61 [43648/50000]\tLoss: 0.5422\tLR: 0.020000\n",
            "Training Epoch: 61 [43776/50000]\tLoss: 0.5228\tLR: 0.020000\n",
            "Training Epoch: 61 [43904/50000]\tLoss: 0.4988\tLR: 0.020000\n",
            "Training Epoch: 61 [44032/50000]\tLoss: 0.4477\tLR: 0.020000\n",
            "Training Epoch: 61 [44160/50000]\tLoss: 0.4898\tLR: 0.020000\n",
            "Training Epoch: 61 [44288/50000]\tLoss: 0.4122\tLR: 0.020000\n",
            "Training Epoch: 61 [44416/50000]\tLoss: 0.5819\tLR: 0.020000\n",
            "Training Epoch: 61 [44544/50000]\tLoss: 0.7368\tLR: 0.020000\n",
            "Training Epoch: 61 [44672/50000]\tLoss: 0.5218\tLR: 0.020000\n",
            "Training Epoch: 61 [44800/50000]\tLoss: 0.6030\tLR: 0.020000\n",
            "Training Epoch: 61 [44928/50000]\tLoss: 0.4573\tLR: 0.020000\n",
            "Training Epoch: 61 [45056/50000]\tLoss: 0.5407\tLR: 0.020000\n",
            "Training Epoch: 61 [45184/50000]\tLoss: 0.5971\tLR: 0.020000\n",
            "Training Epoch: 61 [45312/50000]\tLoss: 0.6735\tLR: 0.020000\n",
            "Training Epoch: 61 [45440/50000]\tLoss: 0.5630\tLR: 0.020000\n",
            "Training Epoch: 61 [45568/50000]\tLoss: 0.4972\tLR: 0.020000\n",
            "Training Epoch: 61 [45696/50000]\tLoss: 0.7094\tLR: 0.020000\n",
            "Training Epoch: 61 [45824/50000]\tLoss: 0.5043\tLR: 0.020000\n",
            "Training Epoch: 61 [45952/50000]\tLoss: 0.5739\tLR: 0.020000\n",
            "Training Epoch: 61 [46080/50000]\tLoss: 0.7060\tLR: 0.020000\n",
            "Training Epoch: 61 [46208/50000]\tLoss: 0.6564\tLR: 0.020000\n",
            "Training Epoch: 61 [46336/50000]\tLoss: 0.7161\tLR: 0.020000\n",
            "Training Epoch: 61 [46464/50000]\tLoss: 0.5282\tLR: 0.020000\n",
            "Training Epoch: 61 [46592/50000]\tLoss: 0.4117\tLR: 0.020000\n",
            "Training Epoch: 61 [46720/50000]\tLoss: 0.6195\tLR: 0.020000\n",
            "Training Epoch: 61 [46848/50000]\tLoss: 0.5601\tLR: 0.020000\n",
            "Training Epoch: 61 [46976/50000]\tLoss: 0.5108\tLR: 0.020000\n",
            "Training Epoch: 61 [47104/50000]\tLoss: 0.6694\tLR: 0.020000\n",
            "Training Epoch: 61 [47232/50000]\tLoss: 0.6436\tLR: 0.020000\n",
            "Training Epoch: 61 [47360/50000]\tLoss: 0.5500\tLR: 0.020000\n",
            "Training Epoch: 61 [47488/50000]\tLoss: 0.5723\tLR: 0.020000\n",
            "Training Epoch: 61 [47616/50000]\tLoss: 0.7052\tLR: 0.020000\n",
            "Training Epoch: 61 [47744/50000]\tLoss: 0.6644\tLR: 0.020000\n",
            "Training Epoch: 61 [47872/50000]\tLoss: 0.5287\tLR: 0.020000\n",
            "Training Epoch: 61 [48000/50000]\tLoss: 0.5605\tLR: 0.020000\n",
            "Training Epoch: 61 [48128/50000]\tLoss: 0.7757\tLR: 0.020000\n",
            "Training Epoch: 61 [48256/50000]\tLoss: 0.5749\tLR: 0.020000\n",
            "Training Epoch: 61 [48384/50000]\tLoss: 0.5943\tLR: 0.020000\n",
            "Training Epoch: 61 [48512/50000]\tLoss: 0.5043\tLR: 0.020000\n",
            "Training Epoch: 61 [48640/50000]\tLoss: 0.3834\tLR: 0.020000\n",
            "Training Epoch: 61 [48768/50000]\tLoss: 0.5781\tLR: 0.020000\n",
            "Training Epoch: 61 [48896/50000]\tLoss: 0.4780\tLR: 0.020000\n",
            "Training Epoch: 61 [49024/50000]\tLoss: 0.3454\tLR: 0.020000\n",
            "Training Epoch: 61 [49152/50000]\tLoss: 0.5312\tLR: 0.020000\n",
            "Training Epoch: 61 [49280/50000]\tLoss: 0.5809\tLR: 0.020000\n",
            "Training Epoch: 61 [49408/50000]\tLoss: 0.4768\tLR: 0.020000\n",
            "Training Epoch: 61 [49536/50000]\tLoss: 0.6505\tLR: 0.020000\n",
            "Training Epoch: 61 [49664/50000]\tLoss: 0.3424\tLR: 0.020000\n",
            "Training Epoch: 61 [49792/50000]\tLoss: 0.7034\tLR: 0.020000\n",
            "Training Epoch: 61 [49920/50000]\tLoss: 0.6383\tLR: 0.020000\n",
            "Training Epoch: 61 [50000/50000]\tLoss: 0.5651\tLR: 0.020000\n",
            "epoch 61 training time consumed: 145.32s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  133678 KB |    1044 MB |  223542 GB |  223542 GB |\n",
            "|       from large pool |  122880 KB |    1034 MB |  223362 GB |  223362 GB |\n",
            "|       from small pool |   10798 KB |      13 MB |     180 GB |     180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  133678 KB |    1044 MB |  223542 GB |  223542 GB |\n",
            "|       from large pool |  122880 KB |    1034 MB |  223362 GB |  223362 GB |\n",
            "|       from small pool |   10798 KB |      13 MB |     180 GB |     180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    1838 MB |    1838 MB |    1838 MB |       0 B  |\n",
            "|       from large pool |    1822 MB |    1822 MB |    1822 MB |       0 B  |\n",
            "|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  433618 KB |  778793 KB |  103939 GB |  103938 GB |\n",
            "|       from large pool |  432128 KB |  776192 KB |  103758 GB |  103758 GB |\n",
            "|       from small pool |    1490 KB |    3494 KB |     180 GB |     180 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     255    |     335    |    9044 K  |    9043 K  |\n",
            "|       from large pool |      24    |      65    |    4433 K  |    4433 K  |\n",
            "|       from small pool |     231    |     274    |    4610 K  |    4610 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     255    |     335    |    9044 K  |    9043 K  |\n",
            "|       from large pool |      24    |      65    |    4433 K  |    4433 K  |\n",
            "|       from small pool |     231    |     274    |    4610 K  |    4610 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      20    |      20    |      20    |       0    |\n",
            "|       from large pool |      12    |      12    |      12    |       0    |\n",
            "|       from small pool |       8    |       8    |       8    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      35    |      45    |    5235 K  |    5235 K  |\n",
            "|       from large pool |      10    |      15    |    2006 K  |    2006 K  |\n",
            "|       from small pool |      25    |      35    |    3228 K  |    3228 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 61, Average loss: 0.0077, Accuracy: 0.7250, Time consumed:9.03s\n",
            "\n",
            "Training Epoch: 62 [128/50000]\tLoss: 0.3441\tLR: 0.020000\n",
            "Training Epoch: 62 [256/50000]\tLoss: 0.4486\tLR: 0.020000\n",
            "Training Epoch: 62 [384/50000]\tLoss: 0.3024\tLR: 0.020000\n",
            "Training Epoch: 62 [512/50000]\tLoss: 0.4535\tLR: 0.020000\n",
            "Training Epoch: 62 [640/50000]\tLoss: 0.4464\tLR: 0.020000\n",
            "Training Epoch: 62 [768/50000]\tLoss: 0.5302\tLR: 0.020000\n",
            "Training Epoch: 62 [896/50000]\tLoss: 0.5551\tLR: 0.020000\n",
            "Training Epoch: 62 [1024/50000]\tLoss: 0.5773\tLR: 0.020000\n",
            "Training Epoch: 62 [1152/50000]\tLoss: 0.5469\tLR: 0.020000\n",
            "Training Epoch: 62 [1280/50000]\tLoss: 0.5017\tLR: 0.020000\n",
            "Training Epoch: 62 [1408/50000]\tLoss: 0.4645\tLR: 0.020000\n",
            "Training Epoch: 62 [1536/50000]\tLoss: 0.4869\tLR: 0.020000\n",
            "Training Epoch: 62 [1664/50000]\tLoss: 0.4666\tLR: 0.020000\n",
            "Training Epoch: 62 [1792/50000]\tLoss: 0.5580\tLR: 0.020000\n",
            "Training Epoch: 62 [1920/50000]\tLoss: 0.3749\tLR: 0.020000\n",
            "Training Epoch: 62 [2048/50000]\tLoss: 0.4895\tLR: 0.020000\n",
            "Training Epoch: 62 [2176/50000]\tLoss: 0.3524\tLR: 0.020000\n",
            "Training Epoch: 62 [2304/50000]\tLoss: 0.4520\tLR: 0.020000\n",
            "Training Epoch: 62 [2432/50000]\tLoss: 0.4502\tLR: 0.020000\n",
            "Training Epoch: 62 [2560/50000]\tLoss: 0.5004\tLR: 0.020000\n",
            "Training Epoch: 62 [2688/50000]\tLoss: 0.5213\tLR: 0.020000\n",
            "Training Epoch: 62 [2816/50000]\tLoss: 0.4827\tLR: 0.020000\n",
            "Training Epoch: 62 [2944/50000]\tLoss: 0.4486\tLR: 0.020000\n",
            "Training Epoch: 62 [3072/50000]\tLoss: 0.4281\tLR: 0.020000\n",
            "Training Epoch: 62 [3200/50000]\tLoss: 0.4296\tLR: 0.020000\n",
            "Training Epoch: 62 [3328/50000]\tLoss: 0.5186\tLR: 0.020000\n",
            "Training Epoch: 62 [3456/50000]\tLoss: 0.5006\tLR: 0.020000\n",
            "Training Epoch: 62 [3584/50000]\tLoss: 0.4147\tLR: 0.020000\n",
            "Training Epoch: 62 [3712/50000]\tLoss: 0.4303\tLR: 0.020000\n",
            "Training Epoch: 62 [3840/50000]\tLoss: 0.5595\tLR: 0.020000\n",
            "Training Epoch: 62 [3968/50000]\tLoss: 0.4280\tLR: 0.020000\n",
            "Training Epoch: 62 [4096/50000]\tLoss: 0.5490\tLR: 0.020000\n",
            "Training Epoch: 62 [4224/50000]\tLoss: 0.4527\tLR: 0.020000\n",
            "Training Epoch: 62 [4352/50000]\tLoss: 0.5794\tLR: 0.020000\n",
            "Training Epoch: 62 [4480/50000]\tLoss: 0.5306\tLR: 0.020000\n",
            "Training Epoch: 62 [4608/50000]\tLoss: 0.4874\tLR: 0.020000\n",
            "Training Epoch: 62 [4736/50000]\tLoss: 0.4187\tLR: 0.020000\n",
            "Training Epoch: 62 [4864/50000]\tLoss: 0.4368\tLR: 0.020000\n",
            "Training Epoch: 62 [4992/50000]\tLoss: 0.4520\tLR: 0.020000\n",
            "Training Epoch: 62 [5120/50000]\tLoss: 0.4751\tLR: 0.020000\n",
            "Training Epoch: 62 [5248/50000]\tLoss: 0.5208\tLR: 0.020000\n",
            "Training Epoch: 62 [5376/50000]\tLoss: 0.4755\tLR: 0.020000\n",
            "Training Epoch: 62 [5504/50000]\tLoss: 0.4257\tLR: 0.020000\n",
            "Training Epoch: 62 [5632/50000]\tLoss: 0.6001\tLR: 0.020000\n",
            "Training Epoch: 62 [5760/50000]\tLoss: 0.5001\tLR: 0.020000\n",
            "Training Epoch: 62 [5888/50000]\tLoss: 0.4863\tLR: 0.020000\n",
            "Training Epoch: 62 [6016/50000]\tLoss: 0.4331\tLR: 0.020000\n",
            "Training Epoch: 62 [6144/50000]\tLoss: 0.5192\tLR: 0.020000\n",
            "Training Epoch: 62 [6272/50000]\tLoss: 0.5456\tLR: 0.020000\n",
            "Training Epoch: 62 [6400/50000]\tLoss: 0.4683\tLR: 0.020000\n",
            "Training Epoch: 62 [6528/50000]\tLoss: 0.5571\tLR: 0.020000\n",
            "Training Epoch: 62 [6656/50000]\tLoss: 0.4222\tLR: 0.020000\n",
            "Training Epoch: 62 [6784/50000]\tLoss: 0.5073\tLR: 0.020000\n",
            "Training Epoch: 62 [6912/50000]\tLoss: 0.4742\tLR: 0.020000\n",
            "Training Epoch: 62 [7040/50000]\tLoss: 0.6335\tLR: 0.020000\n",
            "Training Epoch: 62 [7168/50000]\tLoss: 0.5153\tLR: 0.020000\n",
            "Training Epoch: 62 [7296/50000]\tLoss: 0.4184\tLR: 0.020000\n",
            "Training Epoch: 62 [7424/50000]\tLoss: 0.4932\tLR: 0.020000\n",
            "Training Epoch: 62 [7552/50000]\tLoss: 0.4292\tLR: 0.020000\n",
            "Training Epoch: 62 [7680/50000]\tLoss: 0.3931\tLR: 0.020000\n",
            "Training Epoch: 62 [7808/50000]\tLoss: 0.5044\tLR: 0.020000\n",
            "Training Epoch: 62 [7936/50000]\tLoss: 0.4528\tLR: 0.020000\n",
            "Training Epoch: 62 [8064/50000]\tLoss: 0.4572\tLR: 0.020000\n",
            "Training Epoch: 62 [8192/50000]\tLoss: 0.4743\tLR: 0.020000\n",
            "Training Epoch: 62 [8320/50000]\tLoss: 0.4291\tLR: 0.020000\n",
            "Training Epoch: 62 [8448/50000]\tLoss: 0.6202\tLR: 0.020000\n",
            "Training Epoch: 62 [8576/50000]\tLoss: 0.4398\tLR: 0.020000\n",
            "Training Epoch: 62 [8704/50000]\tLoss: 0.4335\tLR: 0.020000\n",
            "Training Epoch: 62 [8832/50000]\tLoss: 0.4324\tLR: 0.020000\n",
            "Training Epoch: 62 [8960/50000]\tLoss: 0.5072\tLR: 0.020000\n",
            "Training Epoch: 62 [9088/50000]\tLoss: 0.5206\tLR: 0.020000\n",
            "Training Epoch: 62 [9216/50000]\tLoss: 0.4246\tLR: 0.020000\n",
            "Training Epoch: 62 [9344/50000]\tLoss: 0.5546\tLR: 0.020000\n",
            "Training Epoch: 62 [9472/50000]\tLoss: 0.4069\tLR: 0.020000\n",
            "Training Epoch: 62 [9600/50000]\tLoss: 0.5144\tLR: 0.020000\n",
            "Training Epoch: 62 [9728/50000]\tLoss: 0.6173\tLR: 0.020000\n",
            "Training Epoch: 62 [9856/50000]\tLoss: 0.4334\tLR: 0.020000\n",
            "Training Epoch: 62 [9984/50000]\tLoss: 0.4834\tLR: 0.020000\n",
            "Training Epoch: 62 [10112/50000]\tLoss: 0.4078\tLR: 0.020000\n",
            "Training Epoch: 62 [10240/50000]\tLoss: 0.4262\tLR: 0.020000\n",
            "Training Epoch: 62 [10368/50000]\tLoss: 0.6062\tLR: 0.020000\n",
            "Training Epoch: 62 [10496/50000]\tLoss: 0.4693\tLR: 0.020000\n",
            "Training Epoch: 62 [10624/50000]\tLoss: 0.5028\tLR: 0.020000\n",
            "Training Epoch: 62 [10752/50000]\tLoss: 0.3485\tLR: 0.020000\n",
            "Training Epoch: 62 [10880/50000]\tLoss: 0.5559\tLR: 0.020000\n",
            "Training Epoch: 62 [11008/50000]\tLoss: 0.4546\tLR: 0.020000\n",
            "Training Epoch: 62 [11136/50000]\tLoss: 0.3774\tLR: 0.020000\n",
            "Training Epoch: 62 [11264/50000]\tLoss: 0.4373\tLR: 0.020000\n",
            "Training Epoch: 62 [11392/50000]\tLoss: 0.5127\tLR: 0.020000\n",
            "Training Epoch: 62 [11520/50000]\tLoss: 0.3826\tLR: 0.020000\n",
            "Training Epoch: 62 [11648/50000]\tLoss: 0.5479\tLR: 0.020000\n",
            "Training Epoch: 62 [11776/50000]\tLoss: 0.6220\tLR: 0.020000\n",
            "Training Epoch: 62 [11904/50000]\tLoss: 0.4628\tLR: 0.020000\n",
            "Training Epoch: 62 [12032/50000]\tLoss: 0.4956\tLR: 0.020000\n",
            "Training Epoch: 62 [12160/50000]\tLoss: 0.4769\tLR: 0.020000\n",
            "Training Epoch: 62 [12288/50000]\tLoss: 0.5928\tLR: 0.020000\n",
            "Training Epoch: 62 [12416/50000]\tLoss: 0.6550\tLR: 0.020000\n",
            "Training Epoch: 62 [12544/50000]\tLoss: 0.4316\tLR: 0.020000\n",
            "Training Epoch: 62 [12672/50000]\tLoss: 0.4805\tLR: 0.020000\n",
            "Training Epoch: 62 [12800/50000]\tLoss: 0.5188\tLR: 0.020000\n",
            "Training Epoch: 62 [12928/50000]\tLoss: 0.6466\tLR: 0.020000\n",
            "Training Epoch: 62 [13056/50000]\tLoss: 0.5372\tLR: 0.020000\n",
            "Training Epoch: 62 [13184/50000]\tLoss: 0.4077\tLR: 0.020000\n",
            "Training Epoch: 62 [13312/50000]\tLoss: 0.5344\tLR: 0.020000\n",
            "Training Epoch: 62 [13440/50000]\tLoss: 0.4903\tLR: 0.020000\n",
            "Training Epoch: 62 [13568/50000]\tLoss: 0.5090\tLR: 0.020000\n",
            "Training Epoch: 62 [13696/50000]\tLoss: 0.5454\tLR: 0.020000\n",
            "Training Epoch: 62 [13824/50000]\tLoss: 0.4315\tLR: 0.020000\n",
            "Training Epoch: 62 [13952/50000]\tLoss: 0.5927\tLR: 0.020000\n",
            "Training Epoch: 62 [14080/50000]\tLoss: 0.5078\tLR: 0.020000\n",
            "Training Epoch: 62 [14208/50000]\tLoss: 0.5163\tLR: 0.020000\n",
            "Training Epoch: 62 [14336/50000]\tLoss: 0.5314\tLR: 0.020000\n",
            "Training Epoch: 62 [14464/50000]\tLoss: 0.4846\tLR: 0.020000\n",
            "Training Epoch: 62 [14592/50000]\tLoss: 0.5931\tLR: 0.020000\n",
            "Training Epoch: 62 [14720/50000]\tLoss: 0.4586\tLR: 0.020000\n",
            "Training Epoch: 62 [14848/50000]\tLoss: 0.5278\tLR: 0.020000\n",
            "Training Epoch: 62 [14976/50000]\tLoss: 0.4977\tLR: 0.020000\n",
            "Training Epoch: 62 [15104/50000]\tLoss: 0.5236\tLR: 0.020000\n",
            "Training Epoch: 62 [15232/50000]\tLoss: 0.4610\tLR: 0.020000\n",
            "Training Epoch: 62 [15360/50000]\tLoss: 0.4990\tLR: 0.020000\n",
            "Training Epoch: 62 [15488/50000]\tLoss: 0.4809\tLR: 0.020000\n",
            "Training Epoch: 62 [15616/50000]\tLoss: 0.4233\tLR: 0.020000\n",
            "Training Epoch: 62 [15744/50000]\tLoss: 0.5208\tLR: 0.020000\n",
            "Training Epoch: 62 [15872/50000]\tLoss: 0.5522\tLR: 0.020000\n",
            "Training Epoch: 62 [16000/50000]\tLoss: 0.3851\tLR: 0.020000\n",
            "Training Epoch: 62 [16128/50000]\tLoss: 0.4946\tLR: 0.020000\n",
            "Training Epoch: 62 [16256/50000]\tLoss: 0.6056\tLR: 0.020000\n",
            "Training Epoch: 62 [16384/50000]\tLoss: 0.5816\tLR: 0.020000\n",
            "Training Epoch: 62 [16512/50000]\tLoss: 0.6116\tLR: 0.020000\n",
            "Training Epoch: 62 [16640/50000]\tLoss: 0.4441\tLR: 0.020000\n",
            "Training Epoch: 62 [16768/50000]\tLoss: 0.6311\tLR: 0.020000\n",
            "Training Epoch: 62 [16896/50000]\tLoss: 0.4097\tLR: 0.020000\n",
            "Training Epoch: 62 [17024/50000]\tLoss: 0.5368\tLR: 0.020000\n",
            "Training Epoch: 62 [17152/50000]\tLoss: 0.3699\tLR: 0.020000\n",
            "Training Epoch: 62 [17280/50000]\tLoss: 0.5059\tLR: 0.020000\n",
            "Training Epoch: 62 [17408/50000]\tLoss: 0.4208\tLR: 0.020000\n",
            "Training Epoch: 62 [17536/50000]\tLoss: 0.4639\tLR: 0.020000\n",
            "Training Epoch: 62 [17664/50000]\tLoss: 0.3907\tLR: 0.020000\n",
            "Training Epoch: 62 [17792/50000]\tLoss: 0.4140\tLR: 0.020000\n",
            "Training Epoch: 62 [17920/50000]\tLoss: 0.4540\tLR: 0.020000\n",
            "Training Epoch: 62 [18048/50000]\tLoss: 0.4568\tLR: 0.020000\n",
            "Training Epoch: 62 [18176/50000]\tLoss: 0.4199\tLR: 0.020000\n",
            "Training Epoch: 62 [18304/50000]\tLoss: 0.6114\tLR: 0.020000\n",
            "Training Epoch: 62 [18432/50000]\tLoss: 0.3846\tLR: 0.020000\n",
            "Training Epoch: 62 [18560/50000]\tLoss: 0.4671\tLR: 0.020000\n",
            "Training Epoch: 62 [18688/50000]\tLoss: 0.5187\tLR: 0.020000\n",
            "Training Epoch: 62 [18816/50000]\tLoss: 0.5007\tLR: 0.020000\n",
            "Training Epoch: 62 [18944/50000]\tLoss: 0.5673\tLR: 0.020000\n",
            "Training Epoch: 62 [19072/50000]\tLoss: 0.4337\tLR: 0.020000\n",
            "Training Epoch: 62 [19200/50000]\tLoss: 0.4420\tLR: 0.020000\n",
            "Training Epoch: 62 [19328/50000]\tLoss: 0.3435\tLR: 0.020000\n",
            "Training Epoch: 62 [19456/50000]\tLoss: 0.5085\tLR: 0.020000\n",
            "Training Epoch: 62 [19584/50000]\tLoss: 0.4668\tLR: 0.020000\n",
            "Training Epoch: 62 [19712/50000]\tLoss: 0.4699\tLR: 0.020000\n",
            "Training Epoch: 62 [19840/50000]\tLoss: 0.5798\tLR: 0.020000\n",
            "Training Epoch: 62 [19968/50000]\tLoss: 0.4667\tLR: 0.020000\n",
            "Training Epoch: 62 [20096/50000]\tLoss: 0.3702\tLR: 0.020000\n",
            "Training Epoch: 62 [20224/50000]\tLoss: 0.4230\tLR: 0.020000\n",
            "Training Epoch: 62 [20352/50000]\tLoss: 0.5468\tLR: 0.020000\n",
            "Training Epoch: 62 [20480/50000]\tLoss: 0.3485\tLR: 0.020000\n",
            "Training Epoch: 62 [20608/50000]\tLoss: 0.5372\tLR: 0.020000\n",
            "Training Epoch: 62 [20736/50000]\tLoss: 0.4644\tLR: 0.020000\n",
            "Training Epoch: 62 [20864/50000]\tLoss: 0.4263\tLR: 0.020000\n",
            "Training Epoch: 62 [20992/50000]\tLoss: 0.4333\tLR: 0.020000\n",
            "Training Epoch: 62 [21120/50000]\tLoss: 0.5919\tLR: 0.020000\n",
            "Training Epoch: 62 [21248/50000]\tLoss: 0.3629\tLR: 0.020000\n",
            "Training Epoch: 62 [21376/50000]\tLoss: 0.4483\tLR: 0.020000\n",
            "Training Epoch: 62 [21504/50000]\tLoss: 0.4584\tLR: 0.020000\n",
            "Training Epoch: 62 [21632/50000]\tLoss: 0.6571\tLR: 0.020000\n",
            "Training Epoch: 62 [21760/50000]\tLoss: 0.5675\tLR: 0.020000\n",
            "Training Epoch: 62 [21888/50000]\tLoss: 0.5167\tLR: 0.020000\n",
            "Training Epoch: 62 [22016/50000]\tLoss: 0.5962\tLR: 0.020000\n",
            "Training Epoch: 62 [22144/50000]\tLoss: 0.5641\tLR: 0.020000\n",
            "Training Epoch: 62 [22272/50000]\tLoss: 0.4184\tLR: 0.020000\n",
            "Training Epoch: 62 [22400/50000]\tLoss: 0.5441\tLR: 0.020000\n",
            "Training Epoch: 62 [22528/50000]\tLoss: 0.4818\tLR: 0.020000\n",
            "Training Epoch: 62 [22656/50000]\tLoss: 0.5092\tLR: 0.020000\n",
            "Training Epoch: 62 [22784/50000]\tLoss: 0.3593\tLR: 0.020000\n",
            "Training Epoch: 62 [22912/50000]\tLoss: 0.6004\tLR: 0.020000\n",
            "Training Epoch: 62 [23040/50000]\tLoss: 0.4904\tLR: 0.020000\n",
            "Training Epoch: 62 [23168/50000]\tLoss: 0.4519\tLR: 0.020000\n",
            "Training Epoch: 62 [23296/50000]\tLoss: 0.5107\tLR: 0.020000\n",
            "Training Epoch: 62 [23424/50000]\tLoss: 0.5151\tLR: 0.020000\n",
            "Training Epoch: 62 [23552/50000]\tLoss: 0.4960\tLR: 0.020000\n",
            "Training Epoch: 62 [23680/50000]\tLoss: 0.4466\tLR: 0.020000\n",
            "Training Epoch: 62 [23808/50000]\tLoss: 0.5472\tLR: 0.020000\n",
            "Training Epoch: 62 [23936/50000]\tLoss: 0.4366\tLR: 0.020000\n",
            "Training Epoch: 62 [24064/50000]\tLoss: 0.3359\tLR: 0.020000\n",
            "Training Epoch: 62 [24192/50000]\tLoss: 0.5549\tLR: 0.020000\n",
            "Training Epoch: 62 [24320/50000]\tLoss: 0.6095\tLR: 0.020000\n",
            "Training Epoch: 62 [24448/50000]\tLoss: 0.5443\tLR: 0.020000\n",
            "Training Epoch: 62 [24576/50000]\tLoss: 0.5756\tLR: 0.020000\n",
            "Training Epoch: 62 [24704/50000]\tLoss: 0.4422\tLR: 0.020000\n",
            "Training Epoch: 62 [24832/50000]\tLoss: 0.5703\tLR: 0.020000\n",
            "Training Epoch: 62 [24960/50000]\tLoss: 0.6327\tLR: 0.020000\n",
            "Training Epoch: 62 [25088/50000]\tLoss: 0.5785\tLR: 0.020000\n",
            "Training Epoch: 62 [25216/50000]\tLoss: 0.4927\tLR: 0.020000\n",
            "Training Epoch: 62 [25344/50000]\tLoss: 0.4752\tLR: 0.020000\n",
            "Training Epoch: 62 [25472/50000]\tLoss: 0.4673\tLR: 0.020000\n",
            "Training Epoch: 62 [25600/50000]\tLoss: 0.5234\tLR: 0.020000\n",
            "Training Epoch: 62 [25728/50000]\tLoss: 0.4200\tLR: 0.020000\n",
            "Training Epoch: 62 [25856/50000]\tLoss: 0.4573\tLR: 0.020000\n",
            "Training Epoch: 62 [25984/50000]\tLoss: 0.3984\tLR: 0.020000\n",
            "Training Epoch: 62 [26112/50000]\tLoss: 0.4316\tLR: 0.020000\n",
            "Training Epoch: 62 [26240/50000]\tLoss: 0.5011\tLR: 0.020000\n",
            "Training Epoch: 62 [26368/50000]\tLoss: 0.4793\tLR: 0.020000\n",
            "Training Epoch: 62 [26496/50000]\tLoss: 0.6391\tLR: 0.020000\n",
            "Training Epoch: 62 [26624/50000]\tLoss: 0.4416\tLR: 0.020000\n",
            "Training Epoch: 62 [26752/50000]\tLoss: 0.4424\tLR: 0.020000\n",
            "Training Epoch: 62 [26880/50000]\tLoss: 0.5378\tLR: 0.020000\n",
            "Training Epoch: 62 [27008/50000]\tLoss: 0.5754\tLR: 0.020000\n",
            "Training Epoch: 62 [27136/50000]\tLoss: 0.4415\tLR: 0.020000\n",
            "Training Epoch: 62 [27264/50000]\tLoss: 0.5150\tLR: 0.020000\n",
            "Training Epoch: 62 [27392/50000]\tLoss: 0.6260\tLR: 0.020000\n",
            "Training Epoch: 62 [27520/50000]\tLoss: 0.4512\tLR: 0.020000\n",
            "Training Epoch: 62 [27648/50000]\tLoss: 0.5318\tLR: 0.020000\n",
            "Training Epoch: 62 [27776/50000]\tLoss: 0.5580\tLR: 0.020000\n",
            "Training Epoch: 62 [27904/50000]\tLoss: 0.3790\tLR: 0.020000\n",
            "Training Epoch: 62 [28032/50000]\tLoss: 0.4100\tLR: 0.020000\n",
            "Training Epoch: 62 [28160/50000]\tLoss: 0.7030\tLR: 0.020000\n",
            "Training Epoch: 62 [28288/50000]\tLoss: 0.3942\tLR: 0.020000\n",
            "Training Epoch: 62 [28416/50000]\tLoss: 0.5781\tLR: 0.020000\n",
            "Training Epoch: 62 [28544/50000]\tLoss: 0.4908\tLR: 0.020000\n",
            "Training Epoch: 62 [28672/50000]\tLoss: 0.5614\tLR: 0.020000\n",
            "Training Epoch: 62 [28800/50000]\tLoss: 0.6416\tLR: 0.020000\n",
            "Training Epoch: 62 [28928/50000]\tLoss: 0.4989\tLR: 0.020000\n",
            "Training Epoch: 62 [29056/50000]\tLoss: 0.4803\tLR: 0.020000\n",
            "Training Epoch: 62 [29184/50000]\tLoss: 0.5092\tLR: 0.020000\n",
            "Training Epoch: 62 [29312/50000]\tLoss: 0.5691\tLR: 0.020000\n",
            "Training Epoch: 62 [29440/50000]\tLoss: 0.4569\tLR: 0.020000\n",
            "Training Epoch: 62 [29568/50000]\tLoss: 0.5044\tLR: 0.020000\n",
            "Training Epoch: 62 [29696/50000]\tLoss: 0.5848\tLR: 0.020000\n",
            "Training Epoch: 62 [29824/50000]\tLoss: 0.5253\tLR: 0.020000\n",
            "Training Epoch: 62 [29952/50000]\tLoss: 0.5050\tLR: 0.020000\n",
            "Training Epoch: 62 [30080/50000]\tLoss: 0.3584\tLR: 0.020000\n",
            "Training Epoch: 62 [30208/50000]\tLoss: 0.3992\tLR: 0.020000\n",
            "Training Epoch: 62 [30336/50000]\tLoss: 0.5180\tLR: 0.020000\n",
            "Training Epoch: 62 [30464/50000]\tLoss: 0.4888\tLR: 0.020000\n",
            "Training Epoch: 62 [30592/50000]\tLoss: 0.5842\tLR: 0.020000\n",
            "Training Epoch: 62 [30720/50000]\tLoss: 0.5970\tLR: 0.020000\n",
            "Training Epoch: 62 [30848/50000]\tLoss: 0.6237\tLR: 0.020000\n",
            "Training Epoch: 62 [30976/50000]\tLoss: 0.3295\tLR: 0.020000\n",
            "Training Epoch: 62 [31104/50000]\tLoss: 0.4630\tLR: 0.020000\n",
            "Training Epoch: 62 [31232/50000]\tLoss: 0.5703\tLR: 0.020000\n",
            "Training Epoch: 62 [31360/50000]\tLoss: 0.4629\tLR: 0.020000\n",
            "Training Epoch: 62 [31488/50000]\tLoss: 0.4044\tLR: 0.020000\n",
            "Training Epoch: 62 [31616/50000]\tLoss: 0.5711\tLR: 0.020000\n",
            "Training Epoch: 62 [31744/50000]\tLoss: 0.5070\tLR: 0.020000\n",
            "Training Epoch: 62 [31872/50000]\tLoss: 0.5562\tLR: 0.020000\n",
            "Training Epoch: 62 [32000/50000]\tLoss: 0.4850\tLR: 0.020000\n",
            "Training Epoch: 62 [32128/50000]\tLoss: 0.6255\tLR: 0.020000\n",
            "Training Epoch: 62 [32256/50000]\tLoss: 0.5163\tLR: 0.020000\n",
            "Training Epoch: 62 [32384/50000]\tLoss: 0.4907\tLR: 0.020000\n",
            "Training Epoch: 62 [32512/50000]\tLoss: 0.4922\tLR: 0.020000\n",
            "Training Epoch: 62 [32640/50000]\tLoss: 0.6008\tLR: 0.020000\n",
            "Training Epoch: 62 [32768/50000]\tLoss: 0.5953\tLR: 0.020000\n",
            "Training Epoch: 62 [32896/50000]\tLoss: 0.4039\tLR: 0.020000\n",
            "Training Epoch: 62 [33024/50000]\tLoss: 0.4677\tLR: 0.020000\n",
            "Training Epoch: 62 [33152/50000]\tLoss: 0.3983\tLR: 0.020000\n",
            "Training Epoch: 62 [33280/50000]\tLoss: 0.5272\tLR: 0.020000\n",
            "Training Epoch: 62 [33408/50000]\tLoss: 0.4714\tLR: 0.020000\n",
            "Training Epoch: 62 [33536/50000]\tLoss: 0.4447\tLR: 0.020000\n",
            "Training Epoch: 62 [33664/50000]\tLoss: 0.4084\tLR: 0.020000\n",
            "Training Epoch: 62 [33792/50000]\tLoss: 0.5593\tLR: 0.020000\n",
            "Training Epoch: 62 [33920/50000]\tLoss: 0.5899\tLR: 0.020000\n",
            "Training Epoch: 62 [34048/50000]\tLoss: 0.4955\tLR: 0.020000\n",
            "Training Epoch: 62 [34176/50000]\tLoss: 0.4632\tLR: 0.020000\n",
            "Training Epoch: 62 [34304/50000]\tLoss: 0.5564\tLR: 0.020000\n",
            "Training Epoch: 62 [34432/50000]\tLoss: 0.4434\tLR: 0.020000\n",
            "Training Epoch: 62 [34560/50000]\tLoss: 0.4746\tLR: 0.020000\n",
            "Training Epoch: 62 [34688/50000]\tLoss: 0.4448\tLR: 0.020000\n",
            "Training Epoch: 62 [34816/50000]\tLoss: 0.4388\tLR: 0.020000\n",
            "Training Epoch: 62 [34944/50000]\tLoss: 0.3079\tLR: 0.020000\n",
            "Training Epoch: 62 [35072/50000]\tLoss: 0.5569\tLR: 0.020000\n",
            "Training Epoch: 62 [35200/50000]\tLoss: 0.5297\tLR: 0.020000\n",
            "Training Epoch: 62 [35328/50000]\tLoss: 0.5714\tLR: 0.020000\n",
            "Training Epoch: 62 [35456/50000]\tLoss: 0.4526\tLR: 0.020000\n",
            "Training Epoch: 62 [35584/50000]\tLoss: 0.4901\tLR: 0.020000\n",
            "Training Epoch: 62 [35712/50000]\tLoss: 0.4838\tLR: 0.020000\n",
            "Training Epoch: 62 [35840/50000]\tLoss: 0.4893\tLR: 0.020000\n",
            "Training Epoch: 62 [35968/50000]\tLoss: 0.5472\tLR: 0.020000\n",
            "Training Epoch: 62 [36096/50000]\tLoss: 0.5789\tLR: 0.020000\n",
            "Training Epoch: 62 [36224/50000]\tLoss: 0.5282\tLR: 0.020000\n",
            "Training Epoch: 62 [36352/50000]\tLoss: 0.6524\tLR: 0.020000\n",
            "Training Epoch: 62 [36480/50000]\tLoss: 0.5492\tLR: 0.020000\n",
            "Training Epoch: 62 [36608/50000]\tLoss: 0.6433\tLR: 0.020000\n",
            "Training Epoch: 62 [36736/50000]\tLoss: 0.4881\tLR: 0.020000\n",
            "Training Epoch: 62 [36864/50000]\tLoss: 0.5999\tLR: 0.020000\n",
            "Training Epoch: 62 [36992/50000]\tLoss: 0.5345\tLR: 0.020000\n",
            "Training Epoch: 62 [37120/50000]\tLoss: 0.4157\tLR: 0.020000\n",
            "Training Epoch: 62 [37248/50000]\tLoss: 0.6504\tLR: 0.020000\n",
            "Training Epoch: 62 [37376/50000]\tLoss: 0.5113\tLR: 0.020000\n",
            "Training Epoch: 62 [37504/50000]\tLoss: 0.5140\tLR: 0.020000\n",
            "Training Epoch: 62 [37632/50000]\tLoss: 0.4372\tLR: 0.020000\n",
            "Training Epoch: 62 [37760/50000]\tLoss: 0.4702\tLR: 0.020000\n",
            "Training Epoch: 62 [37888/50000]\tLoss: 0.4524\tLR: 0.020000\n",
            "Training Epoch: 62 [38016/50000]\tLoss: 0.5513\tLR: 0.020000\n",
            "Training Epoch: 62 [38144/50000]\tLoss: 0.4479\tLR: 0.020000\n",
            "Training Epoch: 62 [38272/50000]\tLoss: 0.3822\tLR: 0.020000\n",
            "Training Epoch: 62 [38400/50000]\tLoss: 0.4910\tLR: 0.020000\n",
            "Training Epoch: 62 [38528/50000]\tLoss: 0.5322\tLR: 0.020000\n",
            "Training Epoch: 62 [38656/50000]\tLoss: 0.6326\tLR: 0.020000\n",
            "Training Epoch: 62 [38784/50000]\tLoss: 0.6456\tLR: 0.020000\n",
            "Training Epoch: 62 [38912/50000]\tLoss: 0.7059\tLR: 0.020000\n",
            "Training Epoch: 62 [39040/50000]\tLoss: 0.5836\tLR: 0.020000\n",
            "Training Epoch: 62 [39168/50000]\tLoss: 0.4556\tLR: 0.020000\n",
            "Training Epoch: 62 [39296/50000]\tLoss: 0.4398\tLR: 0.020000\n",
            "Training Epoch: 62 [39424/50000]\tLoss: 0.6764\tLR: 0.020000\n",
            "Training Epoch: 62 [39552/50000]\tLoss: 0.5014\tLR: 0.020000\n",
            "Training Epoch: 62 [39680/50000]\tLoss: 0.3904\tLR: 0.020000\n",
            "Training Epoch: 62 [39808/50000]\tLoss: 0.4142\tLR: 0.020000\n",
            "Training Epoch: 62 [39936/50000]\tLoss: 0.5634\tLR: 0.020000\n",
            "Training Epoch: 62 [40064/50000]\tLoss: 0.2942\tLR: 0.020000\n",
            "Training Epoch: 62 [40192/50000]\tLoss: 0.5370\tLR: 0.020000\n",
            "Training Epoch: 62 [40320/50000]\tLoss: 0.4545\tLR: 0.020000\n",
            "Training Epoch: 62 [40448/50000]\tLoss: 0.5146\tLR: 0.020000\n",
            "Training Epoch: 62 [40576/50000]\tLoss: 0.5804\tLR: 0.020000\n",
            "Training Epoch: 62 [40704/50000]\tLoss: 0.5001\tLR: 0.020000\n",
            "Training Epoch: 62 [40832/50000]\tLoss: 0.5701\tLR: 0.020000\n",
            "Training Epoch: 62 [40960/50000]\tLoss: 0.4577\tLR: 0.020000\n",
            "Training Epoch: 62 [41088/50000]\tLoss: 0.5455\tLR: 0.020000\n",
            "Training Epoch: 62 [41216/50000]\tLoss: 0.3709\tLR: 0.020000\n",
            "Training Epoch: 62 [41344/50000]\tLoss: 0.4970\tLR: 0.020000\n",
            "Training Epoch: 62 [41472/50000]\tLoss: 0.5987\tLR: 0.020000\n",
            "Training Epoch: 62 [41600/50000]\tLoss: 0.3507\tLR: 0.020000\n",
            "Training Epoch: 62 [41728/50000]\tLoss: 0.5880\tLR: 0.020000\n",
            "Training Epoch: 62 [41856/50000]\tLoss: 0.4011\tLR: 0.020000\n",
            "Training Epoch: 62 [41984/50000]\tLoss: 0.4598\tLR: 0.020000\n",
            "Training Epoch: 62 [42112/50000]\tLoss: 0.5268\tLR: 0.020000\n",
            "Training Epoch: 62 [42240/50000]\tLoss: 0.3766\tLR: 0.020000\n",
            "Training Epoch: 62 [42368/50000]\tLoss: 0.4396\tLR: 0.020000\n",
            "Training Epoch: 62 [42496/50000]\tLoss: 0.4093\tLR: 0.020000\n",
            "Training Epoch: 62 [42624/50000]\tLoss: 0.5055\tLR: 0.020000\n",
            "Training Epoch: 62 [42752/50000]\tLoss: 0.4454\tLR: 0.020000\n",
            "Training Epoch: 62 [42880/50000]\tLoss: 0.5716\tLR: 0.020000\n",
            "Training Epoch: 62 [43008/50000]\tLoss: 0.4927\tLR: 0.020000\n",
            "Training Epoch: 62 [43136/50000]\tLoss: 0.5641\tLR: 0.020000\n",
            "Training Epoch: 62 [43264/50000]\tLoss: 0.3882\tLR: 0.020000\n",
            "Training Epoch: 62 [43392/50000]\tLoss: 0.4791\tLR: 0.020000\n",
            "Training Epoch: 62 [43520/50000]\tLoss: 0.4541\tLR: 0.020000\n",
            "Training Epoch: 62 [43648/50000]\tLoss: 0.5271\tLR: 0.020000\n",
            "Training Epoch: 62 [43776/50000]\tLoss: 0.4366\tLR: 0.020000\n",
            "Training Epoch: 62 [43904/50000]\tLoss: 0.5184\tLR: 0.020000\n",
            "Training Epoch: 62 [44032/50000]\tLoss: 0.4712\tLR: 0.020000\n",
            "Training Epoch: 62 [44160/50000]\tLoss: 0.5124\tLR: 0.020000\n",
            "Training Epoch: 62 [44288/50000]\tLoss: 0.6884\tLR: 0.020000\n",
            "Training Epoch: 62 [44416/50000]\tLoss: 0.5240\tLR: 0.020000\n",
            "Training Epoch: 62 [44544/50000]\tLoss: 0.3294\tLR: 0.020000\n",
            "Training Epoch: 62 [44672/50000]\tLoss: 0.3614\tLR: 0.020000\n",
            "Training Epoch: 62 [44800/50000]\tLoss: 0.5111\tLR: 0.020000\n",
            "Training Epoch: 62 [44928/50000]\tLoss: 0.5667\tLR: 0.020000\n",
            "Training Epoch: 62 [45056/50000]\tLoss: 0.4810\tLR: 0.020000\n",
            "Training Epoch: 62 [45184/50000]\tLoss: 0.6030\tLR: 0.020000\n",
            "Training Epoch: 62 [45312/50000]\tLoss: 0.5221\tLR: 0.020000\n",
            "Training Epoch: 62 [45440/50000]\tLoss: 0.4638\tLR: 0.020000\n",
            "Training Epoch: 62 [45568/50000]\tLoss: 0.5623\tLR: 0.020000\n",
            "Training Epoch: 62 [45696/50000]\tLoss: 0.6649\tLR: 0.020000\n",
            "Training Epoch: 62 [45824/50000]\tLoss: 0.6419\tLR: 0.020000\n",
            "Training Epoch: 62 [45952/50000]\tLoss: 0.6085\tLR: 0.020000\n",
            "Training Epoch: 62 [46080/50000]\tLoss: 0.5000\tLR: 0.020000\n",
            "Training Epoch: 62 [46208/50000]\tLoss: 0.4971\tLR: 0.020000\n",
            "Training Epoch: 62 [46336/50000]\tLoss: 0.3972\tLR: 0.020000\n",
            "Training Epoch: 62 [46464/50000]\tLoss: 0.5087\tLR: 0.020000\n",
            "Training Epoch: 62 [46592/50000]\tLoss: 0.5070\tLR: 0.020000\n",
            "Training Epoch: 62 [46720/50000]\tLoss: 0.4688\tLR: 0.020000\n",
            "Training Epoch: 62 [46848/50000]\tLoss: 0.5318\tLR: 0.020000\n",
            "Training Epoch: 62 [46976/50000]\tLoss: 0.4815\tLR: 0.020000\n",
            "Training Epoch: 62 [47104/50000]\tLoss: 0.5780\tLR: 0.020000\n",
            "Training Epoch: 62 [47232/50000]\tLoss: 0.4383\tLR: 0.020000\n",
            "Training Epoch: 62 [47360/50000]\tLoss: 0.6290\tLR: 0.020000\n",
            "Training Epoch: 62 [47488/50000]\tLoss: 0.5954\tLR: 0.020000\n",
            "Training Epoch: 62 [47616/50000]\tLoss: 0.4475\tLR: 0.020000\n",
            "Training Epoch: 62 [47744/50000]\tLoss: 0.5871\tLR: 0.020000\n",
            "Training Epoch: 62 [47872/50000]\tLoss: 0.5341\tLR: 0.020000\n",
            "Training Epoch: 62 [48000/50000]\tLoss: 0.6532\tLR: 0.020000\n",
            "Training Epoch: 62 [48128/50000]\tLoss: 0.4821\tLR: 0.020000\n",
            "Training Epoch: 62 [48256/50000]\tLoss: 0.6346\tLR: 0.020000\n",
            "Training Epoch: 62 [48384/50000]\tLoss: 0.4782\tLR: 0.020000\n",
            "Training Epoch: 62 [48512/50000]\tLoss: 0.5351\tLR: 0.020000\n",
            "Training Epoch: 62 [48640/50000]\tLoss: 0.4276\tLR: 0.020000\n",
            "Training Epoch: 62 [48768/50000]\tLoss: 0.4482\tLR: 0.020000\n",
            "Training Epoch: 62 [48896/50000]\tLoss: 0.4533\tLR: 0.020000\n",
            "Training Epoch: 62 [49024/50000]\tLoss: 0.5696\tLR: 0.020000\n",
            "Training Epoch: 62 [49152/50000]\tLoss: 0.6202\tLR: 0.020000\n",
            "Training Epoch: 62 [49280/50000]\tLoss: 0.4236\tLR: 0.020000\n",
            "Training Epoch: 62 [49408/50000]\tLoss: 0.4438\tLR: 0.020000\n",
            "Training Epoch: 62 [49536/50000]\tLoss: 0.5473\tLR: 0.020000\n",
            "Training Epoch: 62 [49664/50000]\tLoss: 0.4172\tLR: 0.020000\n",
            "Training Epoch: 62 [49792/50000]\tLoss: 0.3695\tLR: 0.020000\n",
            "Training Epoch: 62 [49920/50000]\tLoss: 0.4189\tLR: 0.020000\n",
            "Training Epoch: 62 [50000/50000]\tLoss: 0.5921\tLR: 0.020000\n",
            "epoch 62 training time consumed: 144.76s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  133678 KB |    1044 MB |  227207 GB |  227207 GB |\n",
            "|       from large pool |  122880 KB |    1034 MB |  227023 GB |  227023 GB |\n",
            "|       from small pool |   10798 KB |      13 MB |     183 GB |     183 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  133678 KB |    1044 MB |  227207 GB |  227207 GB |\n",
            "|       from large pool |  122880 KB |    1034 MB |  227023 GB |  227023 GB |\n",
            "|       from small pool |   10798 KB |      13 MB |     183 GB |     183 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    1838 MB |    1838 MB |    1838 MB |       0 B  |\n",
            "|       from large pool |    1822 MB |    1822 MB |    1822 MB |       0 B  |\n",
            "|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  433618 KB |  778793 KB |  105643 GB |  105642 GB |\n",
            "|       from large pool |  432128 KB |  776192 KB |  105459 GB |  105459 GB |\n",
            "|       from small pool |    1490 KB |    3494 KB |     183 GB |     183 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     255    |     335    |    9192 K  |    9192 K  |\n",
            "|       from large pool |      24    |      65    |    4505 K  |    4505 K  |\n",
            "|       from small pool |     231    |     274    |    4686 K  |    4686 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     255    |     335    |    9192 K  |    9192 K  |\n",
            "|       from large pool |      24    |      65    |    4505 K  |    4505 K  |\n",
            "|       from small pool |     231    |     274    |    4686 K  |    4686 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      20    |      20    |      20    |       0    |\n",
            "|       from large pool |      12    |      12    |      12    |       0    |\n",
            "|       from small pool |       8    |       8    |       8    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      38    |      45    |    5321 K  |    5320 K  |\n",
            "|       from large pool |      10    |      15    |    2039 K  |    2039 K  |\n",
            "|       from small pool |      28    |      35    |    3281 K  |    3281 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 62, Average loss: 0.0076, Accuracy: 0.7326, Time consumed:9.00s\n",
            "\n",
            "Training Epoch: 63 [128/50000]\tLoss: 0.4127\tLR: 0.020000\n",
            "Training Epoch: 63 [256/50000]\tLoss: 0.3234\tLR: 0.020000\n",
            "Training Epoch: 63 [384/50000]\tLoss: 0.4044\tLR: 0.020000\n",
            "Training Epoch: 63 [512/50000]\tLoss: 0.4223\tLR: 0.020000\n",
            "Training Epoch: 63 [640/50000]\tLoss: 0.3114\tLR: 0.020000\n",
            "Training Epoch: 63 [768/50000]\tLoss: 0.3768\tLR: 0.020000\n",
            "Training Epoch: 63 [896/50000]\tLoss: 0.6067\tLR: 0.020000\n",
            "Training Epoch: 63 [1024/50000]\tLoss: 0.4155\tLR: 0.020000\n",
            "Training Epoch: 63 [1152/50000]\tLoss: 0.3424\tLR: 0.020000\n",
            "Training Epoch: 63 [1280/50000]\tLoss: 0.3295\tLR: 0.020000\n",
            "Training Epoch: 63 [1408/50000]\tLoss: 0.5333\tLR: 0.020000\n",
            "Training Epoch: 63 [1536/50000]\tLoss: 0.4885\tLR: 0.020000\n",
            "Training Epoch: 63 [1664/50000]\tLoss: 0.3872\tLR: 0.020000\n",
            "Training Epoch: 63 [1792/50000]\tLoss: 0.5699\tLR: 0.020000\n",
            "Training Epoch: 63 [1920/50000]\tLoss: 0.5657\tLR: 0.020000\n",
            "Training Epoch: 63 [2048/50000]\tLoss: 0.4243\tLR: 0.020000\n",
            "Training Epoch: 63 [2176/50000]\tLoss: 0.3686\tLR: 0.020000\n",
            "Training Epoch: 63 [2304/50000]\tLoss: 0.4681\tLR: 0.020000\n",
            "Training Epoch: 63 [2432/50000]\tLoss: 0.3611\tLR: 0.020000\n",
            "Training Epoch: 63 [2560/50000]\tLoss: 0.5444\tLR: 0.020000\n",
            "Training Epoch: 63 [2688/50000]\tLoss: 0.5405\tLR: 0.020000\n",
            "Training Epoch: 63 [2816/50000]\tLoss: 0.3695\tLR: 0.020000\n",
            "Training Epoch: 63 [2944/50000]\tLoss: 0.4319\tLR: 0.020000\n",
            "Training Epoch: 63 [3072/50000]\tLoss: 0.3899\tLR: 0.020000\n",
            "Training Epoch: 63 [3200/50000]\tLoss: 0.3702\tLR: 0.020000\n",
            "Training Epoch: 63 [3328/50000]\tLoss: 0.3931\tLR: 0.020000\n",
            "Training Epoch: 63 [3456/50000]\tLoss: 0.3642\tLR: 0.020000\n",
            "Training Epoch: 63 [3584/50000]\tLoss: 0.4437\tLR: 0.020000\n",
            "Training Epoch: 63 [3712/50000]\tLoss: 0.4676\tLR: 0.020000\n",
            "Training Epoch: 63 [3840/50000]\tLoss: 0.3831\tLR: 0.020000\n",
            "Training Epoch: 63 [3968/50000]\tLoss: 0.5086\tLR: 0.020000\n",
            "Training Epoch: 63 [4096/50000]\tLoss: 0.4450\tLR: 0.020000\n",
            "Training Epoch: 63 [4224/50000]\tLoss: 0.4419\tLR: 0.020000\n",
            "Training Epoch: 63 [4352/50000]\tLoss: 0.5930\tLR: 0.020000\n",
            "Training Epoch: 63 [4480/50000]\tLoss: 0.5014\tLR: 0.020000\n",
            "Training Epoch: 63 [4608/50000]\tLoss: 0.5019\tLR: 0.020000\n",
            "Training Epoch: 63 [4736/50000]\tLoss: 0.4141\tLR: 0.020000\n",
            "Training Epoch: 63 [4864/50000]\tLoss: 0.3447\tLR: 0.020000\n",
            "Training Epoch: 63 [4992/50000]\tLoss: 0.4528\tLR: 0.020000\n",
            "Training Epoch: 63 [5120/50000]\tLoss: 0.3136\tLR: 0.020000\n",
            "Training Epoch: 63 [5248/50000]\tLoss: 0.3619\tLR: 0.020000\n",
            "Training Epoch: 63 [5376/50000]\tLoss: 0.3652\tLR: 0.020000\n",
            "Training Epoch: 63 [5504/50000]\tLoss: 0.4982\tLR: 0.020000\n",
            "Training Epoch: 63 [5632/50000]\tLoss: 0.4108\tLR: 0.020000\n",
            "Training Epoch: 63 [5760/50000]\tLoss: 0.3776\tLR: 0.020000\n",
            "Training Epoch: 63 [5888/50000]\tLoss: 0.4477\tLR: 0.020000\n",
            "Training Epoch: 63 [6016/50000]\tLoss: 0.3471\tLR: 0.020000\n",
            "Training Epoch: 63 [6144/50000]\tLoss: 0.4345\tLR: 0.020000\n",
            "Training Epoch: 63 [6272/50000]\tLoss: 0.3450\tLR: 0.020000\n",
            "Training Epoch: 63 [6400/50000]\tLoss: 0.3819\tLR: 0.020000\n",
            "Training Epoch: 63 [6528/50000]\tLoss: 0.4714\tLR: 0.020000\n",
            "Training Epoch: 63 [6656/50000]\tLoss: 0.4720\tLR: 0.020000\n",
            "Training Epoch: 63 [6784/50000]\tLoss: 0.3770\tLR: 0.020000\n",
            "Training Epoch: 63 [6912/50000]\tLoss: 0.3376\tLR: 0.020000\n",
            "Training Epoch: 63 [7040/50000]\tLoss: 0.4163\tLR: 0.020000\n",
            "Training Epoch: 63 [7168/50000]\tLoss: 0.3807\tLR: 0.020000\n",
            "Training Epoch: 63 [7296/50000]\tLoss: 0.4884\tLR: 0.020000\n",
            "Training Epoch: 63 [7424/50000]\tLoss: 0.3137\tLR: 0.020000\n",
            "Training Epoch: 63 [7552/50000]\tLoss: 0.2796\tLR: 0.020000\n",
            "Training Epoch: 63 [7680/50000]\tLoss: 0.4874\tLR: 0.020000\n",
            "Training Epoch: 63 [7808/50000]\tLoss: 0.4206\tLR: 0.020000\n",
            "Training Epoch: 63 [7936/50000]\tLoss: 0.4462\tLR: 0.020000\n",
            "Training Epoch: 63 [8064/50000]\tLoss: 0.3990\tLR: 0.020000\n",
            "Training Epoch: 63 [8192/50000]\tLoss: 0.3996\tLR: 0.020000\n",
            "Training Epoch: 63 [8320/50000]\tLoss: 0.4528\tLR: 0.020000\n",
            "Training Epoch: 63 [8448/50000]\tLoss: 0.4859\tLR: 0.020000\n",
            "Training Epoch: 63 [8576/50000]\tLoss: 0.4750\tLR: 0.020000\n",
            "Training Epoch: 63 [8704/50000]\tLoss: 0.4961\tLR: 0.020000\n",
            "Training Epoch: 63 [8832/50000]\tLoss: 0.3436\tLR: 0.020000\n",
            "Training Epoch: 63 [8960/50000]\tLoss: 0.4572\tLR: 0.020000\n",
            "Training Epoch: 63 [9088/50000]\tLoss: 0.4614\tLR: 0.020000\n",
            "Training Epoch: 63 [9216/50000]\tLoss: 0.4107\tLR: 0.020000\n",
            "Training Epoch: 63 [9344/50000]\tLoss: 0.3291\tLR: 0.020000\n",
            "Training Epoch: 63 [9472/50000]\tLoss: 0.4155\tLR: 0.020000\n",
            "Training Epoch: 63 [9600/50000]\tLoss: 0.4130\tLR: 0.020000\n",
            "Training Epoch: 63 [9728/50000]\tLoss: 0.4949\tLR: 0.020000\n",
            "Training Epoch: 63 [9856/50000]\tLoss: 0.4037\tLR: 0.020000\n",
            "Training Epoch: 63 [9984/50000]\tLoss: 0.5569\tLR: 0.020000\n",
            "Training Epoch: 63 [10112/50000]\tLoss: 0.3901\tLR: 0.020000\n",
            "Training Epoch: 63 [10240/50000]\tLoss: 0.3657\tLR: 0.020000\n",
            "Training Epoch: 63 [10368/50000]\tLoss: 0.4417\tLR: 0.020000\n",
            "Training Epoch: 63 [10496/50000]\tLoss: 0.4134\tLR: 0.020000\n",
            "Training Epoch: 63 [10624/50000]\tLoss: 0.3646\tLR: 0.020000\n",
            "Training Epoch: 63 [10752/50000]\tLoss: 0.3858\tLR: 0.020000\n",
            "Training Epoch: 63 [10880/50000]\tLoss: 0.4583\tLR: 0.020000\n",
            "Training Epoch: 63 [11008/50000]\tLoss: 0.4916\tLR: 0.020000\n",
            "Training Epoch: 63 [11136/50000]\tLoss: 0.3546\tLR: 0.020000\n",
            "Training Epoch: 63 [11264/50000]\tLoss: 0.3735\tLR: 0.020000\n",
            "Training Epoch: 63 [11392/50000]\tLoss: 0.4615\tLR: 0.020000\n",
            "Training Epoch: 63 [11520/50000]\tLoss: 0.4754\tLR: 0.020000\n",
            "Training Epoch: 63 [11648/50000]\tLoss: 0.5050\tLR: 0.020000\n",
            "Training Epoch: 63 [11776/50000]\tLoss: 0.5093\tLR: 0.020000\n",
            "Training Epoch: 63 [11904/50000]\tLoss: 0.4370\tLR: 0.020000\n",
            "Training Epoch: 63 [12032/50000]\tLoss: 0.4419\tLR: 0.020000\n",
            "Training Epoch: 63 [12160/50000]\tLoss: 0.4323\tLR: 0.020000\n",
            "Training Epoch: 63 [12288/50000]\tLoss: 0.4838\tLR: 0.020000\n",
            "Training Epoch: 63 [12416/50000]\tLoss: 0.4787\tLR: 0.020000\n",
            "Training Epoch: 63 [12544/50000]\tLoss: 0.6223\tLR: 0.020000\n",
            "Training Epoch: 63 [12672/50000]\tLoss: 0.3442\tLR: 0.020000\n",
            "Training Epoch: 63 [12800/50000]\tLoss: 0.4231\tLR: 0.020000\n",
            "Training Epoch: 63 [12928/50000]\tLoss: 0.3610\tLR: 0.020000\n",
            "Training Epoch: 63 [13056/50000]\tLoss: 0.4674\tLR: 0.020000\n",
            "Training Epoch: 63 [13184/50000]\tLoss: 0.4698\tLR: 0.020000\n",
            "Training Epoch: 63 [13312/50000]\tLoss: 0.4580\tLR: 0.020000\n",
            "Training Epoch: 63 [13440/50000]\tLoss: 0.5227\tLR: 0.020000\n",
            "Training Epoch: 63 [13568/50000]\tLoss: 0.4536\tLR: 0.020000\n",
            "Training Epoch: 63 [13696/50000]\tLoss: 0.3706\tLR: 0.020000\n",
            "Training Epoch: 63 [13824/50000]\tLoss: 0.2884\tLR: 0.020000\n",
            "Training Epoch: 63 [13952/50000]\tLoss: 0.4626\tLR: 0.020000\n",
            "Training Epoch: 63 [14080/50000]\tLoss: 0.3988\tLR: 0.020000\n",
            "Training Epoch: 63 [14208/50000]\tLoss: 0.4883\tLR: 0.020000\n",
            "Training Epoch: 63 [14336/50000]\tLoss: 0.4388\tLR: 0.020000\n",
            "Training Epoch: 63 [14464/50000]\tLoss: 0.4322\tLR: 0.020000\n",
            "Training Epoch: 63 [14592/50000]\tLoss: 0.4889\tLR: 0.020000\n",
            "Training Epoch: 63 [14720/50000]\tLoss: 0.4546\tLR: 0.020000\n",
            "Training Epoch: 63 [14848/50000]\tLoss: 0.4235\tLR: 0.020000\n",
            "Training Epoch: 63 [14976/50000]\tLoss: 0.2923\tLR: 0.020000\n",
            "Training Epoch: 63 [15104/50000]\tLoss: 0.3194\tLR: 0.020000\n",
            "Training Epoch: 63 [15232/50000]\tLoss: 0.4920\tLR: 0.020000\n",
            "Training Epoch: 63 [15360/50000]\tLoss: 0.3608\tLR: 0.020000\n",
            "Training Epoch: 63 [15488/50000]\tLoss: 0.3037\tLR: 0.020000\n",
            "Training Epoch: 63 [15616/50000]\tLoss: 0.3649\tLR: 0.020000\n",
            "Training Epoch: 63 [15744/50000]\tLoss: 0.3014\tLR: 0.020000\n",
            "Training Epoch: 63 [15872/50000]\tLoss: 0.4638\tLR: 0.020000\n",
            "Training Epoch: 63 [16000/50000]\tLoss: 0.5811\tLR: 0.020000\n",
            "Training Epoch: 63 [16128/50000]\tLoss: 0.3295\tLR: 0.020000\n",
            "Training Epoch: 63 [16256/50000]\tLoss: 0.3796\tLR: 0.020000\n",
            "Training Epoch: 63 [16384/50000]\tLoss: 0.4205\tLR: 0.020000\n",
            "Training Epoch: 63 [16512/50000]\tLoss: 0.5659\tLR: 0.020000\n",
            "Training Epoch: 63 [16640/50000]\tLoss: 0.3937\tLR: 0.020000\n",
            "Training Epoch: 63 [16768/50000]\tLoss: 0.4183\tLR: 0.020000\n",
            "Training Epoch: 63 [16896/50000]\tLoss: 0.4519\tLR: 0.020000\n",
            "Training Epoch: 63 [17024/50000]\tLoss: 0.4483\tLR: 0.020000\n",
            "Training Epoch: 63 [17152/50000]\tLoss: 0.4127\tLR: 0.020000\n",
            "Training Epoch: 63 [17280/50000]\tLoss: 0.4474\tLR: 0.020000\n",
            "Training Epoch: 63 [17408/50000]\tLoss: 0.3952\tLR: 0.020000\n",
            "Training Epoch: 63 [17536/50000]\tLoss: 0.5187\tLR: 0.020000\n",
            "Training Epoch: 63 [17664/50000]\tLoss: 0.4594\tLR: 0.020000\n",
            "Training Epoch: 63 [17792/50000]\tLoss: 0.4919\tLR: 0.020000\n",
            "Training Epoch: 63 [17920/50000]\tLoss: 0.3855\tLR: 0.020000\n",
            "Training Epoch: 63 [18048/50000]\tLoss: 0.5303\tLR: 0.020000\n",
            "Training Epoch: 63 [18176/50000]\tLoss: 0.4326\tLR: 0.020000\n",
            "Training Epoch: 63 [18304/50000]\tLoss: 0.4968\tLR: 0.020000\n",
            "Training Epoch: 63 [18432/50000]\tLoss: 0.3849\tLR: 0.020000\n",
            "Training Epoch: 63 [18560/50000]\tLoss: 0.3848\tLR: 0.020000\n",
            "Training Epoch: 63 [18688/50000]\tLoss: 0.4226\tLR: 0.020000\n",
            "Training Epoch: 63 [18816/50000]\tLoss: 0.4455\tLR: 0.020000\n",
            "Training Epoch: 63 [18944/50000]\tLoss: 0.3586\tLR: 0.020000\n",
            "Training Epoch: 63 [19072/50000]\tLoss: 0.5563\tLR: 0.020000\n",
            "Training Epoch: 63 [19200/50000]\tLoss: 0.4256\tLR: 0.020000\n",
            "Training Epoch: 63 [19328/50000]\tLoss: 0.4152\tLR: 0.020000\n",
            "Training Epoch: 63 [19456/50000]\tLoss: 0.3790\tLR: 0.020000\n",
            "Training Epoch: 63 [19584/50000]\tLoss: 0.4258\tLR: 0.020000\n",
            "Training Epoch: 63 [19712/50000]\tLoss: 0.3747\tLR: 0.020000\n",
            "Training Epoch: 63 [19840/50000]\tLoss: 0.3904\tLR: 0.020000\n",
            "Training Epoch: 63 [19968/50000]\tLoss: 0.3667\tLR: 0.020000\n",
            "Training Epoch: 63 [20096/50000]\tLoss: 0.5618\tLR: 0.020000\n",
            "Training Epoch: 63 [20224/50000]\tLoss: 0.5430\tLR: 0.020000\n",
            "Training Epoch: 63 [20352/50000]\tLoss: 0.4312\tLR: 0.020000\n",
            "Training Epoch: 63 [20480/50000]\tLoss: 0.5297\tLR: 0.020000\n",
            "Training Epoch: 63 [20608/50000]\tLoss: 0.5337\tLR: 0.020000\n",
            "Training Epoch: 63 [20736/50000]\tLoss: 0.4595\tLR: 0.020000\n",
            "Training Epoch: 63 [20864/50000]\tLoss: 0.3505\tLR: 0.020000\n",
            "Training Epoch: 63 [20992/50000]\tLoss: 0.4106\tLR: 0.020000\n",
            "Training Epoch: 63 [21120/50000]\tLoss: 0.3692\tLR: 0.020000\n",
            "Training Epoch: 63 [21248/50000]\tLoss: 0.3028\tLR: 0.020000\n",
            "Training Epoch: 63 [21376/50000]\tLoss: 0.3744\tLR: 0.020000\n",
            "Training Epoch: 63 [21504/50000]\tLoss: 0.5608\tLR: 0.020000\n",
            "Training Epoch: 63 [21632/50000]\tLoss: 0.4356\tLR: 0.020000\n",
            "Training Epoch: 63 [21760/50000]\tLoss: 0.5376\tLR: 0.020000\n",
            "Training Epoch: 63 [21888/50000]\tLoss: 0.6229\tLR: 0.020000\n",
            "Training Epoch: 63 [22016/50000]\tLoss: 0.4793\tLR: 0.020000\n",
            "Training Epoch: 63 [22144/50000]\tLoss: 0.5442\tLR: 0.020000\n",
            "Training Epoch: 63 [22272/50000]\tLoss: 0.3725\tLR: 0.020000\n",
            "Training Epoch: 63 [22400/50000]\tLoss: 0.5423\tLR: 0.020000\n",
            "Training Epoch: 63 [22528/50000]\tLoss: 0.5051\tLR: 0.020000\n",
            "Training Epoch: 63 [22656/50000]\tLoss: 0.4355\tLR: 0.020000\n",
            "Training Epoch: 63 [22784/50000]\tLoss: 0.3562\tLR: 0.020000\n",
            "Training Epoch: 63 [22912/50000]\tLoss: 0.6026\tLR: 0.020000\n",
            "Training Epoch: 63 [23040/50000]\tLoss: 0.4917\tLR: 0.020000\n",
            "Training Epoch: 63 [23168/50000]\tLoss: 0.3287\tLR: 0.020000\n",
            "Training Epoch: 63 [23296/50000]\tLoss: 0.3623\tLR: 0.020000\n",
            "Training Epoch: 63 [23424/50000]\tLoss: 0.3726\tLR: 0.020000\n",
            "Training Epoch: 63 [23552/50000]\tLoss: 0.5428\tLR: 0.020000\n",
            "Training Epoch: 63 [23680/50000]\tLoss: 0.5583\tLR: 0.020000\n",
            "Training Epoch: 63 [23808/50000]\tLoss: 0.5559\tLR: 0.020000\n",
            "Training Epoch: 63 [23936/50000]\tLoss: 0.5680\tLR: 0.020000\n",
            "Training Epoch: 63 [24064/50000]\tLoss: 0.4515\tLR: 0.020000\n",
            "Training Epoch: 63 [24192/50000]\tLoss: 0.5449\tLR: 0.020000\n",
            "Training Epoch: 63 [24320/50000]\tLoss: 0.4141\tLR: 0.020000\n",
            "Training Epoch: 63 [24448/50000]\tLoss: 0.4392\tLR: 0.020000\n",
            "Training Epoch: 63 [24576/50000]\tLoss: 0.4961\tLR: 0.020000\n",
            "Training Epoch: 63 [24704/50000]\tLoss: 0.4171\tLR: 0.020000\n",
            "Training Epoch: 63 [24832/50000]\tLoss: 0.5829\tLR: 0.020000\n",
            "Training Epoch: 63 [24960/50000]\tLoss: 0.4410\tLR: 0.020000\n",
            "Training Epoch: 63 [25088/50000]\tLoss: 0.4701\tLR: 0.020000\n",
            "Training Epoch: 63 [25216/50000]\tLoss: 0.4911\tLR: 0.020000\n",
            "Training Epoch: 63 [25344/50000]\tLoss: 0.3310\tLR: 0.020000\n",
            "Training Epoch: 63 [25472/50000]\tLoss: 0.5528\tLR: 0.020000\n",
            "Training Epoch: 63 [25600/50000]\tLoss: 0.4409\tLR: 0.020000\n",
            "Training Epoch: 63 [25728/50000]\tLoss: 0.4381\tLR: 0.020000\n",
            "Training Epoch: 63 [25856/50000]\tLoss: 0.5065\tLR: 0.020000\n",
            "Training Epoch: 63 [25984/50000]\tLoss: 0.3172\tLR: 0.020000\n",
            "Training Epoch: 63 [26112/50000]\tLoss: 0.4884\tLR: 0.020000\n",
            "Training Epoch: 63 [26240/50000]\tLoss: 0.5057\tLR: 0.020000\n",
            "Training Epoch: 63 [26368/50000]\tLoss: 0.3528\tLR: 0.020000\n",
            "Training Epoch: 63 [26496/50000]\tLoss: 0.3382\tLR: 0.020000\n",
            "Training Epoch: 63 [26624/50000]\tLoss: 0.4040\tLR: 0.020000\n",
            "Training Epoch: 63 [26752/50000]\tLoss: 0.3303\tLR: 0.020000\n",
            "Training Epoch: 63 [26880/50000]\tLoss: 0.5263\tLR: 0.020000\n",
            "Training Epoch: 63 [27008/50000]\tLoss: 0.4717\tLR: 0.020000\n",
            "Training Epoch: 63 [27136/50000]\tLoss: 0.6001\tLR: 0.020000\n",
            "Training Epoch: 63 [27264/50000]\tLoss: 0.5192\tLR: 0.020000\n",
            "Training Epoch: 63 [27392/50000]\tLoss: 0.4934\tLR: 0.020000\n",
            "Training Epoch: 63 [27520/50000]\tLoss: 0.3943\tLR: 0.020000\n",
            "Training Epoch: 63 [27648/50000]\tLoss: 0.5040\tLR: 0.020000\n",
            "Training Epoch: 63 [27776/50000]\tLoss: 0.3521\tLR: 0.020000\n",
            "Training Epoch: 63 [27904/50000]\tLoss: 0.5100\tLR: 0.020000\n",
            "Training Epoch: 63 [28032/50000]\tLoss: 0.4498\tLR: 0.020000\n",
            "Training Epoch: 63 [28160/50000]\tLoss: 0.4690\tLR: 0.020000\n",
            "Training Epoch: 63 [28288/50000]\tLoss: 0.3559\tLR: 0.020000\n",
            "Training Epoch: 63 [28416/50000]\tLoss: 0.4626\tLR: 0.020000\n",
            "Training Epoch: 63 [28544/50000]\tLoss: 0.4490\tLR: 0.020000\n",
            "Training Epoch: 63 [28672/50000]\tLoss: 0.3867\tLR: 0.020000\n",
            "Training Epoch: 63 [28800/50000]\tLoss: 0.5179\tLR: 0.020000\n",
            "Training Epoch: 63 [28928/50000]\tLoss: 0.5641\tLR: 0.020000\n",
            "Training Epoch: 63 [29056/50000]\tLoss: 0.3784\tLR: 0.020000\n",
            "Training Epoch: 63 [29184/50000]\tLoss: 0.5229\tLR: 0.020000\n",
            "Training Epoch: 63 [29312/50000]\tLoss: 0.4299\tLR: 0.020000\n",
            "Training Epoch: 63 [29440/50000]\tLoss: 0.5400\tLR: 0.020000\n",
            "Training Epoch: 63 [29568/50000]\tLoss: 0.4574\tLR: 0.020000\n",
            "Training Epoch: 63 [29696/50000]\tLoss: 0.3762\tLR: 0.020000\n",
            "Training Epoch: 63 [29824/50000]\tLoss: 0.4707\tLR: 0.020000\n",
            "Training Epoch: 63 [29952/50000]\tLoss: 0.6339\tLR: 0.020000\n",
            "Training Epoch: 63 [30080/50000]\tLoss: 0.5035\tLR: 0.020000\n",
            "Training Epoch: 63 [30208/50000]\tLoss: 0.5059\tLR: 0.020000\n",
            "Training Epoch: 63 [30336/50000]\tLoss: 0.3550\tLR: 0.020000\n",
            "Training Epoch: 63 [30464/50000]\tLoss: 0.3422\tLR: 0.020000\n",
            "Training Epoch: 63 [30592/50000]\tLoss: 0.6183\tLR: 0.020000\n",
            "Training Epoch: 63 [30720/50000]\tLoss: 0.5075\tLR: 0.020000\n",
            "Training Epoch: 63 [30848/50000]\tLoss: 0.5193\tLR: 0.020000\n",
            "Training Epoch: 63 [30976/50000]\tLoss: 0.5552\tLR: 0.020000\n",
            "Training Epoch: 63 [31104/50000]\tLoss: 0.4772\tLR: 0.020000\n",
            "Training Epoch: 63 [31232/50000]\tLoss: 0.5424\tLR: 0.020000\n",
            "Training Epoch: 63 [31360/50000]\tLoss: 0.3139\tLR: 0.020000\n",
            "Training Epoch: 63 [31488/50000]\tLoss: 0.2697\tLR: 0.020000\n",
            "Training Epoch: 63 [31616/50000]\tLoss: 0.3817\tLR: 0.020000\n",
            "Training Epoch: 63 [31744/50000]\tLoss: 0.3884\tLR: 0.020000\n",
            "Training Epoch: 63 [31872/50000]\tLoss: 0.4829\tLR: 0.020000\n",
            "Training Epoch: 63 [32000/50000]\tLoss: 0.4152\tLR: 0.020000\n",
            "Training Epoch: 63 [32128/50000]\tLoss: 0.4921\tLR: 0.020000\n",
            "Training Epoch: 63 [32256/50000]\tLoss: 0.3971\tLR: 0.020000\n",
            "Training Epoch: 63 [32384/50000]\tLoss: 0.5210\tLR: 0.020000\n",
            "Training Epoch: 63 [32512/50000]\tLoss: 0.4431\tLR: 0.020000\n",
            "Training Epoch: 63 [32640/50000]\tLoss: 0.3477\tLR: 0.020000\n",
            "Training Epoch: 63 [32768/50000]\tLoss: 0.4149\tLR: 0.020000\n",
            "Training Epoch: 63 [32896/50000]\tLoss: 0.5081\tLR: 0.020000\n",
            "Training Epoch: 63 [33024/50000]\tLoss: 0.5102\tLR: 0.020000\n",
            "Training Epoch: 63 [33152/50000]\tLoss: 0.4557\tLR: 0.020000\n",
            "Training Epoch: 63 [33280/50000]\tLoss: 0.4402\tLR: 0.020000\n",
            "Training Epoch: 63 [33408/50000]\tLoss: 0.6383\tLR: 0.020000\n",
            "Training Epoch: 63 [33536/50000]\tLoss: 0.4931\tLR: 0.020000\n",
            "Training Epoch: 63 [33664/50000]\tLoss: 0.5319\tLR: 0.020000\n",
            "Training Epoch: 63 [33792/50000]\tLoss: 0.3900\tLR: 0.020000\n",
            "Training Epoch: 63 [33920/50000]\tLoss: 0.4827\tLR: 0.020000\n",
            "Training Epoch: 63 [34048/50000]\tLoss: 0.3104\tLR: 0.020000\n",
            "Training Epoch: 63 [34176/50000]\tLoss: 0.5340\tLR: 0.020000\n",
            "Training Epoch: 63 [34304/50000]\tLoss: 0.4937\tLR: 0.020000\n",
            "Training Epoch: 63 [34432/50000]\tLoss: 0.6536\tLR: 0.020000\n",
            "Training Epoch: 63 [34560/50000]\tLoss: 0.5985\tLR: 0.020000\n",
            "Training Epoch: 63 [34688/50000]\tLoss: 0.5021\tLR: 0.020000\n",
            "Training Epoch: 63 [34816/50000]\tLoss: 0.5039\tLR: 0.020000\n",
            "Training Epoch: 63 [34944/50000]\tLoss: 0.3276\tLR: 0.020000\n",
            "Training Epoch: 63 [35072/50000]\tLoss: 0.3576\tLR: 0.020000\n",
            "Training Epoch: 63 [35200/50000]\tLoss: 0.3760\tLR: 0.020000\n",
            "Training Epoch: 63 [35328/50000]\tLoss: 0.5470\tLR: 0.020000\n",
            "Training Epoch: 63 [35456/50000]\tLoss: 0.6558\tLR: 0.020000\n",
            "Training Epoch: 63 [35584/50000]\tLoss: 0.4048\tLR: 0.020000\n",
            "Training Epoch: 63 [35712/50000]\tLoss: 0.4742\tLR: 0.020000\n",
            "Training Epoch: 63 [35840/50000]\tLoss: 0.5094\tLR: 0.020000\n",
            "Training Epoch: 63 [35968/50000]\tLoss: 0.4146\tLR: 0.020000\n",
            "Training Epoch: 63 [36096/50000]\tLoss: 0.4362\tLR: 0.020000\n",
            "Training Epoch: 63 [36224/50000]\tLoss: 0.4960\tLR: 0.020000\n",
            "Training Epoch: 63 [36352/50000]\tLoss: 0.5219\tLR: 0.020000\n",
            "Training Epoch: 63 [36480/50000]\tLoss: 0.5538\tLR: 0.020000\n",
            "Training Epoch: 63 [36608/50000]\tLoss: 0.4259\tLR: 0.020000\n",
            "Training Epoch: 63 [36736/50000]\tLoss: 0.4016\tLR: 0.020000\n",
            "Training Epoch: 63 [36864/50000]\tLoss: 0.4898\tLR: 0.020000\n",
            "Training Epoch: 63 [36992/50000]\tLoss: 0.5159\tLR: 0.020000\n",
            "Training Epoch: 63 [37120/50000]\tLoss: 0.4268\tLR: 0.020000\n",
            "Training Epoch: 63 [37248/50000]\tLoss: 0.5157\tLR: 0.020000\n",
            "Training Epoch: 63 [37376/50000]\tLoss: 0.5406\tLR: 0.020000\n",
            "Training Epoch: 63 [37504/50000]\tLoss: 0.5762\tLR: 0.020000\n",
            "Training Epoch: 63 [37632/50000]\tLoss: 0.4367\tLR: 0.020000\n",
            "Training Epoch: 63 [37760/50000]\tLoss: 0.3363\tLR: 0.020000\n",
            "Training Epoch: 63 [37888/50000]\tLoss: 0.5042\tLR: 0.020000\n",
            "Training Epoch: 63 [38016/50000]\tLoss: 0.3718\tLR: 0.020000\n",
            "Training Epoch: 63 [38144/50000]\tLoss: 0.4483\tLR: 0.020000\n",
            "Training Epoch: 63 [38272/50000]\tLoss: 0.5120\tLR: 0.020000\n",
            "Training Epoch: 63 [38400/50000]\tLoss: 0.3048\tLR: 0.020000\n",
            "Training Epoch: 63 [38528/50000]\tLoss: 0.5671\tLR: 0.020000\n",
            "Training Epoch: 63 [38656/50000]\tLoss: 0.5512\tLR: 0.020000\n",
            "Training Epoch: 63 [38784/50000]\tLoss: 0.4935\tLR: 0.020000\n",
            "Training Epoch: 63 [38912/50000]\tLoss: 0.4023\tLR: 0.020000\n",
            "Training Epoch: 63 [39040/50000]\tLoss: 0.4862\tLR: 0.020000\n",
            "Training Epoch: 63 [39168/50000]\tLoss: 0.4102\tLR: 0.020000\n",
            "Training Epoch: 63 [39296/50000]\tLoss: 0.5308\tLR: 0.020000\n",
            "Training Epoch: 63 [39424/50000]\tLoss: 0.3375\tLR: 0.020000\n",
            "Training Epoch: 63 [39552/50000]\tLoss: 0.5108\tLR: 0.020000\n",
            "Training Epoch: 63 [39680/50000]\tLoss: 0.3812\tLR: 0.020000\n",
            "Training Epoch: 63 [39808/50000]\tLoss: 0.4221\tLR: 0.020000\n",
            "Training Epoch: 63 [39936/50000]\tLoss: 0.5170\tLR: 0.020000\n",
            "Training Epoch: 63 [40064/50000]\tLoss: 0.4197\tLR: 0.020000\n",
            "Training Epoch: 63 [40192/50000]\tLoss: 0.5898\tLR: 0.020000\n",
            "Training Epoch: 63 [40320/50000]\tLoss: 0.5235\tLR: 0.020000\n",
            "Training Epoch: 63 [40448/50000]\tLoss: 0.4502\tLR: 0.020000\n",
            "Training Epoch: 63 [40576/50000]\tLoss: 0.4085\tLR: 0.020000\n",
            "Training Epoch: 63 [40704/50000]\tLoss: 0.3392\tLR: 0.020000\n",
            "Training Epoch: 63 [40832/50000]\tLoss: 0.5763\tLR: 0.020000\n",
            "Training Epoch: 63 [40960/50000]\tLoss: 0.5532\tLR: 0.020000\n",
            "Training Epoch: 63 [41088/50000]\tLoss: 0.4100\tLR: 0.020000\n",
            "Training Epoch: 63 [41216/50000]\tLoss: 0.5310\tLR: 0.020000\n",
            "Training Epoch: 63 [41344/50000]\tLoss: 0.3712\tLR: 0.020000\n",
            "Training Epoch: 63 [41472/50000]\tLoss: 0.5270\tLR: 0.020000\n",
            "Training Epoch: 63 [41600/50000]\tLoss: 0.4292\tLR: 0.020000\n",
            "Training Epoch: 63 [41728/50000]\tLoss: 0.3470\tLR: 0.020000\n",
            "Training Epoch: 63 [41856/50000]\tLoss: 0.4210\tLR: 0.020000\n",
            "Training Epoch: 63 [41984/50000]\tLoss: 0.5359\tLR: 0.020000\n",
            "Training Epoch: 63 [42112/50000]\tLoss: 0.3189\tLR: 0.020000\n",
            "Training Epoch: 63 [42240/50000]\tLoss: 0.4244\tLR: 0.020000\n",
            "Training Epoch: 63 [42368/50000]\tLoss: 0.3739\tLR: 0.020000\n",
            "Training Epoch: 63 [42496/50000]\tLoss: 0.4312\tLR: 0.020000\n",
            "Training Epoch: 63 [42624/50000]\tLoss: 0.3147\tLR: 0.020000\n",
            "Training Epoch: 63 [42752/50000]\tLoss: 0.5619\tLR: 0.020000\n",
            "Training Epoch: 63 [42880/50000]\tLoss: 0.3590\tLR: 0.020000\n",
            "Training Epoch: 63 [43008/50000]\tLoss: 0.5637\tLR: 0.020000\n",
            "Training Epoch: 63 [43136/50000]\tLoss: 0.4298\tLR: 0.020000\n",
            "Training Epoch: 63 [43264/50000]\tLoss: 0.4065\tLR: 0.020000\n",
            "Training Epoch: 63 [43392/50000]\tLoss: 0.4842\tLR: 0.020000\n",
            "Training Epoch: 63 [43520/50000]\tLoss: 0.4387\tLR: 0.020000\n",
            "Training Epoch: 63 [43648/50000]\tLoss: 0.5534\tLR: 0.020000\n",
            "Training Epoch: 63 [43776/50000]\tLoss: 0.4337\tLR: 0.020000\n",
            "Training Epoch: 63 [43904/50000]\tLoss: 0.5191\tLR: 0.020000\n",
            "Training Epoch: 63 [44032/50000]\tLoss: 0.3830\tLR: 0.020000\n",
            "Training Epoch: 63 [44160/50000]\tLoss: 0.4284\tLR: 0.020000\n",
            "Training Epoch: 63 [44288/50000]\tLoss: 0.3638\tLR: 0.020000\n",
            "Training Epoch: 63 [44416/50000]\tLoss: 0.3462\tLR: 0.020000\n",
            "Training Epoch: 63 [44544/50000]\tLoss: 0.5851\tLR: 0.020000\n",
            "Training Epoch: 63 [44672/50000]\tLoss: 0.4647\tLR: 0.020000\n",
            "Training Epoch: 63 [44800/50000]\tLoss: 0.4099\tLR: 0.020000\n",
            "Training Epoch: 63 [44928/50000]\tLoss: 0.5674\tLR: 0.020000\n",
            "Training Epoch: 63 [45056/50000]\tLoss: 0.4926\tLR: 0.020000\n",
            "Training Epoch: 63 [45184/50000]\tLoss: 0.4084\tLR: 0.020000\n",
            "Training Epoch: 63 [45312/50000]\tLoss: 0.3742\tLR: 0.020000\n",
            "Training Epoch: 63 [45440/50000]\tLoss: 0.3865\tLR: 0.020000\n",
            "Training Epoch: 63 [45568/50000]\tLoss: 0.5054\tLR: 0.020000\n",
            "Training Epoch: 63 [45696/50000]\tLoss: 0.4107\tLR: 0.020000\n",
            "Training Epoch: 63 [45824/50000]\tLoss: 0.4990\tLR: 0.020000\n",
            "Training Epoch: 63 [45952/50000]\tLoss: 0.5471\tLR: 0.020000\n",
            "Training Epoch: 63 [46080/50000]\tLoss: 0.5733\tLR: 0.020000\n",
            "Training Epoch: 63 [46208/50000]\tLoss: 0.3614\tLR: 0.020000\n",
            "Training Epoch: 63 [46336/50000]\tLoss: 0.4538\tLR: 0.020000\n",
            "Training Epoch: 63 [46464/50000]\tLoss: 0.4288\tLR: 0.020000\n",
            "Training Epoch: 63 [46592/50000]\tLoss: 0.4816\tLR: 0.020000\n",
            "Training Epoch: 63 [46720/50000]\tLoss: 0.4043\tLR: 0.020000\n",
            "Training Epoch: 63 [46848/50000]\tLoss: 0.5392\tLR: 0.020000\n",
            "Training Epoch: 63 [46976/50000]\tLoss: 0.4930\tLR: 0.020000\n",
            "Training Epoch: 63 [47104/50000]\tLoss: 0.5317\tLR: 0.020000\n",
            "Training Epoch: 63 [47232/50000]\tLoss: 0.4364\tLR: 0.020000\n",
            "Training Epoch: 63 [47360/50000]\tLoss: 0.4903\tLR: 0.020000\n",
            "Training Epoch: 63 [47488/50000]\tLoss: 0.5147\tLR: 0.020000\n",
            "Training Epoch: 63 [47616/50000]\tLoss: 0.5776\tLR: 0.020000\n",
            "Training Epoch: 63 [47744/50000]\tLoss: 0.5970\tLR: 0.020000\n",
            "Training Epoch: 63 [47872/50000]\tLoss: 0.4535\tLR: 0.020000\n",
            "Training Epoch: 63 [48000/50000]\tLoss: 0.4239\tLR: 0.020000\n",
            "Training Epoch: 63 [48128/50000]\tLoss: 0.4739\tLR: 0.020000\n",
            "Training Epoch: 63 [48256/50000]\tLoss: 0.5197\tLR: 0.020000\n",
            "Training Epoch: 63 [48384/50000]\tLoss: 0.3699\tLR: 0.020000\n",
            "Training Epoch: 63 [48512/50000]\tLoss: 0.5673\tLR: 0.020000\n",
            "Training Epoch: 63 [48640/50000]\tLoss: 0.6146\tLR: 0.020000\n",
            "Training Epoch: 63 [48768/50000]\tLoss: 0.4598\tLR: 0.020000\n",
            "Training Epoch: 63 [48896/50000]\tLoss: 0.5716\tLR: 0.020000\n",
            "Training Epoch: 63 [49024/50000]\tLoss: 0.5212\tLR: 0.020000\n",
            "Training Epoch: 63 [49152/50000]\tLoss: 0.5315\tLR: 0.020000\n",
            "Training Epoch: 63 [49280/50000]\tLoss: 0.5042\tLR: 0.020000\n",
            "Training Epoch: 63 [49408/50000]\tLoss: 0.4736\tLR: 0.020000\n",
            "Training Epoch: 63 [49536/50000]\tLoss: 0.4563\tLR: 0.020000\n",
            "Training Epoch: 63 [49664/50000]\tLoss: 0.5552\tLR: 0.020000\n",
            "Training Epoch: 63 [49792/50000]\tLoss: 0.5198\tLR: 0.020000\n",
            "Training Epoch: 63 [49920/50000]\tLoss: 0.4531\tLR: 0.020000\n",
            "Training Epoch: 63 [50000/50000]\tLoss: 0.3064\tLR: 0.020000\n",
            "epoch 63 training time consumed: 144.98s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  133678 KB |    1044 MB |  230872 GB |  230871 GB |\n",
            "|       from large pool |  122880 KB |    1034 MB |  230685 GB |  230685 GB |\n",
            "|       from small pool |   10798 KB |      13 MB |     186 GB |     186 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  133678 KB |    1044 MB |  230872 GB |  230871 GB |\n",
            "|       from large pool |  122880 KB |    1034 MB |  230685 GB |  230685 GB |\n",
            "|       from small pool |   10798 KB |      13 MB |     186 GB |     186 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    1838 MB |    1838 MB |    1838 MB |       0 B  |\n",
            "|       from large pool |    1822 MB |    1822 MB |    1822 MB |       0 B  |\n",
            "|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  433618 KB |  778793 KB |  107347 GB |  107346 GB |\n",
            "|       from large pool |  432128 KB |  776192 KB |  107160 GB |  107160 GB |\n",
            "|       from small pool |    1490 KB |    3494 KB |     186 GB |     186 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     255    |     335    |    9340 K  |    9340 K  |\n",
            "|       from large pool |      24    |      65    |    4578 K  |    4578 K  |\n",
            "|       from small pool |     231    |     274    |    4762 K  |    4761 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     255    |     335    |    9340 K  |    9340 K  |\n",
            "|       from large pool |      24    |      65    |    4578 K  |    4578 K  |\n",
            "|       from small pool |     231    |     274    |    4762 K  |    4761 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      20    |      20    |      20    |       0    |\n",
            "|       from large pool |      12    |      12    |      12    |       0    |\n",
            "|       from small pool |       8    |       8    |       8    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      35    |      45    |    5407 K  |    5407 K  |\n",
            "|       from large pool |      10    |      15    |    2072 K  |    2072 K  |\n",
            "|       from small pool |      25    |      35    |    3334 K  |    3334 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 63, Average loss: 0.0079, Accuracy: 0.7289, Time consumed:8.99s\n",
            "\n",
            "Training Epoch: 64 [128/50000]\tLoss: 0.3758\tLR: 0.020000\n",
            "Training Epoch: 64 [256/50000]\tLoss: 0.4097\tLR: 0.020000\n",
            "Training Epoch: 64 [384/50000]\tLoss: 0.4620\tLR: 0.020000\n",
            "Training Epoch: 64 [512/50000]\tLoss: 0.3437\tLR: 0.020000\n",
            "Training Epoch: 64 [640/50000]\tLoss: 0.4476\tLR: 0.020000\n",
            "Training Epoch: 64 [768/50000]\tLoss: 0.4426\tLR: 0.020000\n",
            "Training Epoch: 64 [896/50000]\tLoss: 0.3313\tLR: 0.020000\n",
            "Training Epoch: 64 [1024/50000]\tLoss: 0.4397\tLR: 0.020000\n",
            "Training Epoch: 64 [1152/50000]\tLoss: 0.4724\tLR: 0.020000\n",
            "Training Epoch: 64 [1280/50000]\tLoss: 0.3935\tLR: 0.020000\n",
            "Training Epoch: 64 [1408/50000]\tLoss: 0.3890\tLR: 0.020000\n",
            "Training Epoch: 64 [1536/50000]\tLoss: 0.3355\tLR: 0.020000\n",
            "Training Epoch: 64 [1664/50000]\tLoss: 0.3582\tLR: 0.020000\n",
            "Training Epoch: 64 [1792/50000]\tLoss: 0.3779\tLR: 0.020000\n",
            "Training Epoch: 64 [1920/50000]\tLoss: 0.4021\tLR: 0.020000\n",
            "Training Epoch: 64 [2048/50000]\tLoss: 0.3360\tLR: 0.020000\n",
            "Training Epoch: 64 [2176/50000]\tLoss: 0.5076\tLR: 0.020000\n",
            "Training Epoch: 64 [2304/50000]\tLoss: 0.3880\tLR: 0.020000\n",
            "Training Epoch: 64 [2432/50000]\tLoss: 0.2950\tLR: 0.020000\n",
            "Training Epoch: 64 [2560/50000]\tLoss: 0.3594\tLR: 0.020000\n",
            "Training Epoch: 64 [2688/50000]\tLoss: 0.3669\tLR: 0.020000\n",
            "Training Epoch: 64 [2816/50000]\tLoss: 0.2591\tLR: 0.020000\n",
            "Training Epoch: 64 [2944/50000]\tLoss: 0.4062\tLR: 0.020000\n",
            "Training Epoch: 64 [3072/50000]\tLoss: 0.3972\tLR: 0.020000\n",
            "Training Epoch: 64 [3200/50000]\tLoss: 0.3606\tLR: 0.020000\n",
            "Training Epoch: 64 [3328/50000]\tLoss: 0.4867\tLR: 0.020000\n",
            "Training Epoch: 64 [3456/50000]\tLoss: 0.4277\tLR: 0.020000\n",
            "Training Epoch: 64 [3584/50000]\tLoss: 0.4020\tLR: 0.020000\n",
            "Training Epoch: 64 [3712/50000]\tLoss: 0.3283\tLR: 0.020000\n",
            "Training Epoch: 64 [3840/50000]\tLoss: 0.3342\tLR: 0.020000\n",
            "Training Epoch: 64 [3968/50000]\tLoss: 0.3968\tLR: 0.020000\n",
            "Training Epoch: 64 [4096/50000]\tLoss: 0.2955\tLR: 0.020000\n",
            "Training Epoch: 64 [4224/50000]\tLoss: 0.3012\tLR: 0.020000\n",
            "Training Epoch: 64 [4352/50000]\tLoss: 0.3171\tLR: 0.020000\n",
            "Training Epoch: 64 [4480/50000]\tLoss: 0.4032\tLR: 0.020000\n",
            "Training Epoch: 64 [4608/50000]\tLoss: 0.4086\tLR: 0.020000\n",
            "Training Epoch: 64 [4736/50000]\tLoss: 0.3153\tLR: 0.020000\n",
            "Training Epoch: 64 [4864/50000]\tLoss: 0.3946\tLR: 0.020000\n",
            "Training Epoch: 64 [4992/50000]\tLoss: 0.3516\tLR: 0.020000\n",
            "Training Epoch: 64 [5120/50000]\tLoss: 0.3023\tLR: 0.020000\n",
            "Training Epoch: 64 [5248/50000]\tLoss: 0.3711\tLR: 0.020000\n",
            "Training Epoch: 64 [5376/50000]\tLoss: 0.3793\tLR: 0.020000\n",
            "Training Epoch: 64 [5504/50000]\tLoss: 0.2797\tLR: 0.020000\n",
            "Training Epoch: 64 [5632/50000]\tLoss: 0.4200\tLR: 0.020000\n",
            "Training Epoch: 64 [5760/50000]\tLoss: 0.3474\tLR: 0.020000\n",
            "Training Epoch: 64 [5888/50000]\tLoss: 0.3312\tLR: 0.020000\n",
            "Training Epoch: 64 [6016/50000]\tLoss: 0.3585\tLR: 0.020000\n",
            "Training Epoch: 64 [6144/50000]\tLoss: 0.4146\tLR: 0.020000\n",
            "Training Epoch: 64 [6272/50000]\tLoss: 0.3283\tLR: 0.020000\n",
            "Training Epoch: 64 [6400/50000]\tLoss: 0.3763\tLR: 0.020000\n",
            "Training Epoch: 64 [6528/50000]\tLoss: 0.3660\tLR: 0.020000\n",
            "Training Epoch: 64 [6656/50000]\tLoss: 0.3504\tLR: 0.020000\n",
            "Training Epoch: 64 [6784/50000]\tLoss: 0.3806\tLR: 0.020000\n",
            "Training Epoch: 64 [6912/50000]\tLoss: 0.2729\tLR: 0.020000\n",
            "Training Epoch: 64 [7040/50000]\tLoss: 0.4723\tLR: 0.020000\n",
            "Training Epoch: 64 [7168/50000]\tLoss: 0.4074\tLR: 0.020000\n",
            "Training Epoch: 64 [7296/50000]\tLoss: 0.3881\tLR: 0.020000\n",
            "Training Epoch: 64 [7424/50000]\tLoss: 0.5391\tLR: 0.020000\n",
            "Training Epoch: 64 [7552/50000]\tLoss: 0.4790\tLR: 0.020000\n",
            "Training Epoch: 64 [7680/50000]\tLoss: 0.3516\tLR: 0.020000\n",
            "Training Epoch: 64 [7808/50000]\tLoss: 0.5190\tLR: 0.020000\n",
            "Training Epoch: 64 [7936/50000]\tLoss: 0.4545\tLR: 0.020000\n",
            "Training Epoch: 64 [8064/50000]\tLoss: 0.3900\tLR: 0.020000\n",
            "Training Epoch: 64 [8192/50000]\tLoss: 0.4549\tLR: 0.020000\n",
            "Training Epoch: 64 [8320/50000]\tLoss: 0.3597\tLR: 0.020000\n",
            "Training Epoch: 64 [8448/50000]\tLoss: 0.4427\tLR: 0.020000\n",
            "Training Epoch: 64 [8576/50000]\tLoss: 0.3764\tLR: 0.020000\n",
            "Training Epoch: 64 [8704/50000]\tLoss: 0.3068\tLR: 0.020000\n",
            "Training Epoch: 64 [8832/50000]\tLoss: 0.3822\tLR: 0.020000\n",
            "Training Epoch: 64 [8960/50000]\tLoss: 0.3342\tLR: 0.020000\n",
            "Training Epoch: 64 [9088/50000]\tLoss: 0.3762\tLR: 0.020000\n",
            "Training Epoch: 64 [9216/50000]\tLoss: 0.3998\tLR: 0.020000\n",
            "Training Epoch: 64 [9344/50000]\tLoss: 0.3925\tLR: 0.020000\n",
            "Training Epoch: 64 [9472/50000]\tLoss: 0.4458\tLR: 0.020000\n",
            "Training Epoch: 64 [9600/50000]\tLoss: 0.3624\tLR: 0.020000\n",
            "Training Epoch: 64 [9728/50000]\tLoss: 0.3268\tLR: 0.020000\n",
            "Training Epoch: 64 [9856/50000]\tLoss: 0.2981\tLR: 0.020000\n",
            "Training Epoch: 64 [9984/50000]\tLoss: 0.5782\tLR: 0.020000\n",
            "Training Epoch: 64 [10112/50000]\tLoss: 0.2863\tLR: 0.020000\n",
            "Training Epoch: 64 [10240/50000]\tLoss: 0.3138\tLR: 0.020000\n",
            "Training Epoch: 64 [10368/50000]\tLoss: 0.4620\tLR: 0.020000\n",
            "Training Epoch: 64 [10496/50000]\tLoss: 0.3696\tLR: 0.020000\n",
            "Training Epoch: 64 [10624/50000]\tLoss: 0.4459\tLR: 0.020000\n",
            "Training Epoch: 64 [10752/50000]\tLoss: 0.4164\tLR: 0.020000\n",
            "Training Epoch: 64 [10880/50000]\tLoss: 0.5332\tLR: 0.020000\n",
            "Training Epoch: 64 [11008/50000]\tLoss: 0.3972\tLR: 0.020000\n",
            "Training Epoch: 64 [11136/50000]\tLoss: 0.3432\tLR: 0.020000\n",
            "Training Epoch: 64 [11264/50000]\tLoss: 0.3487\tLR: 0.020000\n",
            "Training Epoch: 64 [11392/50000]\tLoss: 0.2773\tLR: 0.020000\n",
            "Training Epoch: 64 [11520/50000]\tLoss: 0.3326\tLR: 0.020000\n",
            "Training Epoch: 64 [11648/50000]\tLoss: 0.3964\tLR: 0.020000\n",
            "Training Epoch: 64 [11776/50000]\tLoss: 0.4446\tLR: 0.020000\n",
            "Training Epoch: 64 [11904/50000]\tLoss: 0.4216\tLR: 0.020000\n",
            "Training Epoch: 64 [12032/50000]\tLoss: 0.3235\tLR: 0.020000\n",
            "Training Epoch: 64 [12160/50000]\tLoss: 0.4146\tLR: 0.020000\n",
            "Training Epoch: 64 [12288/50000]\tLoss: 0.4232\tLR: 0.020000\n",
            "Training Epoch: 64 [12416/50000]\tLoss: 0.2930\tLR: 0.020000\n",
            "Training Epoch: 64 [12544/50000]\tLoss: 0.5305\tLR: 0.020000\n",
            "Training Epoch: 64 [12672/50000]\tLoss: 0.4717\tLR: 0.020000\n",
            "Training Epoch: 64 [12800/50000]\tLoss: 0.5026\tLR: 0.020000\n",
            "Training Epoch: 64 [12928/50000]\tLoss: 0.4171\tLR: 0.020000\n",
            "Training Epoch: 64 [13056/50000]\tLoss: 0.3937\tLR: 0.020000\n",
            "Training Epoch: 64 [13184/50000]\tLoss: 0.3466\tLR: 0.020000\n",
            "Training Epoch: 64 [13312/50000]\tLoss: 0.3163\tLR: 0.020000\n",
            "Training Epoch: 64 [13440/50000]\tLoss: 0.3746\tLR: 0.020000\n",
            "Training Epoch: 64 [13568/50000]\tLoss: 0.3265\tLR: 0.020000\n",
            "Training Epoch: 64 [13696/50000]\tLoss: 0.4637\tLR: 0.020000\n",
            "Training Epoch: 64 [13824/50000]\tLoss: 0.3379\tLR: 0.020000\n",
            "Training Epoch: 64 [13952/50000]\tLoss: 0.4169\tLR: 0.020000\n",
            "Training Epoch: 64 [14080/50000]\tLoss: 0.5022\tLR: 0.020000\n",
            "Training Epoch: 64 [14208/50000]\tLoss: 0.4463\tLR: 0.020000\n",
            "Training Epoch: 64 [14336/50000]\tLoss: 0.4978\tLR: 0.020000\n",
            "Training Epoch: 64 [14464/50000]\tLoss: 0.4316\tLR: 0.020000\n",
            "Training Epoch: 64 [14592/50000]\tLoss: 0.3622\tLR: 0.020000\n",
            "Training Epoch: 64 [14720/50000]\tLoss: 0.4198\tLR: 0.020000\n",
            "Training Epoch: 64 [14848/50000]\tLoss: 0.5479\tLR: 0.020000\n",
            "Training Epoch: 64 [14976/50000]\tLoss: 0.3608\tLR: 0.020000\n",
            "Training Epoch: 64 [15104/50000]\tLoss: 0.4264\tLR: 0.020000\n",
            "Training Epoch: 64 [15232/50000]\tLoss: 0.3891\tLR: 0.020000\n",
            "Training Epoch: 64 [15360/50000]\tLoss: 0.3073\tLR: 0.020000\n",
            "Training Epoch: 64 [15488/50000]\tLoss: 0.4624\tLR: 0.020000\n",
            "Training Epoch: 64 [15616/50000]\tLoss: 0.4626\tLR: 0.020000\n",
            "Training Epoch: 64 [15744/50000]\tLoss: 0.4134\tLR: 0.020000\n",
            "Training Epoch: 64 [15872/50000]\tLoss: 0.3823\tLR: 0.020000\n",
            "Training Epoch: 64 [16000/50000]\tLoss: 0.4693\tLR: 0.020000\n",
            "Training Epoch: 64 [16128/50000]\tLoss: 0.4404\tLR: 0.020000\n",
            "Training Epoch: 64 [16256/50000]\tLoss: 0.4085\tLR: 0.020000\n",
            "Training Epoch: 64 [16384/50000]\tLoss: 0.3740\tLR: 0.020000\n",
            "Training Epoch: 64 [16512/50000]\tLoss: 0.3083\tLR: 0.020000\n",
            "Training Epoch: 64 [16640/50000]\tLoss: 0.3375\tLR: 0.020000\n",
            "Training Epoch: 64 [16768/50000]\tLoss: 0.3697\tLR: 0.020000\n",
            "Training Epoch: 64 [16896/50000]\tLoss: 0.3832\tLR: 0.020000\n",
            "Training Epoch: 64 [17024/50000]\tLoss: 0.4244\tLR: 0.020000\n",
            "Training Epoch: 64 [17152/50000]\tLoss: 0.4500\tLR: 0.020000\n",
            "Training Epoch: 64 [17280/50000]\tLoss: 0.4354\tLR: 0.020000\n",
            "Training Epoch: 64 [17408/50000]\tLoss: 0.4557\tLR: 0.020000\n",
            "Training Epoch: 64 [17536/50000]\tLoss: 0.3854\tLR: 0.020000\n",
            "Training Epoch: 64 [17664/50000]\tLoss: 0.4107\tLR: 0.020000\n",
            "Training Epoch: 64 [17792/50000]\tLoss: 0.4350\tLR: 0.020000\n",
            "Training Epoch: 64 [17920/50000]\tLoss: 0.3377\tLR: 0.020000\n",
            "Training Epoch: 64 [18048/50000]\tLoss: 0.3086\tLR: 0.020000\n",
            "Training Epoch: 64 [18176/50000]\tLoss: 0.4382\tLR: 0.020000\n",
            "Training Epoch: 64 [18304/50000]\tLoss: 0.4259\tLR: 0.020000\n",
            "Training Epoch: 64 [18432/50000]\tLoss: 0.3521\tLR: 0.020000\n",
            "Training Epoch: 64 [18560/50000]\tLoss: 0.3825\tLR: 0.020000\n",
            "Training Epoch: 64 [18688/50000]\tLoss: 0.4441\tLR: 0.020000\n",
            "Training Epoch: 64 [18816/50000]\tLoss: 0.3778\tLR: 0.020000\n",
            "Training Epoch: 64 [18944/50000]\tLoss: 0.3921\tLR: 0.020000\n",
            "Training Epoch: 64 [19072/50000]\tLoss: 0.3850\tLR: 0.020000\n",
            "Training Epoch: 64 [19200/50000]\tLoss: 0.3943\tLR: 0.020000\n",
            "Training Epoch: 64 [19328/50000]\tLoss: 0.4954\tLR: 0.020000\n",
            "Training Epoch: 64 [19456/50000]\tLoss: 0.4321\tLR: 0.020000\n",
            "Training Epoch: 64 [19584/50000]\tLoss: 0.4385\tLR: 0.020000\n",
            "Training Epoch: 64 [19712/50000]\tLoss: 0.3228\tLR: 0.020000\n",
            "Training Epoch: 64 [19840/50000]\tLoss: 0.4205\tLR: 0.020000\n",
            "Training Epoch: 64 [19968/50000]\tLoss: 0.3737\tLR: 0.020000\n",
            "Training Epoch: 64 [20096/50000]\tLoss: 0.3662\tLR: 0.020000\n",
            "Training Epoch: 64 [20224/50000]\tLoss: 0.5201\tLR: 0.020000\n",
            "Training Epoch: 64 [20352/50000]\tLoss: 0.3497\tLR: 0.020000\n",
            "Training Epoch: 64 [20480/50000]\tLoss: 0.5518\tLR: 0.020000\n",
            "Training Epoch: 64 [20608/50000]\tLoss: 0.3218\tLR: 0.020000\n",
            "Training Epoch: 64 [20736/50000]\tLoss: 0.3462\tLR: 0.020000\n",
            "Training Epoch: 64 [20864/50000]\tLoss: 0.3588\tLR: 0.020000\n",
            "Training Epoch: 64 [20992/50000]\tLoss: 0.3401\tLR: 0.020000\n",
            "Training Epoch: 64 [21120/50000]\tLoss: 0.4563\tLR: 0.020000\n",
            "Training Epoch: 64 [21248/50000]\tLoss: 0.5142\tLR: 0.020000\n",
            "Training Epoch: 64 [21376/50000]\tLoss: 0.3821\tLR: 0.020000\n",
            "Training Epoch: 64 [21504/50000]\tLoss: 0.3719\tLR: 0.020000\n",
            "Training Epoch: 64 [21632/50000]\tLoss: 0.4834\tLR: 0.020000\n",
            "Training Epoch: 64 [21760/50000]\tLoss: 0.3529\tLR: 0.020000\n",
            "Training Epoch: 64 [21888/50000]\tLoss: 0.3057\tLR: 0.020000\n",
            "Training Epoch: 64 [22016/50000]\tLoss: 0.2728\tLR: 0.020000\n",
            "Training Epoch: 64 [22144/50000]\tLoss: 0.3360\tLR: 0.020000\n",
            "Training Epoch: 64 [22272/50000]\tLoss: 0.4712\tLR: 0.020000\n",
            "Training Epoch: 64 [22400/50000]\tLoss: 0.6287\tLR: 0.020000\n",
            "Training Epoch: 64 [22528/50000]\tLoss: 0.5458\tLR: 0.020000\n",
            "Training Epoch: 64 [22656/50000]\tLoss: 0.3744\tLR: 0.020000\n",
            "Training Epoch: 64 [22784/50000]\tLoss: 0.5959\tLR: 0.020000\n",
            "Training Epoch: 64 [22912/50000]\tLoss: 0.4165\tLR: 0.020000\n",
            "Training Epoch: 64 [23040/50000]\tLoss: 0.3546\tLR: 0.020000\n",
            "Training Epoch: 64 [23168/50000]\tLoss: 0.5540\tLR: 0.020000\n",
            "Training Epoch: 64 [23296/50000]\tLoss: 0.4598\tLR: 0.020000\n",
            "Training Epoch: 64 [23424/50000]\tLoss: 0.5348\tLR: 0.020000\n",
            "Training Epoch: 64 [23552/50000]\tLoss: 0.3307\tLR: 0.020000\n",
            "Training Epoch: 64 [23680/50000]\tLoss: 0.4942\tLR: 0.020000\n",
            "Training Epoch: 64 [23808/50000]\tLoss: 0.5796\tLR: 0.020000\n",
            "Training Epoch: 64 [23936/50000]\tLoss: 0.4551\tLR: 0.020000\n",
            "Training Epoch: 64 [24064/50000]\tLoss: 0.5226\tLR: 0.020000\n",
            "Training Epoch: 64 [24192/50000]\tLoss: 0.4570\tLR: 0.020000\n",
            "Training Epoch: 64 [24320/50000]\tLoss: 0.4132\tLR: 0.020000\n",
            "Training Epoch: 64 [24448/50000]\tLoss: 0.3662\tLR: 0.020000\n",
            "Training Epoch: 64 [24576/50000]\tLoss: 0.4149\tLR: 0.020000\n",
            "Training Epoch: 64 [24704/50000]\tLoss: 0.4667\tLR: 0.020000\n",
            "Training Epoch: 64 [24832/50000]\tLoss: 0.3217\tLR: 0.020000\n",
            "Training Epoch: 64 [24960/50000]\tLoss: 0.4741\tLR: 0.020000\n",
            "Training Epoch: 64 [25088/50000]\tLoss: 0.4720\tLR: 0.020000\n",
            "Training Epoch: 64 [25216/50000]\tLoss: 0.4946\tLR: 0.020000\n",
            "Training Epoch: 64 [25344/50000]\tLoss: 0.3868\tLR: 0.020000\n",
            "Training Epoch: 64 [25472/50000]\tLoss: 0.4786\tLR: 0.020000\n",
            "Training Epoch: 64 [25600/50000]\tLoss: 0.4860\tLR: 0.020000\n",
            "Training Epoch: 64 [25728/50000]\tLoss: 0.4659\tLR: 0.020000\n",
            "Training Epoch: 64 [25856/50000]\tLoss: 0.4307\tLR: 0.020000\n",
            "Training Epoch: 64 [25984/50000]\tLoss: 0.4100\tLR: 0.020000\n",
            "Training Epoch: 64 [26112/50000]\tLoss: 0.4394\tLR: 0.020000\n",
            "Training Epoch: 64 [26240/50000]\tLoss: 0.3915\tLR: 0.020000\n",
            "Training Epoch: 64 [26368/50000]\tLoss: 0.3350\tLR: 0.020000\n",
            "Training Epoch: 64 [26496/50000]\tLoss: 0.3980\tLR: 0.020000\n",
            "Training Epoch: 64 [26624/50000]\tLoss: 0.5295\tLR: 0.020000\n",
            "Training Epoch: 64 [26752/50000]\tLoss: 0.3131\tLR: 0.020000\n",
            "Training Epoch: 64 [26880/50000]\tLoss: 0.4016\tLR: 0.020000\n",
            "Training Epoch: 64 [27008/50000]\tLoss: 0.5834\tLR: 0.020000\n",
            "Training Epoch: 64 [27136/50000]\tLoss: 0.4723\tLR: 0.020000\n",
            "Training Epoch: 64 [27264/50000]\tLoss: 0.4552\tLR: 0.020000\n",
            "Training Epoch: 64 [27392/50000]\tLoss: 0.3871\tLR: 0.020000\n",
            "Training Epoch: 64 [27520/50000]\tLoss: 0.4319\tLR: 0.020000\n",
            "Training Epoch: 64 [27648/50000]\tLoss: 0.3725\tLR: 0.020000\n",
            "Training Epoch: 64 [27776/50000]\tLoss: 0.3515\tLR: 0.020000\n",
            "Training Epoch: 64 [27904/50000]\tLoss: 0.3897\tLR: 0.020000\n",
            "Training Epoch: 64 [28032/50000]\tLoss: 0.5389\tLR: 0.020000\n",
            "Training Epoch: 64 [28160/50000]\tLoss: 0.4993\tLR: 0.020000\n",
            "Training Epoch: 64 [28288/50000]\tLoss: 0.4136\tLR: 0.020000\n",
            "Training Epoch: 64 [28416/50000]\tLoss: 0.3878\tLR: 0.020000\n",
            "Training Epoch: 64 [28544/50000]\tLoss: 0.4517\tLR: 0.020000\n",
            "Training Epoch: 64 [28672/50000]\tLoss: 0.4093\tLR: 0.020000\n",
            "Training Epoch: 64 [28800/50000]\tLoss: 0.3658\tLR: 0.020000\n",
            "Training Epoch: 64 [28928/50000]\tLoss: 0.3881\tLR: 0.020000\n",
            "Training Epoch: 64 [29056/50000]\tLoss: 0.4768\tLR: 0.020000\n",
            "Training Epoch: 64 [29184/50000]\tLoss: 0.4632\tLR: 0.020000\n",
            "Training Epoch: 64 [29312/50000]\tLoss: 0.4327\tLR: 0.020000\n",
            "Training Epoch: 64 [29440/50000]\tLoss: 0.5295\tLR: 0.020000\n",
            "Training Epoch: 64 [29568/50000]\tLoss: 0.5654\tLR: 0.020000\n",
            "Training Epoch: 64 [29696/50000]\tLoss: 0.5252\tLR: 0.020000\n",
            "Training Epoch: 64 [29824/50000]\tLoss: 0.4494\tLR: 0.020000\n",
            "Training Epoch: 64 [29952/50000]\tLoss: 0.4777\tLR: 0.020000\n",
            "Training Epoch: 64 [30080/50000]\tLoss: 0.3286\tLR: 0.020000\n",
            "Training Epoch: 64 [30208/50000]\tLoss: 0.3304\tLR: 0.020000\n",
            "Training Epoch: 64 [30336/50000]\tLoss: 0.3701\tLR: 0.020000\n",
            "Training Epoch: 64 [30464/50000]\tLoss: 0.3620\tLR: 0.020000\n",
            "Training Epoch: 64 [30592/50000]\tLoss: 0.3774\tLR: 0.020000\n",
            "Training Epoch: 64 [30720/50000]\tLoss: 0.3105\tLR: 0.020000\n",
            "Training Epoch: 64 [30848/50000]\tLoss: 0.4359\tLR: 0.020000\n",
            "Training Epoch: 64 [30976/50000]\tLoss: 0.5026\tLR: 0.020000\n",
            "Training Epoch: 64 [31104/50000]\tLoss: 0.3634\tLR: 0.020000\n",
            "Training Epoch: 64 [31232/50000]\tLoss: 0.5606\tLR: 0.020000\n",
            "Training Epoch: 64 [31360/50000]\tLoss: 0.4166\tLR: 0.020000\n",
            "Training Epoch: 64 [31488/50000]\tLoss: 0.3492\tLR: 0.020000\n",
            "Training Epoch: 64 [31616/50000]\tLoss: 0.3929\tLR: 0.020000\n",
            "Training Epoch: 64 [31744/50000]\tLoss: 0.4885\tLR: 0.020000\n",
            "Training Epoch: 64 [31872/50000]\tLoss: 0.3950\tLR: 0.020000\n",
            "Training Epoch: 64 [32000/50000]\tLoss: 0.4127\tLR: 0.020000\n",
            "Training Epoch: 64 [32128/50000]\tLoss: 0.4586\tLR: 0.020000\n",
            "Training Epoch: 64 [32256/50000]\tLoss: 0.3381\tLR: 0.020000\n",
            "Training Epoch: 64 [32384/50000]\tLoss: 0.3408\tLR: 0.020000\n",
            "Training Epoch: 64 [32512/50000]\tLoss: 0.4299\tLR: 0.020000\n",
            "Training Epoch: 64 [32640/50000]\tLoss: 0.3646\tLR: 0.020000\n",
            "Training Epoch: 64 [32768/50000]\tLoss: 0.5300\tLR: 0.020000\n",
            "Training Epoch: 64 [32896/50000]\tLoss: 0.3780\tLR: 0.020000\n",
            "Training Epoch: 64 [33024/50000]\tLoss: 0.3397\tLR: 0.020000\n",
            "Training Epoch: 64 [33152/50000]\tLoss: 0.3201\tLR: 0.020000\n",
            "Training Epoch: 64 [33280/50000]\tLoss: 0.4207\tLR: 0.020000\n",
            "Training Epoch: 64 [33408/50000]\tLoss: 0.4016\tLR: 0.020000\n",
            "Training Epoch: 64 [33536/50000]\tLoss: 0.4099\tLR: 0.020000\n",
            "Training Epoch: 64 [33664/50000]\tLoss: 0.4002\tLR: 0.020000\n",
            "Training Epoch: 64 [33792/50000]\tLoss: 0.2860\tLR: 0.020000\n",
            "Training Epoch: 64 [33920/50000]\tLoss: 0.4031\tLR: 0.020000\n",
            "Training Epoch: 64 [34048/50000]\tLoss: 0.3940\tLR: 0.020000\n",
            "Training Epoch: 64 [34176/50000]\tLoss: 0.4301\tLR: 0.020000\n",
            "Training Epoch: 64 [34304/50000]\tLoss: 0.3758\tLR: 0.020000\n",
            "Training Epoch: 64 [34432/50000]\tLoss: 0.3682\tLR: 0.020000\n",
            "Training Epoch: 64 [34560/50000]\tLoss: 0.4053\tLR: 0.020000\n",
            "Training Epoch: 64 [34688/50000]\tLoss: 0.4803\tLR: 0.020000\n",
            "Training Epoch: 64 [34816/50000]\tLoss: 0.4671\tLR: 0.020000\n",
            "Training Epoch: 64 [34944/50000]\tLoss: 0.5513\tLR: 0.020000\n",
            "Training Epoch: 64 [35072/50000]\tLoss: 0.3960\tLR: 0.020000\n",
            "Training Epoch: 64 [35200/50000]\tLoss: 0.3911\tLR: 0.020000\n",
            "Training Epoch: 64 [35328/50000]\tLoss: 0.3736\tLR: 0.020000\n",
            "Training Epoch: 64 [35456/50000]\tLoss: 0.3717\tLR: 0.020000\n",
            "Training Epoch: 64 [35584/50000]\tLoss: 0.5247\tLR: 0.020000\n",
            "Training Epoch: 64 [35712/50000]\tLoss: 0.3680\tLR: 0.020000\n",
            "Training Epoch: 64 [35840/50000]\tLoss: 0.3923\tLR: 0.020000\n",
            "Training Epoch: 64 [35968/50000]\tLoss: 0.6445\tLR: 0.020000\n",
            "Training Epoch: 64 [36096/50000]\tLoss: 0.3432\tLR: 0.020000\n",
            "Training Epoch: 64 [36224/50000]\tLoss: 0.5228\tLR: 0.020000\n",
            "Training Epoch: 64 [36352/50000]\tLoss: 0.5151\tLR: 0.020000\n",
            "Training Epoch: 64 [36480/50000]\tLoss: 0.3968\tLR: 0.020000\n",
            "Training Epoch: 64 [36608/50000]\tLoss: 0.4652\tLR: 0.020000\n",
            "Training Epoch: 64 [36736/50000]\tLoss: 0.3712\tLR: 0.020000\n",
            "Training Epoch: 64 [36864/50000]\tLoss: 0.4124\tLR: 0.020000\n",
            "Training Epoch: 64 [36992/50000]\tLoss: 0.3170\tLR: 0.020000\n",
            "Training Epoch: 64 [37120/50000]\tLoss: 0.4687\tLR: 0.020000\n",
            "Training Epoch: 64 [37248/50000]\tLoss: 0.5037\tLR: 0.020000\n",
            "Training Epoch: 64 [37376/50000]\tLoss: 0.3941\tLR: 0.020000\n",
            "Training Epoch: 64 [37504/50000]\tLoss: 0.5797\tLR: 0.020000\n",
            "Training Epoch: 64 [37632/50000]\tLoss: 0.3943\tLR: 0.020000\n",
            "Training Epoch: 64 [37760/50000]\tLoss: 0.5151\tLR: 0.020000\n",
            "Training Epoch: 64 [37888/50000]\tLoss: 0.5454\tLR: 0.020000\n",
            "Training Epoch: 64 [38016/50000]\tLoss: 0.5162\tLR: 0.020000\n",
            "Training Epoch: 64 [38144/50000]\tLoss: 0.4761\tLR: 0.020000\n",
            "Training Epoch: 64 [38272/50000]\tLoss: 0.5875\tLR: 0.020000\n",
            "Training Epoch: 64 [38400/50000]\tLoss: 0.4739\tLR: 0.020000\n",
            "Training Epoch: 64 [38528/50000]\tLoss: 0.3550\tLR: 0.020000\n",
            "Training Epoch: 64 [38656/50000]\tLoss: 0.4869\tLR: 0.020000\n",
            "Training Epoch: 64 [38784/50000]\tLoss: 0.5331\tLR: 0.020000\n",
            "Training Epoch: 64 [38912/50000]\tLoss: 0.4650\tLR: 0.020000\n",
            "Training Epoch: 64 [39040/50000]\tLoss: 0.3658\tLR: 0.020000\n",
            "Training Epoch: 64 [39168/50000]\tLoss: 0.5768\tLR: 0.020000\n",
            "Training Epoch: 64 [39296/50000]\tLoss: 0.5446\tLR: 0.020000\n",
            "Training Epoch: 64 [39424/50000]\tLoss: 0.3773\tLR: 0.020000\n",
            "Training Epoch: 64 [39552/50000]\tLoss: 0.5289\tLR: 0.020000\n",
            "Training Epoch: 64 [39680/50000]\tLoss: 0.3543\tLR: 0.020000\n",
            "Training Epoch: 64 [39808/50000]\tLoss: 0.6342\tLR: 0.020000\n",
            "Training Epoch: 64 [39936/50000]\tLoss: 0.3037\tLR: 0.020000\n",
            "Training Epoch: 64 [40064/50000]\tLoss: 0.3191\tLR: 0.020000\n",
            "Training Epoch: 64 [40192/50000]\tLoss: 0.4886\tLR: 0.020000\n",
            "Training Epoch: 64 [40320/50000]\tLoss: 0.4668\tLR: 0.020000\n",
            "Training Epoch: 64 [40448/50000]\tLoss: 0.4097\tLR: 0.020000\n",
            "Training Epoch: 64 [40576/50000]\tLoss: 0.4634\tLR: 0.020000\n",
            "Training Epoch: 64 [40704/50000]\tLoss: 0.3746\tLR: 0.020000\n",
            "Training Epoch: 64 [40832/50000]\tLoss: 0.5257\tLR: 0.020000\n",
            "Training Epoch: 64 [40960/50000]\tLoss: 0.4239\tLR: 0.020000\n",
            "Training Epoch: 64 [41088/50000]\tLoss: 0.3839\tLR: 0.020000\n",
            "Training Epoch: 64 [41216/50000]\tLoss: 0.4572\tLR: 0.020000\n",
            "Training Epoch: 64 [41344/50000]\tLoss: 0.4308\tLR: 0.020000\n",
            "Training Epoch: 64 [41472/50000]\tLoss: 0.4937\tLR: 0.020000\n",
            "Training Epoch: 64 [41600/50000]\tLoss: 0.5093\tLR: 0.020000\n",
            "Training Epoch: 64 [41728/50000]\tLoss: 0.3760\tLR: 0.020000\n",
            "Training Epoch: 64 [41856/50000]\tLoss: 0.4014\tLR: 0.020000\n",
            "Training Epoch: 64 [41984/50000]\tLoss: 0.6173\tLR: 0.020000\n",
            "Training Epoch: 64 [42112/50000]\tLoss: 0.6142\tLR: 0.020000\n",
            "Training Epoch: 64 [42240/50000]\tLoss: 0.4821\tLR: 0.020000\n",
            "Training Epoch: 64 [42368/50000]\tLoss: 0.5117\tLR: 0.020000\n",
            "Training Epoch: 64 [42496/50000]\tLoss: 0.3618\tLR: 0.020000\n",
            "Training Epoch: 64 [42624/50000]\tLoss: 0.4128\tLR: 0.020000\n",
            "Training Epoch: 64 [42752/50000]\tLoss: 0.4281\tLR: 0.020000\n",
            "Training Epoch: 64 [42880/50000]\tLoss: 0.3939\tLR: 0.020000\n",
            "Training Epoch: 64 [43008/50000]\tLoss: 0.3291\tLR: 0.020000\n",
            "Training Epoch: 64 [43136/50000]\tLoss: 0.5321\tLR: 0.020000\n",
            "Training Epoch: 64 [43264/50000]\tLoss: 0.5166\tLR: 0.020000\n",
            "Training Epoch: 64 [43392/50000]\tLoss: 0.6205\tLR: 0.020000\n",
            "Training Epoch: 64 [43520/50000]\tLoss: 0.4206\tLR: 0.020000\n",
            "Training Epoch: 64 [43648/50000]\tLoss: 0.3780\tLR: 0.020000\n",
            "Training Epoch: 64 [43776/50000]\tLoss: 0.3754\tLR: 0.020000\n",
            "Training Epoch: 64 [43904/50000]\tLoss: 0.4692\tLR: 0.020000\n",
            "Training Epoch: 64 [44032/50000]\tLoss: 0.5668\tLR: 0.020000\n",
            "Training Epoch: 64 [44160/50000]\tLoss: 0.4430\tLR: 0.020000\n",
            "Training Epoch: 64 [44288/50000]\tLoss: 0.4954\tLR: 0.020000\n",
            "Training Epoch: 64 [44416/50000]\tLoss: 0.4921\tLR: 0.020000\n",
            "Training Epoch: 64 [44544/50000]\tLoss: 0.4737\tLR: 0.020000\n",
            "Training Epoch: 64 [44672/50000]\tLoss: 0.5067\tLR: 0.020000\n",
            "Training Epoch: 64 [44800/50000]\tLoss: 0.5364\tLR: 0.020000\n",
            "Training Epoch: 64 [44928/50000]\tLoss: 0.4293\tLR: 0.020000\n",
            "Training Epoch: 64 [45056/50000]\tLoss: 0.3696\tLR: 0.020000\n",
            "Training Epoch: 64 [45184/50000]\tLoss: 0.4680\tLR: 0.020000\n",
            "Training Epoch: 64 [45312/50000]\tLoss: 0.3370\tLR: 0.020000\n",
            "Training Epoch: 64 [45440/50000]\tLoss: 0.3234\tLR: 0.020000\n",
            "Training Epoch: 64 [45568/50000]\tLoss: 0.4044\tLR: 0.020000\n",
            "Training Epoch: 64 [45696/50000]\tLoss: 0.3795\tLR: 0.020000\n",
            "Training Epoch: 64 [45824/50000]\tLoss: 0.5260\tLR: 0.020000\n",
            "Training Epoch: 64 [45952/50000]\tLoss: 0.3675\tLR: 0.020000\n",
            "Training Epoch: 64 [46080/50000]\tLoss: 0.4093\tLR: 0.020000\n",
            "Training Epoch: 64 [46208/50000]\tLoss: 0.4168\tLR: 0.020000\n",
            "Training Epoch: 64 [46336/50000]\tLoss: 0.3853\tLR: 0.020000\n",
            "Training Epoch: 64 [46464/50000]\tLoss: 0.4149\tLR: 0.020000\n",
            "Training Epoch: 64 [46592/50000]\tLoss: 0.5020\tLR: 0.020000\n",
            "Training Epoch: 64 [46720/50000]\tLoss: 0.4596\tLR: 0.020000\n",
            "Training Epoch: 64 [46848/50000]\tLoss: 0.5788\tLR: 0.020000\n",
            "Training Epoch: 64 [46976/50000]\tLoss: 0.3899\tLR: 0.020000\n",
            "Training Epoch: 64 [47104/50000]\tLoss: 0.4651\tLR: 0.020000\n",
            "Training Epoch: 64 [47232/50000]\tLoss: 0.3392\tLR: 0.020000\n",
            "Training Epoch: 64 [47360/50000]\tLoss: 0.4711\tLR: 0.020000\n",
            "Training Epoch: 64 [47488/50000]\tLoss: 0.5148\tLR: 0.020000\n",
            "Training Epoch: 64 [47616/50000]\tLoss: 0.4584\tLR: 0.020000\n",
            "Training Epoch: 64 [47744/50000]\tLoss: 0.4797\tLR: 0.020000\n",
            "Training Epoch: 64 [47872/50000]\tLoss: 0.5018\tLR: 0.020000\n",
            "Training Epoch: 64 [48000/50000]\tLoss: 0.3535\tLR: 0.020000\n",
            "Training Epoch: 64 [48128/50000]\tLoss: 0.4640\tLR: 0.020000\n",
            "Training Epoch: 64 [48256/50000]\tLoss: 0.3870\tLR: 0.020000\n",
            "Training Epoch: 64 [48384/50000]\tLoss: 0.5260\tLR: 0.020000\n",
            "Training Epoch: 64 [48512/50000]\tLoss: 0.3849\tLR: 0.020000\n",
            "Training Epoch: 64 [48640/50000]\tLoss: 0.5931\tLR: 0.020000\n",
            "Training Epoch: 64 [48768/50000]\tLoss: 0.3737\tLR: 0.020000\n",
            "Training Epoch: 64 [48896/50000]\tLoss: 0.3804\tLR: 0.020000\n",
            "Training Epoch: 64 [49024/50000]\tLoss: 0.3381\tLR: 0.020000\n",
            "Training Epoch: 64 [49152/50000]\tLoss: 0.4378\tLR: 0.020000\n",
            "Training Epoch: 64 [49280/50000]\tLoss: 0.3888\tLR: 0.020000\n",
            "Training Epoch: 64 [49408/50000]\tLoss: 0.5102\tLR: 0.020000\n",
            "Training Epoch: 64 [49536/50000]\tLoss: 0.4311\tLR: 0.020000\n",
            "Training Epoch: 64 [49664/50000]\tLoss: 0.5174\tLR: 0.020000\n",
            "Training Epoch: 64 [49792/50000]\tLoss: 0.4074\tLR: 0.020000\n",
            "Training Epoch: 64 [49920/50000]\tLoss: 0.5608\tLR: 0.020000\n",
            "Training Epoch: 64 [50000/50000]\tLoss: 0.4647\tLR: 0.020000\n",
            "epoch 64 training time consumed: 145.00s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  133678 KB |    1044 MB |  234536 GB |  234536 GB |\n",
            "|       from large pool |  122880 KB |    1034 MB |  234347 GB |  234347 GB |\n",
            "|       from small pool |   10798 KB |      13 MB |     189 GB |     189 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  133678 KB |    1044 MB |  234536 GB |  234536 GB |\n",
            "|       from large pool |  122880 KB |    1034 MB |  234347 GB |  234347 GB |\n",
            "|       from small pool |   10798 KB |      13 MB |     189 GB |     189 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    1838 MB |    1838 MB |    1838 MB |       0 B  |\n",
            "|       from large pool |    1822 MB |    1822 MB |    1822 MB |       0 B  |\n",
            "|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  433618 KB |  778793 KB |  109051 GB |  109050 GB |\n",
            "|       from large pool |  432128 KB |  776192 KB |  108861 GB |  108861 GB |\n",
            "|       from small pool |    1490 KB |    3494 KB |     189 GB |     189 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     255    |     335    |    9488 K  |    9488 K  |\n",
            "|       from large pool |      24    |      65    |    4651 K  |    4651 K  |\n",
            "|       from small pool |     231    |     274    |    4837 K  |    4837 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     255    |     335    |    9488 K  |    9488 K  |\n",
            "|       from large pool |      24    |      65    |    4651 K  |    4651 K  |\n",
            "|       from small pool |     231    |     274    |    4837 K  |    4837 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      20    |      20    |      20    |       0    |\n",
            "|       from large pool |      12    |      12    |      12    |       0    |\n",
            "|       from small pool |       8    |       8    |       8    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      37    |      45    |    5492 K  |    5492 K  |\n",
            "|       from large pool |      10    |      15    |    2105 K  |    2105 K  |\n",
            "|       from small pool |      27    |      35    |    3387 K  |    3387 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 64, Average loss: 0.0078, Accuracy: 0.7316, Time consumed:9.01s\n",
            "\n",
            "Training Epoch: 65 [128/50000]\tLoss: 0.4025\tLR: 0.020000\n",
            "Training Epoch: 65 [256/50000]\tLoss: 0.5094\tLR: 0.020000\n",
            "Training Epoch: 65 [384/50000]\tLoss: 0.3829\tLR: 0.020000\n",
            "Training Epoch: 65 [512/50000]\tLoss: 0.4501\tLR: 0.020000\n",
            "Training Epoch: 65 [640/50000]\tLoss: 0.3032\tLR: 0.020000\n",
            "Training Epoch: 65 [768/50000]\tLoss: 0.3824\tLR: 0.020000\n",
            "Training Epoch: 65 [896/50000]\tLoss: 0.2883\tLR: 0.020000\n",
            "Training Epoch: 65 [1024/50000]\tLoss: 0.3956\tLR: 0.020000\n",
            "Training Epoch: 65 [1152/50000]\tLoss: 0.3022\tLR: 0.020000\n",
            "Training Epoch: 65 [1280/50000]\tLoss: 0.4036\tLR: 0.020000\n",
            "Training Epoch: 65 [1408/50000]\tLoss: 0.3509\tLR: 0.020000\n",
            "Training Epoch: 65 [1536/50000]\tLoss: 0.3559\tLR: 0.020000\n",
            "Training Epoch: 65 [1664/50000]\tLoss: 0.2316\tLR: 0.020000\n",
            "Training Epoch: 65 [1792/50000]\tLoss: 0.4235\tLR: 0.020000\n",
            "Training Epoch: 65 [1920/50000]\tLoss: 0.2737\tLR: 0.020000\n",
            "Training Epoch: 65 [2048/50000]\tLoss: 0.2972\tLR: 0.020000\n",
            "Training Epoch: 65 [2176/50000]\tLoss: 0.4335\tLR: 0.020000\n",
            "Training Epoch: 65 [2304/50000]\tLoss: 0.2805\tLR: 0.020000\n",
            "Training Epoch: 65 [2432/50000]\tLoss: 0.3040\tLR: 0.020000\n",
            "Training Epoch: 65 [2560/50000]\tLoss: 0.2953\tLR: 0.020000\n",
            "Training Epoch: 65 [2688/50000]\tLoss: 0.3747\tLR: 0.020000\n",
            "Training Epoch: 65 [2816/50000]\tLoss: 0.4127\tLR: 0.020000\n",
            "Training Epoch: 65 [2944/50000]\tLoss: 0.3430\tLR: 0.020000\n",
            "Training Epoch: 65 [3072/50000]\tLoss: 0.3975\tLR: 0.020000\n",
            "Training Epoch: 65 [3200/50000]\tLoss: 0.4045\tLR: 0.020000\n",
            "Training Epoch: 65 [3328/50000]\tLoss: 0.4223\tLR: 0.020000\n",
            "Training Epoch: 65 [3456/50000]\tLoss: 0.2929\tLR: 0.020000\n",
            "Training Epoch: 65 [3584/50000]\tLoss: 0.2956\tLR: 0.020000\n",
            "Training Epoch: 65 [3712/50000]\tLoss: 0.2648\tLR: 0.020000\n",
            "Training Epoch: 65 [3840/50000]\tLoss: 0.4046\tLR: 0.020000\n",
            "Training Epoch: 65 [3968/50000]\tLoss: 0.3345\tLR: 0.020000\n",
            "Training Epoch: 65 [4096/50000]\tLoss: 0.4471\tLR: 0.020000\n",
            "Training Epoch: 65 [4224/50000]\tLoss: 0.4270\tLR: 0.020000\n",
            "Training Epoch: 65 [4352/50000]\tLoss: 0.3440\tLR: 0.020000\n",
            "Training Epoch: 65 [4480/50000]\tLoss: 0.3409\tLR: 0.020000\n",
            "Training Epoch: 65 [4608/50000]\tLoss: 0.3738\tLR: 0.020000\n",
            "Training Epoch: 65 [4736/50000]\tLoss: 0.4114\tLR: 0.020000\n",
            "Training Epoch: 65 [4864/50000]\tLoss: 0.3599\tLR: 0.020000\n",
            "Training Epoch: 65 [4992/50000]\tLoss: 0.4293\tLR: 0.020000\n",
            "Training Epoch: 65 [5120/50000]\tLoss: 0.3001\tLR: 0.020000\n",
            "Training Epoch: 65 [5248/50000]\tLoss: 0.5089\tLR: 0.020000\n",
            "Training Epoch: 65 [5376/50000]\tLoss: 0.3765\tLR: 0.020000\n",
            "Training Epoch: 65 [5504/50000]\tLoss: 0.3845\tLR: 0.020000\n",
            "Training Epoch: 65 [5632/50000]\tLoss: 0.4039\tLR: 0.020000\n",
            "Training Epoch: 65 [5760/50000]\tLoss: 0.3639\tLR: 0.020000\n",
            "Training Epoch: 65 [5888/50000]\tLoss: 0.2698\tLR: 0.020000\n",
            "Training Epoch: 65 [6016/50000]\tLoss: 0.4701\tLR: 0.020000\n",
            "Training Epoch: 65 [6144/50000]\tLoss: 0.3800\tLR: 0.020000\n",
            "Training Epoch: 65 [6272/50000]\tLoss: 0.4978\tLR: 0.020000\n",
            "Training Epoch: 65 [6400/50000]\tLoss: 0.3948\tLR: 0.020000\n",
            "Training Epoch: 65 [6528/50000]\tLoss: 0.3957\tLR: 0.020000\n",
            "Training Epoch: 65 [6656/50000]\tLoss: 0.4979\tLR: 0.020000\n",
            "Training Epoch: 65 [6784/50000]\tLoss: 0.2995\tLR: 0.020000\n",
            "Training Epoch: 65 [6912/50000]\tLoss: 0.3564\tLR: 0.020000\n",
            "Training Epoch: 65 [7040/50000]\tLoss: 0.3443\tLR: 0.020000\n",
            "Training Epoch: 65 [7168/50000]\tLoss: 0.4004\tLR: 0.020000\n",
            "Training Epoch: 65 [7296/50000]\tLoss: 0.3521\tLR: 0.020000\n",
            "Training Epoch: 65 [7424/50000]\tLoss: 0.3557\tLR: 0.020000\n",
            "Training Epoch: 65 [7552/50000]\tLoss: 0.3090\tLR: 0.020000\n",
            "Training Epoch: 65 [7680/50000]\tLoss: 0.3184\tLR: 0.020000\n",
            "Training Epoch: 65 [7808/50000]\tLoss: 0.3904\tLR: 0.020000\n",
            "Training Epoch: 65 [7936/50000]\tLoss: 0.3414\tLR: 0.020000\n",
            "Training Epoch: 65 [8064/50000]\tLoss: 0.5054\tLR: 0.020000\n",
            "Training Epoch: 65 [8192/50000]\tLoss: 0.4498\tLR: 0.020000\n",
            "Training Epoch: 65 [8320/50000]\tLoss: 0.3741\tLR: 0.020000\n",
            "Training Epoch: 65 [8448/50000]\tLoss: 0.3531\tLR: 0.020000\n",
            "Training Epoch: 65 [8576/50000]\tLoss: 0.4545\tLR: 0.020000\n",
            "Training Epoch: 65 [8704/50000]\tLoss: 0.3534\tLR: 0.020000\n",
            "Training Epoch: 65 [8832/50000]\tLoss: 0.2821\tLR: 0.020000\n",
            "Training Epoch: 65 [8960/50000]\tLoss: 0.3570\tLR: 0.020000\n",
            "Training Epoch: 65 [9088/50000]\tLoss: 0.3078\tLR: 0.020000\n",
            "Training Epoch: 65 [9216/50000]\tLoss: 0.4035\tLR: 0.020000\n",
            "Training Epoch: 65 [9344/50000]\tLoss: 0.4206\tLR: 0.020000\n",
            "Training Epoch: 65 [9472/50000]\tLoss: 0.2850\tLR: 0.020000\n",
            "Training Epoch: 65 [9600/50000]\tLoss: 0.3479\tLR: 0.020000\n",
            "Training Epoch: 65 [9728/50000]\tLoss: 0.4184\tLR: 0.020000\n",
            "Training Epoch: 65 [9856/50000]\tLoss: 0.3536\tLR: 0.020000\n",
            "Training Epoch: 65 [9984/50000]\tLoss: 0.3810\tLR: 0.020000\n",
            "Training Epoch: 65 [10112/50000]\tLoss: 0.3509\tLR: 0.020000\n",
            "Training Epoch: 65 [10240/50000]\tLoss: 0.3974\tLR: 0.020000\n",
            "Training Epoch: 65 [10368/50000]\tLoss: 0.3650\tLR: 0.020000\n",
            "Training Epoch: 65 [10496/50000]\tLoss: 0.3048\tLR: 0.020000\n",
            "Training Epoch: 65 [10624/50000]\tLoss: 0.4794\tLR: 0.020000\n",
            "Training Epoch: 65 [10752/50000]\tLoss: 0.3607\tLR: 0.020000\n",
            "Training Epoch: 65 [10880/50000]\tLoss: 0.3480\tLR: 0.020000\n",
            "Training Epoch: 65 [11008/50000]\tLoss: 0.4094\tLR: 0.020000\n",
            "Training Epoch: 65 [11136/50000]\tLoss: 0.3477\tLR: 0.020000\n",
            "Training Epoch: 65 [11264/50000]\tLoss: 0.3967\tLR: 0.020000\n",
            "Training Epoch: 65 [11392/50000]\tLoss: 0.2639\tLR: 0.020000\n",
            "Training Epoch: 65 [11520/50000]\tLoss: 0.4613\tLR: 0.020000\n",
            "Training Epoch: 65 [11648/50000]\tLoss: 0.4099\tLR: 0.020000\n",
            "Training Epoch: 65 [11776/50000]\tLoss: 0.3909\tLR: 0.020000\n",
            "Training Epoch: 65 [11904/50000]\tLoss: 0.2830\tLR: 0.020000\n",
            "Training Epoch: 65 [12032/50000]\tLoss: 0.2817\tLR: 0.020000\n",
            "Training Epoch: 65 [12160/50000]\tLoss: 0.2714\tLR: 0.020000\n",
            "Training Epoch: 65 [12288/50000]\tLoss: 0.3644\tLR: 0.020000\n",
            "Training Epoch: 65 [12416/50000]\tLoss: 0.4018\tLR: 0.020000\n",
            "Training Epoch: 65 [12544/50000]\tLoss: 0.4055\tLR: 0.020000\n",
            "Training Epoch: 65 [12672/50000]\tLoss: 0.4884\tLR: 0.020000\n",
            "Training Epoch: 65 [12800/50000]\tLoss: 0.4312\tLR: 0.020000\n",
            "Training Epoch: 65 [12928/50000]\tLoss: 0.3884\tLR: 0.020000\n",
            "Training Epoch: 65 [13056/50000]\tLoss: 0.4009\tLR: 0.020000\n",
            "Training Epoch: 65 [13184/50000]\tLoss: 0.2987\tLR: 0.020000\n",
            "Training Epoch: 65 [13312/50000]\tLoss: 0.3164\tLR: 0.020000\n",
            "Training Epoch: 65 [13440/50000]\tLoss: 0.3879\tLR: 0.020000\n",
            "Training Epoch: 65 [13568/50000]\tLoss: 0.4029\tLR: 0.020000\n",
            "Training Epoch: 65 [13696/50000]\tLoss: 0.2633\tLR: 0.020000\n",
            "Training Epoch: 65 [13824/50000]\tLoss: 0.4426\tLR: 0.020000\n",
            "Training Epoch: 65 [13952/50000]\tLoss: 0.4394\tLR: 0.020000\n",
            "Training Epoch: 65 [14080/50000]\tLoss: 0.3040\tLR: 0.020000\n",
            "Training Epoch: 65 [14208/50000]\tLoss: 0.4708\tLR: 0.020000\n",
            "Training Epoch: 65 [14336/50000]\tLoss: 0.4068\tLR: 0.020000\n",
            "Training Epoch: 65 [14464/50000]\tLoss: 0.4598\tLR: 0.020000\n",
            "Training Epoch: 65 [14592/50000]\tLoss: 0.3350\tLR: 0.020000\n",
            "Training Epoch: 65 [14720/50000]\tLoss: 0.5348\tLR: 0.020000\n",
            "Training Epoch: 65 [14848/50000]\tLoss: 0.3282\tLR: 0.020000\n",
            "Training Epoch: 65 [14976/50000]\tLoss: 0.3164\tLR: 0.020000\n",
            "Training Epoch: 65 [15104/50000]\tLoss: 0.4019\tLR: 0.020000\n",
            "Training Epoch: 65 [15232/50000]\tLoss: 0.3556\tLR: 0.020000\n",
            "Training Epoch: 65 [15360/50000]\tLoss: 0.2893\tLR: 0.020000\n",
            "Training Epoch: 65 [15488/50000]\tLoss: 0.2914\tLR: 0.020000\n",
            "Training Epoch: 65 [15616/50000]\tLoss: 0.2733\tLR: 0.020000\n",
            "Training Epoch: 65 [15744/50000]\tLoss: 0.2971\tLR: 0.020000\n",
            "Training Epoch: 65 [15872/50000]\tLoss: 0.3222\tLR: 0.020000\n",
            "Training Epoch: 65 [16000/50000]\tLoss: 0.3792\tLR: 0.020000\n",
            "Training Epoch: 65 [16128/50000]\tLoss: 0.3189\tLR: 0.020000\n",
            "Training Epoch: 65 [16256/50000]\tLoss: 0.4405\tLR: 0.020000\n",
            "Training Epoch: 65 [16384/50000]\tLoss: 0.3915\tLR: 0.020000\n",
            "Training Epoch: 65 [16512/50000]\tLoss: 0.5164\tLR: 0.020000\n",
            "Training Epoch: 65 [16640/50000]\tLoss: 0.4211\tLR: 0.020000\n",
            "Training Epoch: 65 [16768/50000]\tLoss: 0.5304\tLR: 0.020000\n",
            "Training Epoch: 65 [16896/50000]\tLoss: 0.3264\tLR: 0.020000\n",
            "Training Epoch: 65 [17024/50000]\tLoss: 0.4037\tLR: 0.020000\n",
            "Training Epoch: 65 [17152/50000]\tLoss: 0.3241\tLR: 0.020000\n",
            "Training Epoch: 65 [17280/50000]\tLoss: 0.3002\tLR: 0.020000\n",
            "Training Epoch: 65 [17408/50000]\tLoss: 0.4141\tLR: 0.020000\n",
            "Training Epoch: 65 [17536/50000]\tLoss: 0.2965\tLR: 0.020000\n",
            "Training Epoch: 65 [17664/50000]\tLoss: 0.2513\tLR: 0.020000\n",
            "Training Epoch: 65 [17792/50000]\tLoss: 0.3713\tLR: 0.020000\n",
            "Training Epoch: 65 [17920/50000]\tLoss: 0.3542\tLR: 0.020000\n",
            "Training Epoch: 65 [18048/50000]\tLoss: 0.3839\tLR: 0.020000\n",
            "Training Epoch: 65 [18176/50000]\tLoss: 0.3058\tLR: 0.020000\n",
            "Training Epoch: 65 [18304/50000]\tLoss: 0.4200\tLR: 0.020000\n",
            "Training Epoch: 65 [18432/50000]\tLoss: 0.4135\tLR: 0.020000\n",
            "Training Epoch: 65 [18560/50000]\tLoss: 0.4149\tLR: 0.020000\n",
            "Training Epoch: 65 [18688/50000]\tLoss: 0.4466\tLR: 0.020000\n",
            "Training Epoch: 65 [18816/50000]\tLoss: 0.4238\tLR: 0.020000\n",
            "Training Epoch: 65 [18944/50000]\tLoss: 0.3423\tLR: 0.020000\n",
            "Training Epoch: 65 [19072/50000]\tLoss: 0.3135\tLR: 0.020000\n",
            "Training Epoch: 65 [19200/50000]\tLoss: 0.3095\tLR: 0.020000\n",
            "Training Epoch: 65 [19328/50000]\tLoss: 0.4336\tLR: 0.020000\n",
            "Training Epoch: 65 [19456/50000]\tLoss: 0.2769\tLR: 0.020000\n",
            "Training Epoch: 65 [19584/50000]\tLoss: 0.3282\tLR: 0.020000\n",
            "Training Epoch: 65 [19712/50000]\tLoss: 0.3756\tLR: 0.020000\n",
            "Training Epoch: 65 [19840/50000]\tLoss: 0.3543\tLR: 0.020000\n",
            "Training Epoch: 65 [19968/50000]\tLoss: 0.3012\tLR: 0.020000\n",
            "Training Epoch: 65 [20096/50000]\tLoss: 0.4624\tLR: 0.020000\n",
            "Training Epoch: 65 [20224/50000]\tLoss: 0.5291\tLR: 0.020000\n",
            "Training Epoch: 65 [20352/50000]\tLoss: 0.3726\tLR: 0.020000\n",
            "Training Epoch: 65 [20480/50000]\tLoss: 0.3486\tLR: 0.020000\n",
            "Training Epoch: 65 [20608/50000]\tLoss: 0.4159\tLR: 0.020000\n",
            "Training Epoch: 65 [20736/50000]\tLoss: 0.3986\tLR: 0.020000\n",
            "Training Epoch: 65 [20864/50000]\tLoss: 0.3428\tLR: 0.020000\n",
            "Training Epoch: 65 [20992/50000]\tLoss: 0.3377\tLR: 0.020000\n",
            "Training Epoch: 65 [21120/50000]\tLoss: 0.4496\tLR: 0.020000\n",
            "Training Epoch: 65 [21248/50000]\tLoss: 0.3299\tLR: 0.020000\n",
            "Training Epoch: 65 [21376/50000]\tLoss: 0.3895\tLR: 0.020000\n",
            "Training Epoch: 65 [21504/50000]\tLoss: 0.4504\tLR: 0.020000\n",
            "Training Epoch: 65 [21632/50000]\tLoss: 0.4560\tLR: 0.020000\n",
            "Training Epoch: 65 [21760/50000]\tLoss: 0.4609\tLR: 0.020000\n",
            "Training Epoch: 65 [21888/50000]\tLoss: 0.2841\tLR: 0.020000\n",
            "Training Epoch: 65 [22016/50000]\tLoss: 0.3610\tLR: 0.020000\n",
            "Training Epoch: 65 [22144/50000]\tLoss: 0.4963\tLR: 0.020000\n",
            "Training Epoch: 65 [22272/50000]\tLoss: 0.3172\tLR: 0.020000\n",
            "Training Epoch: 65 [22400/50000]\tLoss: 0.4019\tLR: 0.020000\n",
            "Training Epoch: 65 [22528/50000]\tLoss: 0.4717\tLR: 0.020000\n",
            "Training Epoch: 65 [22656/50000]\tLoss: 0.4701\tLR: 0.020000\n",
            "Training Epoch: 65 [22784/50000]\tLoss: 0.4529\tLR: 0.020000\n",
            "Training Epoch: 65 [22912/50000]\tLoss: 0.3658\tLR: 0.020000\n",
            "Training Epoch: 65 [23040/50000]\tLoss: 0.3877\tLR: 0.020000\n",
            "Training Epoch: 65 [23168/50000]\tLoss: 0.5080\tLR: 0.020000\n",
            "Training Epoch: 65 [23296/50000]\tLoss: 0.4905\tLR: 0.020000\n",
            "Training Epoch: 65 [23424/50000]\tLoss: 0.3144\tLR: 0.020000\n",
            "Training Epoch: 65 [23552/50000]\tLoss: 0.2764\tLR: 0.020000\n",
            "Training Epoch: 65 [23680/50000]\tLoss: 0.3816\tLR: 0.020000\n",
            "Training Epoch: 65 [23808/50000]\tLoss: 0.4221\tLR: 0.020000\n",
            "Training Epoch: 65 [23936/50000]\tLoss: 0.3948\tLR: 0.020000\n",
            "Training Epoch: 65 [24064/50000]\tLoss: 0.3780\tLR: 0.020000\n",
            "Training Epoch: 65 [24192/50000]\tLoss: 0.3743\tLR: 0.020000\n",
            "Training Epoch: 65 [24320/50000]\tLoss: 0.2771\tLR: 0.020000\n",
            "Training Epoch: 65 [24448/50000]\tLoss: 0.3917\tLR: 0.020000\n",
            "Training Epoch: 65 [24576/50000]\tLoss: 0.2711\tLR: 0.020000\n",
            "Training Epoch: 65 [24704/50000]\tLoss: 0.3774\tLR: 0.020000\n",
            "Training Epoch: 65 [24832/50000]\tLoss: 0.3675\tLR: 0.020000\n",
            "Training Epoch: 65 [24960/50000]\tLoss: 0.4051\tLR: 0.020000\n",
            "Training Epoch: 65 [25088/50000]\tLoss: 0.4243\tLR: 0.020000\n",
            "Training Epoch: 65 [25216/50000]\tLoss: 0.3990\tLR: 0.020000\n",
            "Training Epoch: 65 [25344/50000]\tLoss: 0.4378\tLR: 0.020000\n",
            "Training Epoch: 65 [25472/50000]\tLoss: 0.3722\tLR: 0.020000\n",
            "Training Epoch: 65 [25600/50000]\tLoss: 0.5330\tLR: 0.020000\n",
            "Training Epoch: 65 [25728/50000]\tLoss: 0.3553\tLR: 0.020000\n",
            "Training Epoch: 65 [25856/50000]\tLoss: 0.2589\tLR: 0.020000\n",
            "Training Epoch: 65 [25984/50000]\tLoss: 0.2985\tLR: 0.020000\n",
            "Training Epoch: 65 [26112/50000]\tLoss: 0.3285\tLR: 0.020000\n",
            "Training Epoch: 65 [26240/50000]\tLoss: 0.4055\tLR: 0.020000\n",
            "Training Epoch: 65 [26368/50000]\tLoss: 0.3510\tLR: 0.020000\n",
            "Training Epoch: 65 [26496/50000]\tLoss: 0.4330\tLR: 0.020000\n",
            "Training Epoch: 65 [26624/50000]\tLoss: 0.4742\tLR: 0.020000\n",
            "Training Epoch: 65 [26752/50000]\tLoss: 0.4193\tLR: 0.020000\n",
            "Training Epoch: 65 [26880/50000]\tLoss: 0.3155\tLR: 0.020000\n",
            "Training Epoch: 65 [27008/50000]\tLoss: 0.3499\tLR: 0.020000\n",
            "Training Epoch: 65 [27136/50000]\tLoss: 0.4889\tLR: 0.020000\n",
            "Training Epoch: 65 [27264/50000]\tLoss: 0.3866\tLR: 0.020000\n",
            "Training Epoch: 65 [27392/50000]\tLoss: 0.3589\tLR: 0.020000\n",
            "Training Epoch: 65 [27520/50000]\tLoss: 0.3778\tLR: 0.020000\n",
            "Training Epoch: 65 [27648/50000]\tLoss: 0.4197\tLR: 0.020000\n",
            "Training Epoch: 65 [27776/50000]\tLoss: 0.4970\tLR: 0.020000\n",
            "Training Epoch: 65 [27904/50000]\tLoss: 0.4351\tLR: 0.020000\n",
            "Training Epoch: 65 [28032/50000]\tLoss: 0.3052\tLR: 0.020000\n",
            "Training Epoch: 65 [28160/50000]\tLoss: 0.2975\tLR: 0.020000\n",
            "Training Epoch: 65 [28288/50000]\tLoss: 0.4731\tLR: 0.020000\n",
            "Training Epoch: 65 [28416/50000]\tLoss: 0.4053\tLR: 0.020000\n",
            "Training Epoch: 65 [28544/50000]\tLoss: 0.2938\tLR: 0.020000\n",
            "Training Epoch: 65 [28672/50000]\tLoss: 0.4776\tLR: 0.020000\n",
            "Training Epoch: 65 [28800/50000]\tLoss: 0.3865\tLR: 0.020000\n",
            "Training Epoch: 65 [28928/50000]\tLoss: 0.4874\tLR: 0.020000\n",
            "Training Epoch: 65 [29056/50000]\tLoss: 0.2884\tLR: 0.020000\n",
            "Training Epoch: 65 [29184/50000]\tLoss: 0.2258\tLR: 0.020000\n",
            "Training Epoch: 65 [29312/50000]\tLoss: 0.3935\tLR: 0.020000\n",
            "Training Epoch: 65 [29440/50000]\tLoss: 0.3381\tLR: 0.020000\n",
            "Training Epoch: 65 [29568/50000]\tLoss: 0.3580\tLR: 0.020000\n",
            "Training Epoch: 65 [29696/50000]\tLoss: 0.4435\tLR: 0.020000\n",
            "Training Epoch: 65 [29824/50000]\tLoss: 0.4327\tLR: 0.020000\n",
            "Training Epoch: 65 [29952/50000]\tLoss: 0.5012\tLR: 0.020000\n",
            "Training Epoch: 65 [30080/50000]\tLoss: 0.3683\tLR: 0.020000\n",
            "Training Epoch: 65 [30208/50000]\tLoss: 0.3385\tLR: 0.020000\n",
            "Training Epoch: 65 [30336/50000]\tLoss: 0.3675\tLR: 0.020000\n",
            "Training Epoch: 65 [30464/50000]\tLoss: 0.5465\tLR: 0.020000\n",
            "Training Epoch: 65 [30592/50000]\tLoss: 0.4273\tLR: 0.020000\n",
            "Training Epoch: 65 [30720/50000]\tLoss: 0.6289\tLR: 0.020000\n",
            "Training Epoch: 65 [30848/50000]\tLoss: 0.3655\tLR: 0.020000\n",
            "Training Epoch: 65 [30976/50000]\tLoss: 0.4542\tLR: 0.020000\n",
            "Training Epoch: 65 [31104/50000]\tLoss: 0.4253\tLR: 0.020000\n",
            "Training Epoch: 65 [31232/50000]\tLoss: 0.3832\tLR: 0.020000\n",
            "Training Epoch: 65 [31360/50000]\tLoss: 0.5049\tLR: 0.020000\n",
            "Training Epoch: 65 [31488/50000]\tLoss: 0.4507\tLR: 0.020000\n",
            "Training Epoch: 65 [31616/50000]\tLoss: 0.5893\tLR: 0.020000\n",
            "Training Epoch: 65 [31744/50000]\tLoss: 0.3147\tLR: 0.020000\n",
            "Training Epoch: 65 [31872/50000]\tLoss: 0.2662\tLR: 0.020000\n",
            "Training Epoch: 65 [32000/50000]\tLoss: 0.2942\tLR: 0.020000\n",
            "Training Epoch: 65 [32128/50000]\tLoss: 0.4383\tLR: 0.020000\n",
            "Training Epoch: 65 [32256/50000]\tLoss: 0.3722\tLR: 0.020000\n",
            "Training Epoch: 65 [32384/50000]\tLoss: 0.2706\tLR: 0.020000\n",
            "Training Epoch: 65 [32512/50000]\tLoss: 0.5181\tLR: 0.020000\n",
            "Training Epoch: 65 [32640/50000]\tLoss: 0.5593\tLR: 0.020000\n",
            "Training Epoch: 65 [32768/50000]\tLoss: 0.3862\tLR: 0.020000\n",
            "Training Epoch: 65 [32896/50000]\tLoss: 0.4697\tLR: 0.020000\n",
            "Training Epoch: 65 [33024/50000]\tLoss: 0.4034\tLR: 0.020000\n",
            "Training Epoch: 65 [33152/50000]\tLoss: 0.4952\tLR: 0.020000\n",
            "Training Epoch: 65 [33280/50000]\tLoss: 0.5269\tLR: 0.020000\n",
            "Training Epoch: 65 [33408/50000]\tLoss: 0.3757\tLR: 0.020000\n",
            "Training Epoch: 65 [33536/50000]\tLoss: 0.4488\tLR: 0.020000\n",
            "Training Epoch: 65 [33664/50000]\tLoss: 0.3562\tLR: 0.020000\n",
            "Training Epoch: 65 [33792/50000]\tLoss: 0.4514\tLR: 0.020000\n",
            "Training Epoch: 65 [33920/50000]\tLoss: 0.4148\tLR: 0.020000\n",
            "Training Epoch: 65 [34048/50000]\tLoss: 0.4200\tLR: 0.020000\n",
            "Training Epoch: 65 [34176/50000]\tLoss: 0.5350\tLR: 0.020000\n",
            "Training Epoch: 65 [34304/50000]\tLoss: 0.4157\tLR: 0.020000\n",
            "Training Epoch: 65 [34432/50000]\tLoss: 0.3782\tLR: 0.020000\n",
            "Training Epoch: 65 [34560/50000]\tLoss: 0.2812\tLR: 0.020000\n",
            "Training Epoch: 65 [34688/50000]\tLoss: 0.3316\tLR: 0.020000\n",
            "Training Epoch: 65 [34816/50000]\tLoss: 0.4876\tLR: 0.020000\n",
            "Training Epoch: 65 [34944/50000]\tLoss: 0.4444\tLR: 0.020000\n",
            "Training Epoch: 65 [35072/50000]\tLoss: 0.3402\tLR: 0.020000\n",
            "Training Epoch: 65 [35200/50000]\tLoss: 0.4693\tLR: 0.020000\n",
            "Training Epoch: 65 [35328/50000]\tLoss: 0.4228\tLR: 0.020000\n",
            "Training Epoch: 65 [35456/50000]\tLoss: 0.5083\tLR: 0.020000\n",
            "Training Epoch: 65 [35584/50000]\tLoss: 0.4593\tLR: 0.020000\n",
            "Training Epoch: 65 [35712/50000]\tLoss: 0.3349\tLR: 0.020000\n",
            "Training Epoch: 65 [35840/50000]\tLoss: 0.5407\tLR: 0.020000\n",
            "Training Epoch: 65 [35968/50000]\tLoss: 0.2920\tLR: 0.020000\n",
            "Training Epoch: 65 [36096/50000]\tLoss: 0.5006\tLR: 0.020000\n",
            "Training Epoch: 65 [36224/50000]\tLoss: 0.3271\tLR: 0.020000\n",
            "Training Epoch: 65 [36352/50000]\tLoss: 0.3305\tLR: 0.020000\n",
            "Training Epoch: 65 [36480/50000]\tLoss: 0.2531\tLR: 0.020000\n",
            "Training Epoch: 65 [36608/50000]\tLoss: 0.4501\tLR: 0.020000\n",
            "Training Epoch: 65 [36736/50000]\tLoss: 0.3550\tLR: 0.020000\n",
            "Training Epoch: 65 [36864/50000]\tLoss: 0.4926\tLR: 0.020000\n",
            "Training Epoch: 65 [36992/50000]\tLoss: 0.4297\tLR: 0.020000\n",
            "Training Epoch: 65 [37120/50000]\tLoss: 0.3053\tLR: 0.020000\n",
            "Training Epoch: 65 [37248/50000]\tLoss: 0.3889\tLR: 0.020000\n",
            "Training Epoch: 65 [37376/50000]\tLoss: 0.4014\tLR: 0.020000\n",
            "Training Epoch: 65 [37504/50000]\tLoss: 0.3190\tLR: 0.020000\n",
            "Training Epoch: 65 [37632/50000]\tLoss: 0.5541\tLR: 0.020000\n",
            "Training Epoch: 65 [37760/50000]\tLoss: 0.4714\tLR: 0.020000\n",
            "Training Epoch: 65 [37888/50000]\tLoss: 0.3616\tLR: 0.020000\n",
            "Training Epoch: 65 [38016/50000]\tLoss: 0.5281\tLR: 0.020000\n",
            "Training Epoch: 65 [38144/50000]\tLoss: 0.5225\tLR: 0.020000\n",
            "Training Epoch: 65 [38272/50000]\tLoss: 0.3621\tLR: 0.020000\n",
            "Training Epoch: 65 [38400/50000]\tLoss: 0.3849\tLR: 0.020000\n",
            "Training Epoch: 65 [38528/50000]\tLoss: 0.4544\tLR: 0.020000\n",
            "Training Epoch: 65 [38656/50000]\tLoss: 0.3651\tLR: 0.020000\n",
            "Training Epoch: 65 [38784/50000]\tLoss: 0.4083\tLR: 0.020000\n",
            "Training Epoch: 65 [38912/50000]\tLoss: 0.5553\tLR: 0.020000\n",
            "Training Epoch: 65 [39040/50000]\tLoss: 0.4267\tLR: 0.020000\n",
            "Training Epoch: 65 [39168/50000]\tLoss: 0.3279\tLR: 0.020000\n",
            "Training Epoch: 65 [39296/50000]\tLoss: 0.3222\tLR: 0.020000\n",
            "Training Epoch: 65 [39424/50000]\tLoss: 0.3753\tLR: 0.020000\n",
            "Training Epoch: 65 [39552/50000]\tLoss: 0.3386\tLR: 0.020000\n",
            "Training Epoch: 65 [39680/50000]\tLoss: 0.4830\tLR: 0.020000\n",
            "Training Epoch: 65 [39808/50000]\tLoss: 0.4365\tLR: 0.020000\n",
            "Training Epoch: 65 [39936/50000]\tLoss: 0.4212\tLR: 0.020000\n",
            "Training Epoch: 65 [40064/50000]\tLoss: 0.3286\tLR: 0.020000\n",
            "Training Epoch: 65 [40192/50000]\tLoss: 0.3881\tLR: 0.020000\n",
            "Training Epoch: 65 [40320/50000]\tLoss: 0.4217\tLR: 0.020000\n",
            "Training Epoch: 65 [40448/50000]\tLoss: 0.4472\tLR: 0.020000\n",
            "Training Epoch: 65 [40576/50000]\tLoss: 0.3732\tLR: 0.020000\n",
            "Training Epoch: 65 [40704/50000]\tLoss: 0.4342\tLR: 0.020000\n",
            "Training Epoch: 65 [40832/50000]\tLoss: 0.3211\tLR: 0.020000\n",
            "Training Epoch: 65 [40960/50000]\tLoss: 0.4453\tLR: 0.020000\n",
            "Training Epoch: 65 [41088/50000]\tLoss: 0.4161\tLR: 0.020000\n",
            "Training Epoch: 65 [41216/50000]\tLoss: 0.4179\tLR: 0.020000\n",
            "Training Epoch: 65 [41344/50000]\tLoss: 0.3992\tLR: 0.020000\n",
            "Training Epoch: 65 [41472/50000]\tLoss: 0.3623\tLR: 0.020000\n",
            "Training Epoch: 65 [41600/50000]\tLoss: 0.3886\tLR: 0.020000\n",
            "Training Epoch: 65 [41728/50000]\tLoss: 0.4132\tLR: 0.020000\n",
            "Training Epoch: 65 [41856/50000]\tLoss: 0.4810\tLR: 0.020000\n",
            "Training Epoch: 65 [41984/50000]\tLoss: 0.4769\tLR: 0.020000\n",
            "Training Epoch: 65 [42112/50000]\tLoss: 0.4679\tLR: 0.020000\n",
            "Training Epoch: 65 [42240/50000]\tLoss: 0.4035\tLR: 0.020000\n",
            "Training Epoch: 65 [42368/50000]\tLoss: 0.4177\tLR: 0.020000\n",
            "Training Epoch: 65 [42496/50000]\tLoss: 0.3088\tLR: 0.020000\n",
            "Training Epoch: 65 [42624/50000]\tLoss: 0.3357\tLR: 0.020000\n",
            "Training Epoch: 65 [42752/50000]\tLoss: 0.4258\tLR: 0.020000\n",
            "Training Epoch: 65 [42880/50000]\tLoss: 0.5011\tLR: 0.020000\n",
            "Training Epoch: 65 [43008/50000]\tLoss: 0.4569\tLR: 0.020000\n",
            "Training Epoch: 65 [43136/50000]\tLoss: 0.4495\tLR: 0.020000\n",
            "Training Epoch: 65 [43264/50000]\tLoss: 0.4095\tLR: 0.020000\n",
            "Training Epoch: 65 [43392/50000]\tLoss: 0.3978\tLR: 0.020000\n",
            "Training Epoch: 65 [43520/50000]\tLoss: 0.3335\tLR: 0.020000\n",
            "Training Epoch: 65 [43648/50000]\tLoss: 0.4257\tLR: 0.020000\n",
            "Training Epoch: 65 [43776/50000]\tLoss: 0.4571\tLR: 0.020000\n",
            "Training Epoch: 65 [43904/50000]\tLoss: 0.4094\tLR: 0.020000\n",
            "Training Epoch: 65 [44032/50000]\tLoss: 0.4844\tLR: 0.020000\n",
            "Training Epoch: 65 [44160/50000]\tLoss: 0.4869\tLR: 0.020000\n",
            "Training Epoch: 65 [44288/50000]\tLoss: 0.4891\tLR: 0.020000\n",
            "Training Epoch: 65 [44416/50000]\tLoss: 0.6135\tLR: 0.020000\n",
            "Training Epoch: 65 [44544/50000]\tLoss: 0.4115\tLR: 0.020000\n",
            "Training Epoch: 65 [44672/50000]\tLoss: 0.5362\tLR: 0.020000\n",
            "Training Epoch: 65 [44800/50000]\tLoss: 0.5450\tLR: 0.020000\n",
            "Training Epoch: 65 [44928/50000]\tLoss: 0.4657\tLR: 0.020000\n",
            "Training Epoch: 65 [45056/50000]\tLoss: 0.4037\tLR: 0.020000\n",
            "Training Epoch: 65 [45184/50000]\tLoss: 0.6774\tLR: 0.020000\n",
            "Training Epoch: 65 [45312/50000]\tLoss: 0.4089\tLR: 0.020000\n",
            "Training Epoch: 65 [45440/50000]\tLoss: 0.3433\tLR: 0.020000\n",
            "Training Epoch: 65 [45568/50000]\tLoss: 0.4258\tLR: 0.020000\n",
            "Training Epoch: 65 [45696/50000]\tLoss: 0.4741\tLR: 0.020000\n",
            "Training Epoch: 65 [45824/50000]\tLoss: 0.3459\tLR: 0.020000\n",
            "Training Epoch: 65 [45952/50000]\tLoss: 0.3952\tLR: 0.020000\n",
            "Training Epoch: 65 [46080/50000]\tLoss: 0.5146\tLR: 0.020000\n",
            "Training Epoch: 65 [46208/50000]\tLoss: 0.4931\tLR: 0.020000\n",
            "Training Epoch: 65 [46336/50000]\tLoss: 0.4877\tLR: 0.020000\n",
            "Training Epoch: 65 [46464/50000]\tLoss: 0.3284\tLR: 0.020000\n",
            "Training Epoch: 65 [46592/50000]\tLoss: 0.4857\tLR: 0.020000\n",
            "Training Epoch: 65 [46720/50000]\tLoss: 0.4174\tLR: 0.020000\n",
            "Training Epoch: 65 [46848/50000]\tLoss: 0.4910\tLR: 0.020000\n",
            "Training Epoch: 65 [46976/50000]\tLoss: 0.3914\tLR: 0.020000\n",
            "Training Epoch: 65 [47104/50000]\tLoss: 0.5429\tLR: 0.020000\n",
            "Training Epoch: 65 [47232/50000]\tLoss: 0.3087\tLR: 0.020000\n",
            "Training Epoch: 65 [47360/50000]\tLoss: 0.5279\tLR: 0.020000\n",
            "Training Epoch: 65 [47488/50000]\tLoss: 0.3675\tLR: 0.020000\n",
            "Training Epoch: 65 [47616/50000]\tLoss: 0.4521\tLR: 0.020000\n",
            "Training Epoch: 65 [47744/50000]\tLoss: 0.4798\tLR: 0.020000\n",
            "Training Epoch: 65 [47872/50000]\tLoss: 0.3124\tLR: 0.020000\n",
            "Training Epoch: 65 [48000/50000]\tLoss: 0.2958\tLR: 0.020000\n",
            "Training Epoch: 65 [48128/50000]\tLoss: 0.3761\tLR: 0.020000\n",
            "Training Epoch: 65 [48256/50000]\tLoss: 0.3706\tLR: 0.020000\n",
            "Training Epoch: 65 [48384/50000]\tLoss: 0.2576\tLR: 0.020000\n",
            "Training Epoch: 65 [48512/50000]\tLoss: 0.3971\tLR: 0.020000\n",
            "Training Epoch: 65 [48640/50000]\tLoss: 0.3774\tLR: 0.020000\n",
            "Training Epoch: 65 [48768/50000]\tLoss: 0.3682\tLR: 0.020000\n",
            "Training Epoch: 65 [48896/50000]\tLoss: 0.3736\tLR: 0.020000\n",
            "Training Epoch: 65 [49024/50000]\tLoss: 0.3646\tLR: 0.020000\n",
            "Training Epoch: 65 [49152/50000]\tLoss: 0.5429\tLR: 0.020000\n",
            "Training Epoch: 65 [49280/50000]\tLoss: 0.5228\tLR: 0.020000\n",
            "Training Epoch: 65 [49408/50000]\tLoss: 0.4623\tLR: 0.020000\n",
            "Training Epoch: 65 [49536/50000]\tLoss: 0.4604\tLR: 0.020000\n",
            "Training Epoch: 65 [49664/50000]\tLoss: 0.4553\tLR: 0.020000\n",
            "Training Epoch: 65 [49792/50000]\tLoss: 0.4649\tLR: 0.020000\n",
            "Training Epoch: 65 [49920/50000]\tLoss: 0.5027\tLR: 0.020000\n",
            "Training Epoch: 65 [50000/50000]\tLoss: 0.6187\tLR: 0.020000\n",
            "epoch 65 training time consumed: 144.73s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  133678 KB |    1044 MB |  238201 GB |  238201 GB |\n",
            "|       from large pool |  122880 KB |    1034 MB |  238008 GB |  238008 GB |\n",
            "|       from small pool |   10798 KB |      13 MB |     192 GB |     192 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  133678 KB |    1044 MB |  238201 GB |  238201 GB |\n",
            "|       from large pool |  122880 KB |    1034 MB |  238008 GB |  238008 GB |\n",
            "|       from small pool |   10798 KB |      13 MB |     192 GB |     192 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    1838 MB |    1838 MB |    1838 MB |       0 B  |\n",
            "|       from large pool |    1822 MB |    1822 MB |    1822 MB |       0 B  |\n",
            "|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  433618 KB |  778793 KB |  110755 GB |  110754 GB |\n",
            "|       from large pool |  432128 KB |  776192 KB |  110562 GB |  110562 GB |\n",
            "|       from small pool |    1490 KB |    3494 KB |     192 GB |     192 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     255    |     335    |    9637 K  |    9636 K  |\n",
            "|       from large pool |      24    |      65    |    4723 K  |    4723 K  |\n",
            "|       from small pool |     231    |     274    |    4913 K  |    4913 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     255    |     335    |    9637 K  |    9636 K  |\n",
            "|       from large pool |      24    |      65    |    4723 K  |    4723 K  |\n",
            "|       from small pool |     231    |     274    |    4913 K  |    4913 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      20    |      20    |      20    |       0    |\n",
            "|       from large pool |      12    |      12    |      12    |       0    |\n",
            "|       from small pool |       8    |       8    |       8    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      37    |      45    |    5578 K  |    5578 K  |\n",
            "|       from large pool |      10    |      15    |    2137 K  |    2137 K  |\n",
            "|       from small pool |      27    |      35    |    3440 K  |    3440 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 65, Average loss: 0.0082, Accuracy: 0.7211, Time consumed:9.04s\n",
            "\n",
            "Training Epoch: 66 [128/50000]\tLoss: 0.3108\tLR: 0.020000\n",
            "Training Epoch: 66 [256/50000]\tLoss: 0.3870\tLR: 0.020000\n",
            "Training Epoch: 66 [384/50000]\tLoss: 0.3826\tLR: 0.020000\n",
            "Training Epoch: 66 [512/50000]\tLoss: 0.3557\tLR: 0.020000\n",
            "Training Epoch: 66 [640/50000]\tLoss: 0.3895\tLR: 0.020000\n",
            "Training Epoch: 66 [768/50000]\tLoss: 0.3571\tLR: 0.020000\n",
            "Training Epoch: 66 [896/50000]\tLoss: 0.4068\tLR: 0.020000\n",
            "Training Epoch: 66 [1024/50000]\tLoss: 0.3126\tLR: 0.020000\n",
            "Training Epoch: 66 [1152/50000]\tLoss: 0.2641\tLR: 0.020000\n",
            "Training Epoch: 66 [1280/50000]\tLoss: 0.3592\tLR: 0.020000\n",
            "Training Epoch: 66 [1408/50000]\tLoss: 0.2902\tLR: 0.020000\n",
            "Training Epoch: 66 [1536/50000]\tLoss: 0.3602\tLR: 0.020000\n",
            "Training Epoch: 66 [1664/50000]\tLoss: 0.3680\tLR: 0.020000\n",
            "Training Epoch: 66 [1792/50000]\tLoss: 0.2567\tLR: 0.020000\n",
            "Training Epoch: 66 [1920/50000]\tLoss: 0.4304\tLR: 0.020000\n",
            "Training Epoch: 66 [2048/50000]\tLoss: 0.3249\tLR: 0.020000\n",
            "Training Epoch: 66 [2176/50000]\tLoss: 0.4154\tLR: 0.020000\n",
            "Training Epoch: 66 [2304/50000]\tLoss: 0.3603\tLR: 0.020000\n",
            "Training Epoch: 66 [2432/50000]\tLoss: 0.4592\tLR: 0.020000\n",
            "Training Epoch: 66 [2560/50000]\tLoss: 0.4159\tLR: 0.020000\n",
            "Training Epoch: 66 [2688/50000]\tLoss: 0.3435\tLR: 0.020000\n",
            "Training Epoch: 66 [2816/50000]\tLoss: 0.3413\tLR: 0.020000\n",
            "Training Epoch: 66 [2944/50000]\tLoss: 0.2967\tLR: 0.020000\n",
            "Training Epoch: 66 [3072/50000]\tLoss: 0.3277\tLR: 0.020000\n",
            "Training Epoch: 66 [3200/50000]\tLoss: 0.2976\tLR: 0.020000\n",
            "Training Epoch: 66 [3328/50000]\tLoss: 0.2907\tLR: 0.020000\n",
            "Training Epoch: 66 [3456/50000]\tLoss: 0.3090\tLR: 0.020000\n",
            "Training Epoch: 66 [3584/50000]\tLoss: 0.4540\tLR: 0.020000\n",
            "Training Epoch: 66 [3712/50000]\tLoss: 0.2886\tLR: 0.020000\n",
            "Training Epoch: 66 [3840/50000]\tLoss: 0.3809\tLR: 0.020000\n",
            "Training Epoch: 66 [3968/50000]\tLoss: 0.3687\tLR: 0.020000\n",
            "Training Epoch: 66 [4096/50000]\tLoss: 0.3740\tLR: 0.020000\n",
            "Training Epoch: 66 [4224/50000]\tLoss: 0.3101\tLR: 0.020000\n",
            "Training Epoch: 66 [4352/50000]\tLoss: 0.3679\tLR: 0.020000\n",
            "Training Epoch: 66 [4480/50000]\tLoss: 0.2774\tLR: 0.020000\n",
            "Training Epoch: 66 [4608/50000]\tLoss: 0.3297\tLR: 0.020000\n",
            "Training Epoch: 66 [4736/50000]\tLoss: 0.3452\tLR: 0.020000\n",
            "Training Epoch: 66 [4864/50000]\tLoss: 0.2074\tLR: 0.020000\n",
            "Training Epoch: 66 [4992/50000]\tLoss: 0.4350\tLR: 0.020000\n",
            "Training Epoch: 66 [5120/50000]\tLoss: 0.3602\tLR: 0.020000\n",
            "Training Epoch: 66 [5248/50000]\tLoss: 0.3421\tLR: 0.020000\n",
            "Training Epoch: 66 [5376/50000]\tLoss: 0.3426\tLR: 0.020000\n",
            "Training Epoch: 66 [5504/50000]\tLoss: 0.3437\tLR: 0.020000\n",
            "Training Epoch: 66 [5632/50000]\tLoss: 0.2817\tLR: 0.020000\n",
            "Training Epoch: 66 [5760/50000]\tLoss: 0.2973\tLR: 0.020000\n",
            "Training Epoch: 66 [5888/50000]\tLoss: 0.3086\tLR: 0.020000\n",
            "Training Epoch: 66 [6016/50000]\tLoss: 0.2080\tLR: 0.020000\n",
            "Training Epoch: 66 [6144/50000]\tLoss: 0.4534\tLR: 0.020000\n",
            "Training Epoch: 66 [6272/50000]\tLoss: 0.3047\tLR: 0.020000\n",
            "Training Epoch: 66 [6400/50000]\tLoss: 0.4914\tLR: 0.020000\n",
            "Training Epoch: 66 [6528/50000]\tLoss: 0.2672\tLR: 0.020000\n",
            "Training Epoch: 66 [6656/50000]\tLoss: 0.3164\tLR: 0.020000\n",
            "Training Epoch: 66 [6784/50000]\tLoss: 0.3196\tLR: 0.020000\n",
            "Training Epoch: 66 [6912/50000]\tLoss: 0.3751\tLR: 0.020000\n",
            "Training Epoch: 66 [7040/50000]\tLoss: 0.2794\tLR: 0.020000\n",
            "Training Epoch: 66 [7168/50000]\tLoss: 0.3891\tLR: 0.020000\n",
            "Training Epoch: 66 [7296/50000]\tLoss: 0.2724\tLR: 0.020000\n",
            "Training Epoch: 66 [7424/50000]\tLoss: 0.3561\tLR: 0.020000\n",
            "Training Epoch: 66 [7552/50000]\tLoss: 0.3531\tLR: 0.020000\n",
            "Training Epoch: 66 [7680/50000]\tLoss: 0.2563\tLR: 0.020000\n",
            "Training Epoch: 66 [7808/50000]\tLoss: 0.3025\tLR: 0.020000\n",
            "Training Epoch: 66 [7936/50000]\tLoss: 0.4107\tLR: 0.020000\n",
            "Training Epoch: 66 [8064/50000]\tLoss: 0.3631\tLR: 0.020000\n",
            "Training Epoch: 66 [8192/50000]\tLoss: 0.3493\tLR: 0.020000\n",
            "Training Epoch: 66 [8320/50000]\tLoss: 0.4178\tLR: 0.020000\n",
            "Training Epoch: 66 [8448/50000]\tLoss: 0.3949\tLR: 0.020000\n",
            "Training Epoch: 66 [8576/50000]\tLoss: 0.4058\tLR: 0.020000\n",
            "Training Epoch: 66 [8704/50000]\tLoss: 0.3156\tLR: 0.020000\n",
            "Training Epoch: 66 [8832/50000]\tLoss: 0.2523\tLR: 0.020000\n",
            "Training Epoch: 66 [8960/50000]\tLoss: 0.3951\tLR: 0.020000\n",
            "Training Epoch: 66 [9088/50000]\tLoss: 0.3761\tLR: 0.020000\n",
            "Training Epoch: 66 [9216/50000]\tLoss: 0.3352\tLR: 0.020000\n",
            "Training Epoch: 66 [9344/50000]\tLoss: 0.2819\tLR: 0.020000\n",
            "Training Epoch: 66 [9472/50000]\tLoss: 0.3465\tLR: 0.020000\n",
            "Training Epoch: 66 [9600/50000]\tLoss: 0.4646\tLR: 0.020000\n",
            "Training Epoch: 66 [9728/50000]\tLoss: 0.4693\tLR: 0.020000\n",
            "Training Epoch: 66 [9856/50000]\tLoss: 0.3230\tLR: 0.020000\n",
            "Training Epoch: 66 [9984/50000]\tLoss: 0.2487\tLR: 0.020000\n",
            "Training Epoch: 66 [10112/50000]\tLoss: 0.2162\tLR: 0.020000\n",
            "Training Epoch: 66 [10240/50000]\tLoss: 0.3355\tLR: 0.020000\n",
            "Training Epoch: 66 [10368/50000]\tLoss: 0.3747\tLR: 0.020000\n",
            "Training Epoch: 66 [10496/50000]\tLoss: 0.3427\tLR: 0.020000\n",
            "Training Epoch: 66 [10624/50000]\tLoss: 0.4495\tLR: 0.020000\n",
            "Training Epoch: 66 [10752/50000]\tLoss: 0.3829\tLR: 0.020000\n",
            "Training Epoch: 66 [10880/50000]\tLoss: 0.3688\tLR: 0.020000\n",
            "Training Epoch: 66 [11008/50000]\tLoss: 0.4569\tLR: 0.020000\n",
            "Training Epoch: 66 [11136/50000]\tLoss: 0.3097\tLR: 0.020000\n",
            "Training Epoch: 66 [11264/50000]\tLoss: 0.3929\tLR: 0.020000\n",
            "Training Epoch: 66 [11392/50000]\tLoss: 0.4701\tLR: 0.020000\n",
            "Training Epoch: 66 [11520/50000]\tLoss: 0.4172\tLR: 0.020000\n",
            "Training Epoch: 66 [11648/50000]\tLoss: 0.4440\tLR: 0.020000\n",
            "Training Epoch: 66 [11776/50000]\tLoss: 0.2605\tLR: 0.020000\n",
            "Training Epoch: 66 [11904/50000]\tLoss: 0.3812\tLR: 0.020000\n",
            "Training Epoch: 66 [12032/50000]\tLoss: 0.3032\tLR: 0.020000\n",
            "Training Epoch: 66 [12160/50000]\tLoss: 0.3883\tLR: 0.020000\n",
            "Training Epoch: 66 [12288/50000]\tLoss: 0.3502\tLR: 0.020000\n",
            "Training Epoch: 66 [12416/50000]\tLoss: 0.4178\tLR: 0.020000\n",
            "Training Epoch: 66 [12544/50000]\tLoss: 0.4495\tLR: 0.020000\n",
            "Training Epoch: 66 [12672/50000]\tLoss: 0.3982\tLR: 0.020000\n",
            "Training Epoch: 66 [12800/50000]\tLoss: 0.2054\tLR: 0.020000\n",
            "Training Epoch: 66 [12928/50000]\tLoss: 0.3686\tLR: 0.020000\n",
            "Training Epoch: 66 [13056/50000]\tLoss: 0.3414\tLR: 0.020000\n",
            "Training Epoch: 66 [13184/50000]\tLoss: 0.3661\tLR: 0.020000\n",
            "Training Epoch: 66 [13312/50000]\tLoss: 0.3576\tLR: 0.020000\n",
            "Training Epoch: 66 [13440/50000]\tLoss: 0.3508\tLR: 0.020000\n",
            "Training Epoch: 66 [13568/50000]\tLoss: 0.2859\tLR: 0.020000\n",
            "Training Epoch: 66 [13696/50000]\tLoss: 0.3128\tLR: 0.020000\n",
            "Training Epoch: 66 [13824/50000]\tLoss: 0.4432\tLR: 0.020000\n",
            "Training Epoch: 66 [13952/50000]\tLoss: 0.3233\tLR: 0.020000\n",
            "Training Epoch: 66 [14080/50000]\tLoss: 0.3283\tLR: 0.020000\n",
            "Training Epoch: 66 [14208/50000]\tLoss: 0.2644\tLR: 0.020000\n",
            "Training Epoch: 66 [14336/50000]\tLoss: 0.3244\tLR: 0.020000\n",
            "Training Epoch: 66 [14464/50000]\tLoss: 0.2949\tLR: 0.020000\n",
            "Training Epoch: 66 [14592/50000]\tLoss: 0.3152\tLR: 0.020000\n",
            "Training Epoch: 66 [14720/50000]\tLoss: 0.2797\tLR: 0.020000\n",
            "Training Epoch: 66 [14848/50000]\tLoss: 0.4785\tLR: 0.020000\n",
            "Training Epoch: 66 [14976/50000]\tLoss: 0.4207\tLR: 0.020000\n",
            "Training Epoch: 66 [15104/50000]\tLoss: 0.3587\tLR: 0.020000\n",
            "Training Epoch: 66 [15232/50000]\tLoss: 0.3994\tLR: 0.020000\n",
            "Training Epoch: 66 [15360/50000]\tLoss: 0.3234\tLR: 0.020000\n",
            "Training Epoch: 66 [15488/50000]\tLoss: 0.5823\tLR: 0.020000\n",
            "Training Epoch: 66 [15616/50000]\tLoss: 0.4617\tLR: 0.020000\n",
            "Training Epoch: 66 [15744/50000]\tLoss: 0.3732\tLR: 0.020000\n",
            "Training Epoch: 66 [15872/50000]\tLoss: 0.4881\tLR: 0.020000\n",
            "Training Epoch: 66 [16000/50000]\tLoss: 0.3629\tLR: 0.020000\n",
            "Training Epoch: 66 [16128/50000]\tLoss: 0.3100\tLR: 0.020000\n",
            "Training Epoch: 66 [16256/50000]\tLoss: 0.3956\tLR: 0.020000\n",
            "Training Epoch: 66 [16384/50000]\tLoss: 0.4551\tLR: 0.020000\n",
            "Training Epoch: 66 [16512/50000]\tLoss: 0.1964\tLR: 0.020000\n",
            "Training Epoch: 66 [16640/50000]\tLoss: 0.3668\tLR: 0.020000\n",
            "Training Epoch: 66 [16768/50000]\tLoss: 0.3256\tLR: 0.020000\n",
            "Training Epoch: 66 [16896/50000]\tLoss: 0.4453\tLR: 0.020000\n",
            "Training Epoch: 66 [17024/50000]\tLoss: 0.4885\tLR: 0.020000\n",
            "Training Epoch: 66 [17152/50000]\tLoss: 0.4954\tLR: 0.020000\n",
            "Training Epoch: 66 [17280/50000]\tLoss: 0.3487\tLR: 0.020000\n",
            "Training Epoch: 66 [17408/50000]\tLoss: 0.3467\tLR: 0.020000\n",
            "Training Epoch: 66 [17536/50000]\tLoss: 0.2537\tLR: 0.020000\n",
            "Training Epoch: 66 [17664/50000]\tLoss: 0.3236\tLR: 0.020000\n",
            "Training Epoch: 66 [17792/50000]\tLoss: 0.4064\tLR: 0.020000\n",
            "Training Epoch: 66 [17920/50000]\tLoss: 0.3150\tLR: 0.020000\n",
            "Training Epoch: 66 [18048/50000]\tLoss: 0.3690\tLR: 0.020000\n",
            "Training Epoch: 66 [18176/50000]\tLoss: 0.4647\tLR: 0.020000\n",
            "Training Epoch: 66 [18304/50000]\tLoss: 0.3209\tLR: 0.020000\n",
            "Training Epoch: 66 [18432/50000]\tLoss: 0.3744\tLR: 0.020000\n",
            "Training Epoch: 66 [18560/50000]\tLoss: 0.2875\tLR: 0.020000\n",
            "Training Epoch: 66 [18688/50000]\tLoss: 0.2724\tLR: 0.020000\n",
            "Training Epoch: 66 [18816/50000]\tLoss: 0.3539\tLR: 0.020000\n",
            "Training Epoch: 66 [18944/50000]\tLoss: 0.3305\tLR: 0.020000\n",
            "Training Epoch: 66 [19072/50000]\tLoss: 0.4186\tLR: 0.020000\n",
            "Training Epoch: 66 [19200/50000]\tLoss: 0.3637\tLR: 0.020000\n",
            "Training Epoch: 66 [19328/50000]\tLoss: 0.4322\tLR: 0.020000\n",
            "Training Epoch: 66 [19456/50000]\tLoss: 0.4687\tLR: 0.020000\n",
            "Training Epoch: 66 [19584/50000]\tLoss: 0.3761\tLR: 0.020000\n",
            "Training Epoch: 66 [19712/50000]\tLoss: 0.3401\tLR: 0.020000\n",
            "Training Epoch: 66 [19840/50000]\tLoss: 0.3755\tLR: 0.020000\n",
            "Training Epoch: 66 [19968/50000]\tLoss: 0.4367\tLR: 0.020000\n",
            "Training Epoch: 66 [20096/50000]\tLoss: 0.2775\tLR: 0.020000\n",
            "Training Epoch: 66 [20224/50000]\tLoss: 0.3290\tLR: 0.020000\n",
            "Training Epoch: 66 [20352/50000]\tLoss: 0.4380\tLR: 0.020000\n",
            "Training Epoch: 66 [20480/50000]\tLoss: 0.3951\tLR: 0.020000\n",
            "Training Epoch: 66 [20608/50000]\tLoss: 0.4296\tLR: 0.020000\n",
            "Training Epoch: 66 [20736/50000]\tLoss: 0.3722\tLR: 0.020000\n",
            "Training Epoch: 66 [20864/50000]\tLoss: 0.3435\tLR: 0.020000\n",
            "Training Epoch: 66 [20992/50000]\tLoss: 0.4214\tLR: 0.020000\n",
            "Training Epoch: 66 [21120/50000]\tLoss: 0.5171\tLR: 0.020000\n",
            "Training Epoch: 66 [21248/50000]\tLoss: 0.2960\tLR: 0.020000\n",
            "Training Epoch: 66 [21376/50000]\tLoss: 0.3619\tLR: 0.020000\n",
            "Training Epoch: 66 [21504/50000]\tLoss: 0.2987\tLR: 0.020000\n",
            "Training Epoch: 66 [21632/50000]\tLoss: 0.2491\tLR: 0.020000\n",
            "Training Epoch: 66 [21760/50000]\tLoss: 0.3503\tLR: 0.020000\n",
            "Training Epoch: 66 [21888/50000]\tLoss: 0.2934\tLR: 0.020000\n",
            "Training Epoch: 66 [22016/50000]\tLoss: 0.5390\tLR: 0.020000\n",
            "Training Epoch: 66 [22144/50000]\tLoss: 0.3811\tLR: 0.020000\n",
            "Training Epoch: 66 [22272/50000]\tLoss: 0.2961\tLR: 0.020000\n",
            "Training Epoch: 66 [22400/50000]\tLoss: 0.3507\tLR: 0.020000\n",
            "Training Epoch: 66 [22528/50000]\tLoss: 0.3446\tLR: 0.020000\n",
            "Training Epoch: 66 [22656/50000]\tLoss: 0.3417\tLR: 0.020000\n",
            "Training Epoch: 66 [22784/50000]\tLoss: 0.4118\tLR: 0.020000\n",
            "Training Epoch: 66 [22912/50000]\tLoss: 0.3403\tLR: 0.020000\n",
            "Training Epoch: 66 [23040/50000]\tLoss: 0.4877\tLR: 0.020000\n",
            "Training Epoch: 66 [23168/50000]\tLoss: 0.3797\tLR: 0.020000\n",
            "Training Epoch: 66 [23296/50000]\tLoss: 0.4512\tLR: 0.020000\n",
            "Training Epoch: 66 [23424/50000]\tLoss: 0.3628\tLR: 0.020000\n",
            "Training Epoch: 66 [23552/50000]\tLoss: 0.4434\tLR: 0.020000\n",
            "Training Epoch: 66 [23680/50000]\tLoss: 0.4925\tLR: 0.020000\n",
            "Training Epoch: 66 [23808/50000]\tLoss: 0.5689\tLR: 0.020000\n",
            "Training Epoch: 66 [23936/50000]\tLoss: 0.3278\tLR: 0.020000\n",
            "Training Epoch: 66 [24064/50000]\tLoss: 0.3365\tLR: 0.020000\n",
            "Training Epoch: 66 [24192/50000]\tLoss: 0.3128\tLR: 0.020000\n",
            "Training Epoch: 66 [24320/50000]\tLoss: 0.4495\tLR: 0.020000\n",
            "Training Epoch: 66 [24448/50000]\tLoss: 0.4614\tLR: 0.020000\n",
            "Training Epoch: 66 [24576/50000]\tLoss: 0.5289\tLR: 0.020000\n",
            "Training Epoch: 66 [24704/50000]\tLoss: 0.4180\tLR: 0.020000\n",
            "Training Epoch: 66 [24832/50000]\tLoss: 0.4916\tLR: 0.020000\n",
            "Training Epoch: 66 [24960/50000]\tLoss: 0.2983\tLR: 0.020000\n",
            "Training Epoch: 66 [25088/50000]\tLoss: 0.4329\tLR: 0.020000\n",
            "Training Epoch: 66 [25216/50000]\tLoss: 0.3609\tLR: 0.020000\n",
            "Training Epoch: 66 [25344/50000]\tLoss: 0.3577\tLR: 0.020000\n",
            "Training Epoch: 66 [25472/50000]\tLoss: 0.4532\tLR: 0.020000\n",
            "Training Epoch: 66 [25600/50000]\tLoss: 0.4317\tLR: 0.020000\n",
            "Training Epoch: 66 [25728/50000]\tLoss: 0.3064\tLR: 0.020000\n",
            "Training Epoch: 66 [25856/50000]\tLoss: 0.3705\tLR: 0.020000\n",
            "Training Epoch: 66 [25984/50000]\tLoss: 0.4471\tLR: 0.020000\n",
            "Training Epoch: 66 [26112/50000]\tLoss: 0.3975\tLR: 0.020000\n",
            "Training Epoch: 66 [26240/50000]\tLoss: 0.4895\tLR: 0.020000\n",
            "Training Epoch: 66 [26368/50000]\tLoss: 0.3266\tLR: 0.020000\n",
            "Training Epoch: 66 [26496/50000]\tLoss: 0.2987\tLR: 0.020000\n",
            "Training Epoch: 66 [26624/50000]\tLoss: 0.4180\tLR: 0.020000\n",
            "Training Epoch: 66 [26752/50000]\tLoss: 0.3872\tLR: 0.020000\n",
            "Training Epoch: 66 [26880/50000]\tLoss: 0.3095\tLR: 0.020000\n",
            "Training Epoch: 66 [27008/50000]\tLoss: 0.4274\tLR: 0.020000\n",
            "Training Epoch: 66 [27136/50000]\tLoss: 0.4668\tLR: 0.020000\n",
            "Training Epoch: 66 [27264/50000]\tLoss: 0.3068\tLR: 0.020000\n",
            "Training Epoch: 66 [27392/50000]\tLoss: 0.3411\tLR: 0.020000\n",
            "Training Epoch: 66 [27520/50000]\tLoss: 0.3649\tLR: 0.020000\n",
            "Training Epoch: 66 [27648/50000]\tLoss: 0.5761\tLR: 0.020000\n",
            "Training Epoch: 66 [27776/50000]\tLoss: 0.3616\tLR: 0.020000\n",
            "Training Epoch: 66 [27904/50000]\tLoss: 0.3621\tLR: 0.020000\n",
            "Training Epoch: 66 [28032/50000]\tLoss: 0.3252\tLR: 0.020000\n",
            "Training Epoch: 66 [28160/50000]\tLoss: 0.3918\tLR: 0.020000\n",
            "Training Epoch: 66 [28288/50000]\tLoss: 0.3682\tLR: 0.020000\n",
            "Training Epoch: 66 [28416/50000]\tLoss: 0.3438\tLR: 0.020000\n",
            "Training Epoch: 66 [28544/50000]\tLoss: 0.2876\tLR: 0.020000\n",
            "Training Epoch: 66 [28672/50000]\tLoss: 0.3561\tLR: 0.020000\n",
            "Training Epoch: 66 [28800/50000]\tLoss: 0.3285\tLR: 0.020000\n",
            "Training Epoch: 66 [28928/50000]\tLoss: 0.3464\tLR: 0.020000\n",
            "Training Epoch: 66 [29056/50000]\tLoss: 0.2524\tLR: 0.020000\n",
            "Training Epoch: 66 [29184/50000]\tLoss: 0.3856\tLR: 0.020000\n",
            "Training Epoch: 66 [29312/50000]\tLoss: 0.4256\tLR: 0.020000\n",
            "Training Epoch: 66 [29440/50000]\tLoss: 0.3658\tLR: 0.020000\n",
            "Training Epoch: 66 [29568/50000]\tLoss: 0.4182\tLR: 0.020000\n",
            "Training Epoch: 66 [29696/50000]\tLoss: 0.3286\tLR: 0.020000\n",
            "Training Epoch: 66 [29824/50000]\tLoss: 0.3496\tLR: 0.020000\n",
            "Training Epoch: 66 [29952/50000]\tLoss: 0.3544\tLR: 0.020000\n",
            "Training Epoch: 66 [30080/50000]\tLoss: 0.4672\tLR: 0.020000\n",
            "Training Epoch: 66 [30208/50000]\tLoss: 0.4870\tLR: 0.020000\n",
            "Training Epoch: 66 [30336/50000]\tLoss: 0.4666\tLR: 0.020000\n",
            "Training Epoch: 66 [30464/50000]\tLoss: 0.3710\tLR: 0.020000\n",
            "Training Epoch: 66 [30592/50000]\tLoss: 0.4058\tLR: 0.020000\n",
            "Training Epoch: 66 [30720/50000]\tLoss: 0.3127\tLR: 0.020000\n",
            "Training Epoch: 66 [30848/50000]\tLoss: 0.4301\tLR: 0.020000\n",
            "Training Epoch: 66 [30976/50000]\tLoss: 0.4569\tLR: 0.020000\n",
            "Training Epoch: 66 [31104/50000]\tLoss: 0.4083\tLR: 0.020000\n",
            "Training Epoch: 66 [31232/50000]\tLoss: 0.2590\tLR: 0.020000\n",
            "Training Epoch: 66 [31360/50000]\tLoss: 0.3787\tLR: 0.020000\n",
            "Training Epoch: 66 [31488/50000]\tLoss: 0.4245\tLR: 0.020000\n",
            "Training Epoch: 66 [31616/50000]\tLoss: 0.3472\tLR: 0.020000\n",
            "Training Epoch: 66 [31744/50000]\tLoss: 0.3249\tLR: 0.020000\n",
            "Training Epoch: 66 [31872/50000]\tLoss: 0.4237\tLR: 0.020000\n",
            "Training Epoch: 66 [32000/50000]\tLoss: 0.3717\tLR: 0.020000\n",
            "Training Epoch: 66 [32128/50000]\tLoss: 0.4138\tLR: 0.020000\n",
            "Training Epoch: 66 [32256/50000]\tLoss: 0.3426\tLR: 0.020000\n",
            "Training Epoch: 66 [32384/50000]\tLoss: 0.3468\tLR: 0.020000\n",
            "Training Epoch: 66 [32512/50000]\tLoss: 0.4477\tLR: 0.020000\n",
            "Training Epoch: 66 [32640/50000]\tLoss: 0.3531\tLR: 0.020000\n",
            "Training Epoch: 66 [32768/50000]\tLoss: 0.3352\tLR: 0.020000\n",
            "Training Epoch: 66 [32896/50000]\tLoss: 0.3493\tLR: 0.020000\n",
            "Training Epoch: 66 [33024/50000]\tLoss: 0.4152\tLR: 0.020000\n",
            "Training Epoch: 66 [33152/50000]\tLoss: 0.2549\tLR: 0.020000\n",
            "Training Epoch: 66 [33280/50000]\tLoss: 0.4385\tLR: 0.020000\n",
            "Training Epoch: 66 [33408/50000]\tLoss: 0.3172\tLR: 0.020000\n",
            "Training Epoch: 66 [33536/50000]\tLoss: 0.2608\tLR: 0.020000\n",
            "Training Epoch: 66 [33664/50000]\tLoss: 0.4039\tLR: 0.020000\n",
            "Training Epoch: 66 [33792/50000]\tLoss: 0.3334\tLR: 0.020000\n",
            "Training Epoch: 66 [33920/50000]\tLoss: 0.3552\tLR: 0.020000\n",
            "Training Epoch: 66 [34048/50000]\tLoss: 0.4348\tLR: 0.020000\n",
            "Training Epoch: 66 [34176/50000]\tLoss: 0.3039\tLR: 0.020000\n",
            "Training Epoch: 66 [34304/50000]\tLoss: 0.4449\tLR: 0.020000\n",
            "Training Epoch: 66 [34432/50000]\tLoss: 0.3863\tLR: 0.020000\n",
            "Training Epoch: 66 [34560/50000]\tLoss: 0.3272\tLR: 0.020000\n",
            "Training Epoch: 66 [34688/50000]\tLoss: 0.3824\tLR: 0.020000\n",
            "Training Epoch: 66 [34816/50000]\tLoss: 0.3962\tLR: 0.020000\n",
            "Training Epoch: 66 [34944/50000]\tLoss: 0.4084\tLR: 0.020000\n",
            "Training Epoch: 66 [35072/50000]\tLoss: 0.3910\tLR: 0.020000\n",
            "Training Epoch: 66 [35200/50000]\tLoss: 0.4414\tLR: 0.020000\n",
            "Training Epoch: 66 [35328/50000]\tLoss: 0.4229\tLR: 0.020000\n",
            "Training Epoch: 66 [35456/50000]\tLoss: 0.4253\tLR: 0.020000\n",
            "Training Epoch: 66 [35584/50000]\tLoss: 0.4554\tLR: 0.020000\n",
            "Training Epoch: 66 [35712/50000]\tLoss: 0.3524\tLR: 0.020000\n",
            "Training Epoch: 66 [35840/50000]\tLoss: 0.3929\tLR: 0.020000\n",
            "Training Epoch: 66 [35968/50000]\tLoss: 0.3823\tLR: 0.020000\n",
            "Training Epoch: 66 [36096/50000]\tLoss: 0.3462\tLR: 0.020000\n",
            "Training Epoch: 66 [36224/50000]\tLoss: 0.4069\tLR: 0.020000\n",
            "Training Epoch: 66 [36352/50000]\tLoss: 0.4632\tLR: 0.020000\n",
            "Training Epoch: 66 [36480/50000]\tLoss: 0.3970\tLR: 0.020000\n",
            "Training Epoch: 66 [36608/50000]\tLoss: 0.3145\tLR: 0.020000\n",
            "Training Epoch: 66 [36736/50000]\tLoss: 0.4707\tLR: 0.020000\n",
            "Training Epoch: 66 [36864/50000]\tLoss: 0.5033\tLR: 0.020000\n",
            "Training Epoch: 66 [36992/50000]\tLoss: 0.4180\tLR: 0.020000\n",
            "Training Epoch: 66 [37120/50000]\tLoss: 0.3256\tLR: 0.020000\n",
            "Training Epoch: 66 [37248/50000]\tLoss: 0.4138\tLR: 0.020000\n",
            "Training Epoch: 66 [37376/50000]\tLoss: 0.5527\tLR: 0.020000\n",
            "Training Epoch: 66 [37504/50000]\tLoss: 0.3292\tLR: 0.020000\n",
            "Training Epoch: 66 [37632/50000]\tLoss: 0.2682\tLR: 0.020000\n",
            "Training Epoch: 66 [37760/50000]\tLoss: 0.4193\tLR: 0.020000\n",
            "Training Epoch: 66 [37888/50000]\tLoss: 0.4605\tLR: 0.020000\n",
            "Training Epoch: 66 [38016/50000]\tLoss: 0.5076\tLR: 0.020000\n",
            "Training Epoch: 66 [38144/50000]\tLoss: 0.4429\tLR: 0.020000\n",
            "Training Epoch: 66 [38272/50000]\tLoss: 0.4419\tLR: 0.020000\n",
            "Training Epoch: 66 [38400/50000]\tLoss: 0.5916\tLR: 0.020000\n",
            "Training Epoch: 66 [38528/50000]\tLoss: 0.4306\tLR: 0.020000\n",
            "Training Epoch: 66 [38656/50000]\tLoss: 0.4032\tLR: 0.020000\n",
            "Training Epoch: 66 [38784/50000]\tLoss: 0.3849\tLR: 0.020000\n",
            "Training Epoch: 66 [38912/50000]\tLoss: 0.5090\tLR: 0.020000\n",
            "Training Epoch: 66 [39040/50000]\tLoss: 0.4135\tLR: 0.020000\n",
            "Training Epoch: 66 [39168/50000]\tLoss: 0.3937\tLR: 0.020000\n",
            "Training Epoch: 66 [39296/50000]\tLoss: 0.2895\tLR: 0.020000\n",
            "Training Epoch: 66 [39424/50000]\tLoss: 0.3508\tLR: 0.020000\n",
            "Training Epoch: 66 [39552/50000]\tLoss: 0.4109\tLR: 0.020000\n",
            "Training Epoch: 66 [39680/50000]\tLoss: 0.3393\tLR: 0.020000\n",
            "Training Epoch: 66 [39808/50000]\tLoss: 0.2485\tLR: 0.020000\n",
            "Training Epoch: 66 [39936/50000]\tLoss: 0.3839\tLR: 0.020000\n",
            "Training Epoch: 66 [40064/50000]\tLoss: 0.4601\tLR: 0.020000\n",
            "Training Epoch: 66 [40192/50000]\tLoss: 0.3791\tLR: 0.020000\n",
            "Training Epoch: 66 [40320/50000]\tLoss: 0.3493\tLR: 0.020000\n",
            "Training Epoch: 66 [40448/50000]\tLoss: 0.4426\tLR: 0.020000\n",
            "Training Epoch: 66 [40576/50000]\tLoss: 0.4743\tLR: 0.020000\n",
            "Training Epoch: 66 [40704/50000]\tLoss: 0.4535\tLR: 0.020000\n",
            "Training Epoch: 66 [40832/50000]\tLoss: 0.2447\tLR: 0.020000\n",
            "Training Epoch: 66 [40960/50000]\tLoss: 0.4154\tLR: 0.020000\n",
            "Training Epoch: 66 [41088/50000]\tLoss: 0.3988\tLR: 0.020000\n",
            "Training Epoch: 66 [41216/50000]\tLoss: 0.4066\tLR: 0.020000\n",
            "Training Epoch: 66 [41344/50000]\tLoss: 0.3319\tLR: 0.020000\n",
            "Training Epoch: 66 [41472/50000]\tLoss: 0.3194\tLR: 0.020000\n",
            "Training Epoch: 66 [41600/50000]\tLoss: 0.4723\tLR: 0.020000\n",
            "Training Epoch: 66 [41728/50000]\tLoss: 0.4324\tLR: 0.020000\n",
            "Training Epoch: 66 [41856/50000]\tLoss: 0.3722\tLR: 0.020000\n",
            "Training Epoch: 66 [41984/50000]\tLoss: 0.3886\tLR: 0.020000\n",
            "Training Epoch: 66 [42112/50000]\tLoss: 0.2826\tLR: 0.020000\n",
            "Training Epoch: 66 [42240/50000]\tLoss: 0.5106\tLR: 0.020000\n",
            "Training Epoch: 66 [42368/50000]\tLoss: 0.5002\tLR: 0.020000\n",
            "Training Epoch: 66 [42496/50000]\tLoss: 0.3310\tLR: 0.020000\n",
            "Training Epoch: 66 [42624/50000]\tLoss: 0.5015\tLR: 0.020000\n",
            "Training Epoch: 66 [42752/50000]\tLoss: 0.3495\tLR: 0.020000\n",
            "Training Epoch: 66 [42880/50000]\tLoss: 0.3829\tLR: 0.020000\n",
            "Training Epoch: 66 [43008/50000]\tLoss: 0.3955\tLR: 0.020000\n",
            "Training Epoch: 66 [43136/50000]\tLoss: 0.4282\tLR: 0.020000\n",
            "Training Epoch: 66 [43264/50000]\tLoss: 0.5081\tLR: 0.020000\n",
            "Training Epoch: 66 [43392/50000]\tLoss: 0.3729\tLR: 0.020000\n",
            "Training Epoch: 66 [43520/50000]\tLoss: 0.4228\tLR: 0.020000\n",
            "Training Epoch: 66 [43648/50000]\tLoss: 0.4887\tLR: 0.020000\n",
            "Training Epoch: 66 [43776/50000]\tLoss: 0.3869\tLR: 0.020000\n",
            "Training Epoch: 66 [43904/50000]\tLoss: 0.4287\tLR: 0.020000\n",
            "Training Epoch: 66 [44032/50000]\tLoss: 0.4843\tLR: 0.020000\n",
            "Training Epoch: 66 [44160/50000]\tLoss: 0.5210\tLR: 0.020000\n",
            "Training Epoch: 66 [44288/50000]\tLoss: 0.5376\tLR: 0.020000\n",
            "Training Epoch: 66 [44416/50000]\tLoss: 0.3666\tLR: 0.020000\n",
            "Training Epoch: 66 [44544/50000]\tLoss: 0.3001\tLR: 0.020000\n",
            "Training Epoch: 66 [44672/50000]\tLoss: 0.4017\tLR: 0.020000\n",
            "Training Epoch: 66 [44800/50000]\tLoss: 0.4716\tLR: 0.020000\n",
            "Training Epoch: 66 [44928/50000]\tLoss: 0.3750\tLR: 0.020000\n",
            "Training Epoch: 66 [45056/50000]\tLoss: 0.4316\tLR: 0.020000\n",
            "Training Epoch: 66 [45184/50000]\tLoss: 0.3941\tLR: 0.020000\n",
            "Training Epoch: 66 [45312/50000]\tLoss: 0.4381\tLR: 0.020000\n",
            "Training Epoch: 66 [45440/50000]\tLoss: 0.3667\tLR: 0.020000\n",
            "Training Epoch: 66 [45568/50000]\tLoss: 0.3682\tLR: 0.020000\n",
            "Training Epoch: 66 [45696/50000]\tLoss: 0.3088\tLR: 0.020000\n",
            "Training Epoch: 66 [45824/50000]\tLoss: 0.3674\tLR: 0.020000\n",
            "Training Epoch: 66 [45952/50000]\tLoss: 0.4493\tLR: 0.020000\n",
            "Training Epoch: 66 [46080/50000]\tLoss: 0.3934\tLR: 0.020000\n",
            "Training Epoch: 66 [46208/50000]\tLoss: 0.3198\tLR: 0.020000\n",
            "Training Epoch: 66 [46336/50000]\tLoss: 0.4663\tLR: 0.020000\n",
            "Training Epoch: 66 [46464/50000]\tLoss: 0.3054\tLR: 0.020000\n",
            "Training Epoch: 66 [46592/50000]\tLoss: 0.3875\tLR: 0.020000\n",
            "Training Epoch: 66 [46720/50000]\tLoss: 0.4101\tLR: 0.020000\n",
            "Training Epoch: 66 [46848/50000]\tLoss: 0.5327\tLR: 0.020000\n",
            "Training Epoch: 66 [46976/50000]\tLoss: 0.3560\tLR: 0.020000\n",
            "Training Epoch: 66 [47104/50000]\tLoss: 0.5217\tLR: 0.020000\n",
            "Training Epoch: 66 [47232/50000]\tLoss: 0.5032\tLR: 0.020000\n",
            "Training Epoch: 66 [47360/50000]\tLoss: 0.4849\tLR: 0.020000\n",
            "Training Epoch: 66 [47488/50000]\tLoss: 0.4487\tLR: 0.020000\n",
            "Training Epoch: 66 [47616/50000]\tLoss: 0.4437\tLR: 0.020000\n",
            "Training Epoch: 66 [47744/50000]\tLoss: 0.5420\tLR: 0.020000\n",
            "Training Epoch: 66 [47872/50000]\tLoss: 0.4122\tLR: 0.020000\n",
            "Training Epoch: 66 [48000/50000]\tLoss: 0.3720\tLR: 0.020000\n",
            "Training Epoch: 66 [48128/50000]\tLoss: 0.3151\tLR: 0.020000\n",
            "Training Epoch: 66 [48256/50000]\tLoss: 0.5039\tLR: 0.020000\n",
            "Training Epoch: 66 [48384/50000]\tLoss: 0.3357\tLR: 0.020000\n",
            "Training Epoch: 66 [48512/50000]\tLoss: 0.3771\tLR: 0.020000\n",
            "Training Epoch: 66 [48640/50000]\tLoss: 0.5047\tLR: 0.020000\n",
            "Training Epoch: 66 [48768/50000]\tLoss: 0.4096\tLR: 0.020000\n",
            "Training Epoch: 66 [48896/50000]\tLoss: 0.3388\tLR: 0.020000\n",
            "Training Epoch: 66 [49024/50000]\tLoss: 0.4634\tLR: 0.020000\n",
            "Training Epoch: 66 [49152/50000]\tLoss: 0.5625\tLR: 0.020000\n",
            "Training Epoch: 66 [49280/50000]\tLoss: 0.3839\tLR: 0.020000\n",
            "Training Epoch: 66 [49408/50000]\tLoss: 0.3506\tLR: 0.020000\n",
            "Training Epoch: 66 [49536/50000]\tLoss: 0.4290\tLR: 0.020000\n",
            "Training Epoch: 66 [49664/50000]\tLoss: 0.4320\tLR: 0.020000\n",
            "Training Epoch: 66 [49792/50000]\tLoss: 0.4668\tLR: 0.020000\n",
            "Training Epoch: 66 [49920/50000]\tLoss: 0.4344\tLR: 0.020000\n",
            "Training Epoch: 66 [50000/50000]\tLoss: 0.5035\tLR: 0.020000\n",
            "epoch 66 training time consumed: 144.50s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  133678 KB |    1044 MB |  241865 GB |  241865 GB |\n",
            "|       from large pool |  122880 KB |    1034 MB |  241670 GB |  241670 GB |\n",
            "|       from small pool |   10798 KB |      13 MB |     195 GB |     195 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  133678 KB |    1044 MB |  241865 GB |  241865 GB |\n",
            "|       from large pool |  122880 KB |    1034 MB |  241670 GB |  241670 GB |\n",
            "|       from small pool |   10798 KB |      13 MB |     195 GB |     195 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    1838 MB |    1838 MB |    1838 MB |       0 B  |\n",
            "|       from large pool |    1822 MB |    1822 MB |    1822 MB |       0 B  |\n",
            "|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  433618 KB |  778793 KB |  112458 GB |  112458 GB |\n",
            "|       from large pool |  432128 KB |  776192 KB |  112263 GB |  112263 GB |\n",
            "|       from small pool |    1490 KB |    3494 KB |     195 GB |     195 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     255    |     335    |    9785 K  |    9785 K  |\n",
            "|       from large pool |      24    |      65    |    4796 K  |    4796 K  |\n",
            "|       from small pool |     231    |     274    |    4988 K  |    4988 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     255    |     335    |    9785 K  |    9785 K  |\n",
            "|       from large pool |      24    |      65    |    4796 K  |    4796 K  |\n",
            "|       from small pool |     231    |     274    |    4988 K  |    4988 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      20    |      20    |      20    |       0    |\n",
            "|       from large pool |      12    |      12    |      12    |       0    |\n",
            "|       from small pool |       8    |       8    |       8    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      37    |      45    |    5664 K  |    5664 K  |\n",
            "|       from large pool |      10    |      15    |    2170 K  |    2170 K  |\n",
            "|       from small pool |      27    |      35    |    3494 K  |    3494 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 66, Average loss: 0.0085, Accuracy: 0.7179, Time consumed:9.04s\n",
            "\n",
            "Training Epoch: 67 [128/50000]\tLoss: 0.2424\tLR: 0.020000\n",
            "Training Epoch: 67 [256/50000]\tLoss: 0.4020\tLR: 0.020000\n",
            "Training Epoch: 67 [384/50000]\tLoss: 0.4745\tLR: 0.020000\n",
            "Training Epoch: 67 [512/50000]\tLoss: 0.3074\tLR: 0.020000\n",
            "Training Epoch: 67 [640/50000]\tLoss: 0.3960\tLR: 0.020000\n",
            "Training Epoch: 67 [768/50000]\tLoss: 0.3704\tLR: 0.020000\n",
            "Training Epoch: 67 [896/50000]\tLoss: 0.3519\tLR: 0.020000\n",
            "Training Epoch: 67 [1024/50000]\tLoss: 0.3354\tLR: 0.020000\n",
            "Training Epoch: 67 [1152/50000]\tLoss: 0.3989\tLR: 0.020000\n",
            "Training Epoch: 67 [1280/50000]\tLoss: 0.3727\tLR: 0.020000\n",
            "Training Epoch: 67 [1408/50000]\tLoss: 0.2808\tLR: 0.020000\n",
            "Training Epoch: 67 [1536/50000]\tLoss: 0.3965\tLR: 0.020000\n",
            "Training Epoch: 67 [1664/50000]\tLoss: 0.3697\tLR: 0.020000\n",
            "Training Epoch: 67 [1792/50000]\tLoss: 0.3380\tLR: 0.020000\n",
            "Training Epoch: 67 [1920/50000]\tLoss: 0.3619\tLR: 0.020000\n",
            "Training Epoch: 67 [2048/50000]\tLoss: 0.5181\tLR: 0.020000\n",
            "Training Epoch: 67 [2176/50000]\tLoss: 0.3551\tLR: 0.020000\n",
            "Training Epoch: 67 [2304/50000]\tLoss: 0.3316\tLR: 0.020000\n",
            "Training Epoch: 67 [2432/50000]\tLoss: 0.2507\tLR: 0.020000\n",
            "Training Epoch: 67 [2560/50000]\tLoss: 0.3758\tLR: 0.020000\n",
            "Training Epoch: 67 [2688/50000]\tLoss: 0.3239\tLR: 0.020000\n",
            "Training Epoch: 67 [2816/50000]\tLoss: 0.3623\tLR: 0.020000\n",
            "Training Epoch: 67 [2944/50000]\tLoss: 0.2235\tLR: 0.020000\n",
            "Training Epoch: 67 [3072/50000]\tLoss: 0.2344\tLR: 0.020000\n",
            "Training Epoch: 67 [3200/50000]\tLoss: 0.3237\tLR: 0.020000\n",
            "Training Epoch: 67 [3328/50000]\tLoss: 0.3407\tLR: 0.020000\n",
            "Training Epoch: 67 [3456/50000]\tLoss: 0.2636\tLR: 0.020000\n",
            "Training Epoch: 67 [3584/50000]\tLoss: 0.2582\tLR: 0.020000\n",
            "Training Epoch: 67 [3712/50000]\tLoss: 0.2783\tLR: 0.020000\n",
            "Training Epoch: 67 [3840/50000]\tLoss: 0.3467\tLR: 0.020000\n",
            "Training Epoch: 67 [3968/50000]\tLoss: 0.3565\tLR: 0.020000\n",
            "Training Epoch: 67 [4096/50000]\tLoss: 0.3613\tLR: 0.020000\n",
            "Training Epoch: 67 [4224/50000]\tLoss: 0.3364\tLR: 0.020000\n",
            "Training Epoch: 67 [4352/50000]\tLoss: 0.2464\tLR: 0.020000\n",
            "Training Epoch: 67 [4480/50000]\tLoss: 0.3342\tLR: 0.020000\n",
            "Training Epoch: 67 [4608/50000]\tLoss: 0.3120\tLR: 0.020000\n",
            "Training Epoch: 67 [4736/50000]\tLoss: 0.4613\tLR: 0.020000\n",
            "Training Epoch: 67 [4864/50000]\tLoss: 0.2225\tLR: 0.020000\n",
            "Training Epoch: 67 [4992/50000]\tLoss: 0.3444\tLR: 0.020000\n",
            "Training Epoch: 67 [5120/50000]\tLoss: 0.1759\tLR: 0.020000\n",
            "Training Epoch: 67 [5248/50000]\tLoss: 0.2769\tLR: 0.020000\n",
            "Training Epoch: 67 [5376/50000]\tLoss: 0.3313\tLR: 0.020000\n",
            "Training Epoch: 67 [5504/50000]\tLoss: 0.2768\tLR: 0.020000\n",
            "Training Epoch: 67 [5632/50000]\tLoss: 0.2362\tLR: 0.020000\n",
            "Training Epoch: 67 [5760/50000]\tLoss: 0.2916\tLR: 0.020000\n",
            "Training Epoch: 67 [5888/50000]\tLoss: 0.2997\tLR: 0.020000\n",
            "Training Epoch: 67 [6016/50000]\tLoss: 0.3257\tLR: 0.020000\n",
            "Training Epoch: 67 [6144/50000]\tLoss: 0.3192\tLR: 0.020000\n",
            "Training Epoch: 67 [6272/50000]\tLoss: 0.3029\tLR: 0.020000\n",
            "Training Epoch: 67 [6400/50000]\tLoss: 0.3685\tLR: 0.020000\n",
            "Training Epoch: 67 [6528/50000]\tLoss: 0.3245\tLR: 0.020000\n",
            "Training Epoch: 67 [6656/50000]\tLoss: 0.3451\tLR: 0.020000\n",
            "Training Epoch: 67 [6784/50000]\tLoss: 0.4079\tLR: 0.020000\n",
            "Training Epoch: 67 [6912/50000]\tLoss: 0.3398\tLR: 0.020000\n",
            "Training Epoch: 67 [7040/50000]\tLoss: 0.3448\tLR: 0.020000\n",
            "Training Epoch: 67 [7168/50000]\tLoss: 0.4268\tLR: 0.020000\n",
            "Training Epoch: 67 [7296/50000]\tLoss: 0.4100\tLR: 0.020000\n",
            "Training Epoch: 67 [7424/50000]\tLoss: 0.4140\tLR: 0.020000\n",
            "Training Epoch: 67 [7552/50000]\tLoss: 0.3651\tLR: 0.020000\n",
            "Training Epoch: 67 [7680/50000]\tLoss: 0.2835\tLR: 0.020000\n",
            "Training Epoch: 67 [7808/50000]\tLoss: 0.3507\tLR: 0.020000\n",
            "Training Epoch: 67 [7936/50000]\tLoss: 0.3346\tLR: 0.020000\n",
            "Training Epoch: 67 [8064/50000]\tLoss: 0.3881\tLR: 0.020000\n",
            "Training Epoch: 67 [8192/50000]\tLoss: 0.2936\tLR: 0.020000\n",
            "Training Epoch: 67 [8320/50000]\tLoss: 0.4268\tLR: 0.020000\n",
            "Training Epoch: 67 [8448/50000]\tLoss: 0.3452\tLR: 0.020000\n",
            "Training Epoch: 67 [8576/50000]\tLoss: 0.3097\tLR: 0.020000\n",
            "Training Epoch: 67 [8704/50000]\tLoss: 0.3587\tLR: 0.020000\n",
            "Training Epoch: 67 [8832/50000]\tLoss: 0.2917\tLR: 0.020000\n",
            "Training Epoch: 67 [8960/50000]\tLoss: 0.3901\tLR: 0.020000\n",
            "Training Epoch: 67 [9088/50000]\tLoss: 0.3029\tLR: 0.020000\n",
            "Training Epoch: 67 [9216/50000]\tLoss: 0.2933\tLR: 0.020000\n",
            "Training Epoch: 67 [9344/50000]\tLoss: 0.3442\tLR: 0.020000\n",
            "Training Epoch: 67 [9472/50000]\tLoss: 0.3090\tLR: 0.020000\n",
            "Training Epoch: 67 [9600/50000]\tLoss: 0.4432\tLR: 0.020000\n",
            "Training Epoch: 67 [9728/50000]\tLoss: 0.2657\tLR: 0.020000\n",
            "Training Epoch: 67 [9856/50000]\tLoss: 0.3122\tLR: 0.020000\n",
            "Training Epoch: 67 [9984/50000]\tLoss: 0.3113\tLR: 0.020000\n",
            "Training Epoch: 67 [10112/50000]\tLoss: 0.3441\tLR: 0.020000\n",
            "Training Epoch: 67 [10240/50000]\tLoss: 0.3848\tLR: 0.020000\n",
            "Training Epoch: 67 [10368/50000]\tLoss: 0.3909\tLR: 0.020000\n",
            "Training Epoch: 67 [10496/50000]\tLoss: 0.4214\tLR: 0.020000\n",
            "Training Epoch: 67 [10624/50000]\tLoss: 0.3403\tLR: 0.020000\n",
            "Training Epoch: 67 [10752/50000]\tLoss: 0.3900\tLR: 0.020000\n",
            "Training Epoch: 67 [10880/50000]\tLoss: 0.2360\tLR: 0.020000\n",
            "Training Epoch: 67 [11008/50000]\tLoss: 0.2978\tLR: 0.020000\n",
            "Training Epoch: 67 [11136/50000]\tLoss: 0.5071\tLR: 0.020000\n",
            "Training Epoch: 67 [11264/50000]\tLoss: 0.2946\tLR: 0.020000\n",
            "Training Epoch: 67 [11392/50000]\tLoss: 0.3291\tLR: 0.020000\n",
            "Training Epoch: 67 [11520/50000]\tLoss: 0.2923\tLR: 0.020000\n",
            "Training Epoch: 67 [11648/50000]\tLoss: 0.3576\tLR: 0.020000\n",
            "Training Epoch: 67 [11776/50000]\tLoss: 0.3318\tLR: 0.020000\n",
            "Training Epoch: 67 [11904/50000]\tLoss: 0.3889\tLR: 0.020000\n",
            "Training Epoch: 67 [12032/50000]\tLoss: 0.2803\tLR: 0.020000\n",
            "Training Epoch: 67 [12160/50000]\tLoss: 0.3984\tLR: 0.020000\n",
            "Training Epoch: 67 [12288/50000]\tLoss: 0.4283\tLR: 0.020000\n",
            "Training Epoch: 67 [12416/50000]\tLoss: 0.3307\tLR: 0.020000\n",
            "Training Epoch: 67 [12544/50000]\tLoss: 0.2157\tLR: 0.020000\n",
            "Training Epoch: 67 [12672/50000]\tLoss: 0.3050\tLR: 0.020000\n",
            "Training Epoch: 67 [12800/50000]\tLoss: 0.4122\tLR: 0.020000\n",
            "Training Epoch: 67 [12928/50000]\tLoss: 0.4515\tLR: 0.020000\n",
            "Training Epoch: 67 [13056/50000]\tLoss: 0.3186\tLR: 0.020000\n",
            "Training Epoch: 67 [13184/50000]\tLoss: 0.3457\tLR: 0.020000\n",
            "Training Epoch: 67 [13312/50000]\tLoss: 0.3635\tLR: 0.020000\n",
            "Training Epoch: 67 [13440/50000]\tLoss: 0.2888\tLR: 0.020000\n",
            "Training Epoch: 67 [13568/50000]\tLoss: 0.3377\tLR: 0.020000\n",
            "Training Epoch: 67 [13696/50000]\tLoss: 0.3759\tLR: 0.020000\n",
            "Training Epoch: 67 [13824/50000]\tLoss: 0.2872\tLR: 0.020000\n",
            "Training Epoch: 67 [13952/50000]\tLoss: 0.2596\tLR: 0.020000\n",
            "Training Epoch: 67 [14080/50000]\tLoss: 0.3182\tLR: 0.020000\n",
            "Training Epoch: 67 [14208/50000]\tLoss: 0.2822\tLR: 0.020000\n",
            "Training Epoch: 67 [14336/50000]\tLoss: 0.2567\tLR: 0.020000\n",
            "Training Epoch: 67 [14464/50000]\tLoss: 0.3723\tLR: 0.020000\n",
            "Training Epoch: 67 [14592/50000]\tLoss: 0.2985\tLR: 0.020000\n",
            "Training Epoch: 67 [14720/50000]\tLoss: 0.4078\tLR: 0.020000\n",
            "Training Epoch: 67 [14848/50000]\tLoss: 0.3238\tLR: 0.020000\n",
            "Training Epoch: 67 [14976/50000]\tLoss: 0.3981\tLR: 0.020000\n",
            "Training Epoch: 67 [15104/50000]\tLoss: 0.2518\tLR: 0.020000\n",
            "Training Epoch: 67 [15232/50000]\tLoss: 0.3186\tLR: 0.020000\n",
            "Training Epoch: 67 [15360/50000]\tLoss: 0.5002\tLR: 0.020000\n",
            "Training Epoch: 67 [15488/50000]\tLoss: 0.2997\tLR: 0.020000\n",
            "Training Epoch: 67 [15616/50000]\tLoss: 0.3060\tLR: 0.020000\n",
            "Training Epoch: 67 [15744/50000]\tLoss: 0.2745\tLR: 0.020000\n",
            "Training Epoch: 67 [15872/50000]\tLoss: 0.3450\tLR: 0.020000\n",
            "Training Epoch: 67 [16000/50000]\tLoss: 0.4921\tLR: 0.020000\n",
            "Training Epoch: 67 [16128/50000]\tLoss: 0.4918\tLR: 0.020000\n",
            "Training Epoch: 67 [16256/50000]\tLoss: 0.3580\tLR: 0.020000\n",
            "Training Epoch: 67 [16384/50000]\tLoss: 0.3756\tLR: 0.020000\n",
            "Training Epoch: 67 [16512/50000]\tLoss: 0.2772\tLR: 0.020000\n",
            "Training Epoch: 67 [16640/50000]\tLoss: 0.3779\tLR: 0.020000\n",
            "Training Epoch: 67 [16768/50000]\tLoss: 0.3534\tLR: 0.020000\n",
            "Training Epoch: 67 [16896/50000]\tLoss: 0.3030\tLR: 0.020000\n",
            "Training Epoch: 67 [17024/50000]\tLoss: 0.4271\tLR: 0.020000\n",
            "Training Epoch: 67 [17152/50000]\tLoss: 0.3332\tLR: 0.020000\n",
            "Training Epoch: 67 [17280/50000]\tLoss: 0.3346\tLR: 0.020000\n",
            "Training Epoch: 67 [17408/50000]\tLoss: 0.4041\tLR: 0.020000\n",
            "Training Epoch: 67 [17536/50000]\tLoss: 0.3716\tLR: 0.020000\n",
            "Training Epoch: 67 [17664/50000]\tLoss: 0.4409\tLR: 0.020000\n",
            "Training Epoch: 67 [17792/50000]\tLoss: 0.4522\tLR: 0.020000\n",
            "Training Epoch: 67 [17920/50000]\tLoss: 0.3507\tLR: 0.020000\n",
            "Training Epoch: 67 [18048/50000]\tLoss: 0.2739\tLR: 0.020000\n",
            "Training Epoch: 67 [18176/50000]\tLoss: 0.3350\tLR: 0.020000\n",
            "Training Epoch: 67 [18304/50000]\tLoss: 0.2633\tLR: 0.020000\n",
            "Training Epoch: 67 [18432/50000]\tLoss: 0.4638\tLR: 0.020000\n",
            "Training Epoch: 67 [18560/50000]\tLoss: 0.4355\tLR: 0.020000\n",
            "Training Epoch: 67 [18688/50000]\tLoss: 0.2645\tLR: 0.020000\n",
            "Training Epoch: 67 [18816/50000]\tLoss: 0.3507\tLR: 0.020000\n",
            "Training Epoch: 67 [18944/50000]\tLoss: 0.4149\tLR: 0.020000\n",
            "Training Epoch: 67 [19072/50000]\tLoss: 0.4347\tLR: 0.020000\n",
            "Training Epoch: 67 [19200/50000]\tLoss: 0.4263\tLR: 0.020000\n",
            "Training Epoch: 67 [19328/50000]\tLoss: 0.4555\tLR: 0.020000\n",
            "Training Epoch: 67 [19456/50000]\tLoss: 0.5112\tLR: 0.020000\n",
            "Training Epoch: 67 [19584/50000]\tLoss: 0.3874\tLR: 0.020000\n",
            "Training Epoch: 67 [19712/50000]\tLoss: 0.3823\tLR: 0.020000\n",
            "Training Epoch: 67 [19840/50000]\tLoss: 0.2963\tLR: 0.020000\n",
            "Training Epoch: 67 [19968/50000]\tLoss: 0.2348\tLR: 0.020000\n",
            "Training Epoch: 67 [20096/50000]\tLoss: 0.4259\tLR: 0.020000\n",
            "Training Epoch: 67 [20224/50000]\tLoss: 0.3124\tLR: 0.020000\n",
            "Training Epoch: 67 [20352/50000]\tLoss: 0.4264\tLR: 0.020000\n",
            "Training Epoch: 67 [20480/50000]\tLoss: 0.3292\tLR: 0.020000\n",
            "Training Epoch: 67 [20608/50000]\tLoss: 0.3929\tLR: 0.020000\n",
            "Training Epoch: 67 [20736/50000]\tLoss: 0.3948\tLR: 0.020000\n",
            "Training Epoch: 67 [20864/50000]\tLoss: 0.3535\tLR: 0.020000\n",
            "Training Epoch: 67 [20992/50000]\tLoss: 0.3298\tLR: 0.020000\n",
            "Training Epoch: 67 [21120/50000]\tLoss: 0.4310\tLR: 0.020000\n",
            "Training Epoch: 67 [21248/50000]\tLoss: 0.4684\tLR: 0.020000\n",
            "Training Epoch: 67 [21376/50000]\tLoss: 0.4222\tLR: 0.020000\n",
            "Training Epoch: 67 [21504/50000]\tLoss: 0.4144\tLR: 0.020000\n",
            "Training Epoch: 67 [21632/50000]\tLoss: 0.3154\tLR: 0.020000\n",
            "Training Epoch: 67 [21760/50000]\tLoss: 0.3801\tLR: 0.020000\n",
            "Training Epoch: 67 [21888/50000]\tLoss: 0.3705\tLR: 0.020000\n",
            "Training Epoch: 67 [22016/50000]\tLoss: 0.3663\tLR: 0.020000\n",
            "Training Epoch: 67 [22144/50000]\tLoss: 0.4063\tLR: 0.020000\n",
            "Training Epoch: 67 [22272/50000]\tLoss: 0.3360\tLR: 0.020000\n",
            "Training Epoch: 67 [22400/50000]\tLoss: 0.3537\tLR: 0.020000\n",
            "Training Epoch: 67 [22528/50000]\tLoss: 0.3064\tLR: 0.020000\n",
            "Training Epoch: 67 [22656/50000]\tLoss: 0.3092\tLR: 0.020000\n",
            "Training Epoch: 67 [22784/50000]\tLoss: 0.3959\tLR: 0.020000\n",
            "Training Epoch: 67 [22912/50000]\tLoss: 0.4254\tLR: 0.020000\n",
            "Training Epoch: 67 [23040/50000]\tLoss: 0.3193\tLR: 0.020000\n",
            "Training Epoch: 67 [23168/50000]\tLoss: 0.3693\tLR: 0.020000\n",
            "Training Epoch: 67 [23296/50000]\tLoss: 0.3505\tLR: 0.020000\n",
            "Training Epoch: 67 [23424/50000]\tLoss: 0.3489\tLR: 0.020000\n",
            "Training Epoch: 67 [23552/50000]\tLoss: 0.4190\tLR: 0.020000\n",
            "Training Epoch: 67 [23680/50000]\tLoss: 0.2529\tLR: 0.020000\n",
            "Training Epoch: 67 [23808/50000]\tLoss: 0.3588\tLR: 0.020000\n",
            "Training Epoch: 67 [23936/50000]\tLoss: 0.4254\tLR: 0.020000\n",
            "Training Epoch: 67 [24064/50000]\tLoss: 0.3429\tLR: 0.020000\n",
            "Training Epoch: 67 [24192/50000]\tLoss: 0.4184\tLR: 0.020000\n",
            "Training Epoch: 67 [24320/50000]\tLoss: 0.3512\tLR: 0.020000\n",
            "Training Epoch: 67 [24448/50000]\tLoss: 0.3967\tLR: 0.020000\n",
            "Training Epoch: 67 [24576/50000]\tLoss: 0.3256\tLR: 0.020000\n",
            "Training Epoch: 67 [24704/50000]\tLoss: 0.3974\tLR: 0.020000\n",
            "Training Epoch: 67 [24832/50000]\tLoss: 0.3421\tLR: 0.020000\n",
            "Training Epoch: 67 [24960/50000]\tLoss: 0.3602\tLR: 0.020000\n",
            "Training Epoch: 67 [25088/50000]\tLoss: 0.4969\tLR: 0.020000\n",
            "Training Epoch: 67 [25216/50000]\tLoss: 0.3345\tLR: 0.020000\n",
            "Training Epoch: 67 [25344/50000]\tLoss: 0.3422\tLR: 0.020000\n",
            "Training Epoch: 67 [25472/50000]\tLoss: 0.3285\tLR: 0.020000\n",
            "Training Epoch: 67 [25600/50000]\tLoss: 0.3405\tLR: 0.020000\n",
            "Training Epoch: 67 [25728/50000]\tLoss: 0.3444\tLR: 0.020000\n",
            "Training Epoch: 67 [25856/50000]\tLoss: 0.4370\tLR: 0.020000\n",
            "Training Epoch: 67 [25984/50000]\tLoss: 0.3575\tLR: 0.020000\n",
            "Training Epoch: 67 [26112/50000]\tLoss: 0.2687\tLR: 0.020000\n",
            "Training Epoch: 67 [26240/50000]\tLoss: 0.4455\tLR: 0.020000\n",
            "Training Epoch: 67 [26368/50000]\tLoss: 0.3794\tLR: 0.020000\n",
            "Training Epoch: 67 [26496/50000]\tLoss: 0.4119\tLR: 0.020000\n",
            "Training Epoch: 67 [26624/50000]\tLoss: 0.5107\tLR: 0.020000\n",
            "Training Epoch: 67 [26752/50000]\tLoss: 0.2603\tLR: 0.020000\n",
            "Training Epoch: 67 [26880/50000]\tLoss: 0.3523\tLR: 0.020000\n",
            "Training Epoch: 67 [27008/50000]\tLoss: 0.2912\tLR: 0.020000\n",
            "Training Epoch: 67 [27136/50000]\tLoss: 0.2740\tLR: 0.020000\n",
            "Training Epoch: 67 [27264/50000]\tLoss: 0.4121\tLR: 0.020000\n",
            "Training Epoch: 67 [27392/50000]\tLoss: 0.3586\tLR: 0.020000\n",
            "Training Epoch: 67 [27520/50000]\tLoss: 0.3573\tLR: 0.020000\n",
            "Training Epoch: 67 [27648/50000]\tLoss: 0.3001\tLR: 0.020000\n",
            "Training Epoch: 67 [27776/50000]\tLoss: 0.2916\tLR: 0.020000\n",
            "Training Epoch: 67 [27904/50000]\tLoss: 0.3281\tLR: 0.020000\n",
            "Training Epoch: 67 [28032/50000]\tLoss: 0.2830\tLR: 0.020000\n",
            "Training Epoch: 67 [28160/50000]\tLoss: 0.4201\tLR: 0.020000\n",
            "Training Epoch: 67 [28288/50000]\tLoss: 0.3046\tLR: 0.020000\n",
            "Training Epoch: 67 [28416/50000]\tLoss: 0.3375\tLR: 0.020000\n",
            "Training Epoch: 67 [28544/50000]\tLoss: 0.3561\tLR: 0.020000\n",
            "Training Epoch: 67 [28672/50000]\tLoss: 0.3735\tLR: 0.020000\n",
            "Training Epoch: 67 [28800/50000]\tLoss: 0.3334\tLR: 0.020000\n",
            "Training Epoch: 67 [28928/50000]\tLoss: 0.3480\tLR: 0.020000\n",
            "Training Epoch: 67 [29056/50000]\tLoss: 0.2934\tLR: 0.020000\n",
            "Training Epoch: 67 [29184/50000]\tLoss: 0.3313\tLR: 0.020000\n",
            "Training Epoch: 67 [29312/50000]\tLoss: 0.3079\tLR: 0.020000\n",
            "Training Epoch: 67 [29440/50000]\tLoss: 0.4207\tLR: 0.020000\n",
            "Training Epoch: 67 [29568/50000]\tLoss: 0.5288\tLR: 0.020000\n",
            "Training Epoch: 67 [29696/50000]\tLoss: 0.4192\tLR: 0.020000\n",
            "Training Epoch: 67 [29824/50000]\tLoss: 0.3562\tLR: 0.020000\n",
            "Training Epoch: 67 [29952/50000]\tLoss: 0.3063\tLR: 0.020000\n",
            "Training Epoch: 67 [30080/50000]\tLoss: 0.3383\tLR: 0.020000\n",
            "Training Epoch: 67 [30208/50000]\tLoss: 0.2836\tLR: 0.020000\n",
            "Training Epoch: 67 [30336/50000]\tLoss: 0.2968\tLR: 0.020000\n",
            "Training Epoch: 67 [30464/50000]\tLoss: 0.4092\tLR: 0.020000\n",
            "Training Epoch: 67 [30592/50000]\tLoss: 0.3839\tLR: 0.020000\n",
            "Training Epoch: 67 [30720/50000]\tLoss: 0.3887\tLR: 0.020000\n",
            "Training Epoch: 67 [30848/50000]\tLoss: 0.3476\tLR: 0.020000\n",
            "Training Epoch: 67 [30976/50000]\tLoss: 0.3802\tLR: 0.020000\n",
            "Training Epoch: 67 [31104/50000]\tLoss: 0.3985\tLR: 0.020000\n",
            "Training Epoch: 67 [31232/50000]\tLoss: 0.2902\tLR: 0.020000\n",
            "Training Epoch: 67 [31360/50000]\tLoss: 0.2448\tLR: 0.020000\n",
            "Training Epoch: 67 [31488/50000]\tLoss: 0.3931\tLR: 0.020000\n",
            "Training Epoch: 67 [31616/50000]\tLoss: 0.3549\tLR: 0.020000\n",
            "Training Epoch: 67 [31744/50000]\tLoss: 0.3313\tLR: 0.020000\n",
            "Training Epoch: 67 [31872/50000]\tLoss: 0.4837\tLR: 0.020000\n",
            "Training Epoch: 67 [32000/50000]\tLoss: 0.2941\tLR: 0.020000\n",
            "Training Epoch: 67 [32128/50000]\tLoss: 0.3637\tLR: 0.020000\n",
            "Training Epoch: 67 [32256/50000]\tLoss: 0.3973\tLR: 0.020000\n",
            "Training Epoch: 67 [32384/50000]\tLoss: 0.4722\tLR: 0.020000\n",
            "Training Epoch: 67 [32512/50000]\tLoss: 0.2549\tLR: 0.020000\n",
            "Training Epoch: 67 [32640/50000]\tLoss: 0.3439\tLR: 0.020000\n",
            "Training Epoch: 67 [32768/50000]\tLoss: 0.2948\tLR: 0.020000\n",
            "Training Epoch: 67 [32896/50000]\tLoss: 0.3496\tLR: 0.020000\n",
            "Training Epoch: 67 [33024/50000]\tLoss: 0.3408\tLR: 0.020000\n",
            "Training Epoch: 67 [33152/50000]\tLoss: 0.3950\tLR: 0.020000\n",
            "Training Epoch: 67 [33280/50000]\tLoss: 0.3759\tLR: 0.020000\n",
            "Training Epoch: 67 [33408/50000]\tLoss: 0.3659\tLR: 0.020000\n",
            "Training Epoch: 67 [33536/50000]\tLoss: 0.4774\tLR: 0.020000\n",
            "Training Epoch: 67 [33664/50000]\tLoss: 0.4190\tLR: 0.020000\n",
            "Training Epoch: 67 [33792/50000]\tLoss: 0.2712\tLR: 0.020000\n",
            "Training Epoch: 67 [33920/50000]\tLoss: 0.4129\tLR: 0.020000\n",
            "Training Epoch: 67 [34048/50000]\tLoss: 0.3752\tLR: 0.020000\n",
            "Training Epoch: 67 [34176/50000]\tLoss: 0.2077\tLR: 0.020000\n",
            "Training Epoch: 67 [34304/50000]\tLoss: 0.4133\tLR: 0.020000\n",
            "Training Epoch: 67 [34432/50000]\tLoss: 0.3612\tLR: 0.020000\n",
            "Training Epoch: 67 [34560/50000]\tLoss: 0.2843\tLR: 0.020000\n",
            "Training Epoch: 67 [34688/50000]\tLoss: 0.2460\tLR: 0.020000\n",
            "Training Epoch: 67 [34816/50000]\tLoss: 0.3343\tLR: 0.020000\n",
            "Training Epoch: 67 [34944/50000]\tLoss: 0.3639\tLR: 0.020000\n",
            "Training Epoch: 67 [35072/50000]\tLoss: 0.2784\tLR: 0.020000\n",
            "Training Epoch: 67 [35200/50000]\tLoss: 0.4402\tLR: 0.020000\n",
            "Training Epoch: 67 [35328/50000]\tLoss: 0.3308\tLR: 0.020000\n",
            "Training Epoch: 67 [35456/50000]\tLoss: 0.3929\tLR: 0.020000\n",
            "Training Epoch: 67 [35584/50000]\tLoss: 0.3810\tLR: 0.020000\n",
            "Training Epoch: 67 [35712/50000]\tLoss: 0.4430\tLR: 0.020000\n",
            "Training Epoch: 67 [35840/50000]\tLoss: 0.4245\tLR: 0.020000\n",
            "Training Epoch: 67 [35968/50000]\tLoss: 0.3329\tLR: 0.020000\n",
            "Training Epoch: 67 [36096/50000]\tLoss: 0.4827\tLR: 0.020000\n",
            "Training Epoch: 67 [36224/50000]\tLoss: 0.2971\tLR: 0.020000\n",
            "Training Epoch: 67 [36352/50000]\tLoss: 0.3926\tLR: 0.020000\n",
            "Training Epoch: 67 [36480/50000]\tLoss: 0.4239\tLR: 0.020000\n",
            "Training Epoch: 67 [36608/50000]\tLoss: 0.4739\tLR: 0.020000\n",
            "Training Epoch: 67 [36736/50000]\tLoss: 0.3655\tLR: 0.020000\n",
            "Training Epoch: 67 [36864/50000]\tLoss: 0.5024\tLR: 0.020000\n",
            "Training Epoch: 67 [36992/50000]\tLoss: 0.4241\tLR: 0.020000\n",
            "Training Epoch: 67 [37120/50000]\tLoss: 0.4030\tLR: 0.020000\n",
            "Training Epoch: 67 [37248/50000]\tLoss: 0.3661\tLR: 0.020000\n",
            "Training Epoch: 67 [37376/50000]\tLoss: 0.4474\tLR: 0.020000\n",
            "Training Epoch: 67 [37504/50000]\tLoss: 0.4587\tLR: 0.020000\n",
            "Training Epoch: 67 [37632/50000]\tLoss: 0.3762\tLR: 0.020000\n",
            "Training Epoch: 67 [37760/50000]\tLoss: 0.3401\tLR: 0.020000\n",
            "Training Epoch: 67 [37888/50000]\tLoss: 0.3505\tLR: 0.020000\n",
            "Training Epoch: 67 [38016/50000]\tLoss: 0.4182\tLR: 0.020000\n",
            "Training Epoch: 67 [38144/50000]\tLoss: 0.4675\tLR: 0.020000\n",
            "Training Epoch: 67 [38272/50000]\tLoss: 0.3337\tLR: 0.020000\n",
            "Training Epoch: 67 [38400/50000]\tLoss: 0.4267\tLR: 0.020000\n",
            "Training Epoch: 67 [38528/50000]\tLoss: 0.3965\tLR: 0.020000\n",
            "Training Epoch: 67 [38656/50000]\tLoss: 0.2671\tLR: 0.020000\n",
            "Training Epoch: 67 [38784/50000]\tLoss: 0.3620\tLR: 0.020000\n",
            "Training Epoch: 67 [38912/50000]\tLoss: 0.4550\tLR: 0.020000\n",
            "Training Epoch: 67 [39040/50000]\tLoss: 0.3601\tLR: 0.020000\n",
            "Training Epoch: 67 [39168/50000]\tLoss: 0.3321\tLR: 0.020000\n",
            "Training Epoch: 67 [39296/50000]\tLoss: 0.2410\tLR: 0.020000\n",
            "Training Epoch: 67 [39424/50000]\tLoss: 0.4632\tLR: 0.020000\n",
            "Training Epoch: 67 [39552/50000]\tLoss: 0.4688\tLR: 0.020000\n",
            "Training Epoch: 67 [39680/50000]\tLoss: 0.3411\tLR: 0.020000\n",
            "Training Epoch: 67 [39808/50000]\tLoss: 0.3376\tLR: 0.020000\n",
            "Training Epoch: 67 [39936/50000]\tLoss: 0.3194\tLR: 0.020000\n",
            "Training Epoch: 67 [40064/50000]\tLoss: 0.3513\tLR: 0.020000\n",
            "Training Epoch: 67 [40192/50000]\tLoss: 0.5157\tLR: 0.020000\n",
            "Training Epoch: 67 [40320/50000]\tLoss: 0.4675\tLR: 0.020000\n",
            "Training Epoch: 67 [40448/50000]\tLoss: 0.4315\tLR: 0.020000\n",
            "Training Epoch: 67 [40576/50000]\tLoss: 0.3389\tLR: 0.020000\n",
            "Training Epoch: 67 [40704/50000]\tLoss: 0.3377\tLR: 0.020000\n",
            "Training Epoch: 67 [40832/50000]\tLoss: 0.4789\tLR: 0.020000\n",
            "Training Epoch: 67 [40960/50000]\tLoss: 0.2833\tLR: 0.020000\n",
            "Training Epoch: 67 [41088/50000]\tLoss: 0.4588\tLR: 0.020000\n",
            "Training Epoch: 67 [41216/50000]\tLoss: 0.4745\tLR: 0.020000\n",
            "Training Epoch: 67 [41344/50000]\tLoss: 0.3563\tLR: 0.020000\n",
            "Training Epoch: 67 [41472/50000]\tLoss: 0.3446\tLR: 0.020000\n",
            "Training Epoch: 67 [41600/50000]\tLoss: 0.4298\tLR: 0.020000\n",
            "Training Epoch: 67 [41728/50000]\tLoss: 0.3465\tLR: 0.020000\n",
            "Training Epoch: 67 [41856/50000]\tLoss: 0.3813\tLR: 0.020000\n",
            "Training Epoch: 67 [41984/50000]\tLoss: 0.4204\tLR: 0.020000\n",
            "Training Epoch: 67 [42112/50000]\tLoss: 0.3719\tLR: 0.020000\n",
            "Training Epoch: 67 [42240/50000]\tLoss: 0.3446\tLR: 0.020000\n",
            "Training Epoch: 67 [42368/50000]\tLoss: 0.4873\tLR: 0.020000\n",
            "Training Epoch: 67 [42496/50000]\tLoss: 0.3521\tLR: 0.020000\n",
            "Training Epoch: 67 [42624/50000]\tLoss: 0.2952\tLR: 0.020000\n",
            "Training Epoch: 67 [42752/50000]\tLoss: 0.5217\tLR: 0.020000\n",
            "Training Epoch: 67 [42880/50000]\tLoss: 0.4143\tLR: 0.020000\n",
            "Training Epoch: 67 [43008/50000]\tLoss: 0.3781\tLR: 0.020000\n",
            "Training Epoch: 67 [43136/50000]\tLoss: 0.4630\tLR: 0.020000\n",
            "Training Epoch: 67 [43264/50000]\tLoss: 0.5456\tLR: 0.020000\n",
            "Training Epoch: 67 [43392/50000]\tLoss: 0.5085\tLR: 0.020000\n",
            "Training Epoch: 67 [43520/50000]\tLoss: 0.3134\tLR: 0.020000\n",
            "Training Epoch: 67 [43648/50000]\tLoss: 0.4595\tLR: 0.020000\n",
            "Training Epoch: 67 [43776/50000]\tLoss: 0.4630\tLR: 0.020000\n",
            "Training Epoch: 67 [43904/50000]\tLoss: 0.3818\tLR: 0.020000\n",
            "Training Epoch: 67 [44032/50000]\tLoss: 0.5823\tLR: 0.020000\n",
            "Training Epoch: 67 [44160/50000]\tLoss: 0.4175\tLR: 0.020000\n",
            "Training Epoch: 67 [44288/50000]\tLoss: 0.5256\tLR: 0.020000\n",
            "Training Epoch: 67 [44416/50000]\tLoss: 0.4163\tLR: 0.020000\n",
            "Training Epoch: 67 [44544/50000]\tLoss: 0.3330\tLR: 0.020000\n",
            "Training Epoch: 67 [44672/50000]\tLoss: 0.4818\tLR: 0.020000\n",
            "Training Epoch: 67 [44800/50000]\tLoss: 0.4173\tLR: 0.020000\n",
            "Training Epoch: 67 [44928/50000]\tLoss: 0.1928\tLR: 0.020000\n",
            "Training Epoch: 67 [45056/50000]\tLoss: 0.4659\tLR: 0.020000\n",
            "Training Epoch: 67 [45184/50000]\tLoss: 0.4423\tLR: 0.020000\n",
            "Training Epoch: 67 [45312/50000]\tLoss: 0.5121\tLR: 0.020000\n",
            "Training Epoch: 67 [45440/50000]\tLoss: 0.3475\tLR: 0.020000\n",
            "Training Epoch: 67 [45568/50000]\tLoss: 0.4106\tLR: 0.020000\n",
            "Training Epoch: 67 [45696/50000]\tLoss: 0.3513\tLR: 0.020000\n",
            "Training Epoch: 67 [45824/50000]\tLoss: 0.4826\tLR: 0.020000\n",
            "Training Epoch: 67 [45952/50000]\tLoss: 0.3928\tLR: 0.020000\n",
            "Training Epoch: 67 [46080/50000]\tLoss: 0.3667\tLR: 0.020000\n",
            "Training Epoch: 67 [46208/50000]\tLoss: 0.3525\tLR: 0.020000\n",
            "Training Epoch: 67 [46336/50000]\tLoss: 0.3299\tLR: 0.020000\n",
            "Training Epoch: 67 [46464/50000]\tLoss: 0.4347\tLR: 0.020000\n",
            "Training Epoch: 67 [46592/50000]\tLoss: 0.3348\tLR: 0.020000\n",
            "Training Epoch: 67 [46720/50000]\tLoss: 0.5894\tLR: 0.020000\n",
            "Training Epoch: 67 [46848/50000]\tLoss: 0.4375\tLR: 0.020000\n",
            "Training Epoch: 67 [46976/50000]\tLoss: 0.2533\tLR: 0.020000\n",
            "Training Epoch: 67 [47104/50000]\tLoss: 0.5660\tLR: 0.020000\n",
            "Training Epoch: 67 [47232/50000]\tLoss: 0.3344\tLR: 0.020000\n",
            "Training Epoch: 67 [47360/50000]\tLoss: 0.4153\tLR: 0.020000\n",
            "Training Epoch: 67 [47488/50000]\tLoss: 0.3322\tLR: 0.020000\n",
            "Training Epoch: 67 [47616/50000]\tLoss: 0.4014\tLR: 0.020000\n",
            "Training Epoch: 67 [47744/50000]\tLoss: 0.5024\tLR: 0.020000\n",
            "Training Epoch: 67 [47872/50000]\tLoss: 0.3875\tLR: 0.020000\n",
            "Training Epoch: 67 [48000/50000]\tLoss: 0.3612\tLR: 0.020000\n",
            "Training Epoch: 67 [48128/50000]\tLoss: 0.3705\tLR: 0.020000\n",
            "Training Epoch: 67 [48256/50000]\tLoss: 0.3853\tLR: 0.020000\n",
            "Training Epoch: 67 [48384/50000]\tLoss: 0.5787\tLR: 0.020000\n",
            "Training Epoch: 67 [48512/50000]\tLoss: 0.4277\tLR: 0.020000\n",
            "Training Epoch: 67 [48640/50000]\tLoss: 0.3389\tLR: 0.020000\n",
            "Training Epoch: 67 [48768/50000]\tLoss: 0.4534\tLR: 0.020000\n",
            "Training Epoch: 67 [48896/50000]\tLoss: 0.3440\tLR: 0.020000\n",
            "Training Epoch: 67 [49024/50000]\tLoss: 0.2904\tLR: 0.020000\n",
            "Training Epoch: 67 [49152/50000]\tLoss: 0.3245\tLR: 0.020000\n",
            "Training Epoch: 67 [49280/50000]\tLoss: 0.2654\tLR: 0.020000\n",
            "Training Epoch: 67 [49408/50000]\tLoss: 0.4517\tLR: 0.020000\n",
            "Training Epoch: 67 [49536/50000]\tLoss: 0.5078\tLR: 0.020000\n",
            "Training Epoch: 67 [49664/50000]\tLoss: 0.3725\tLR: 0.020000\n",
            "Training Epoch: 67 [49792/50000]\tLoss: 0.3892\tLR: 0.020000\n",
            "Training Epoch: 67 [49920/50000]\tLoss: 0.3673\tLR: 0.020000\n",
            "Training Epoch: 67 [50000/50000]\tLoss: 0.3840\tLR: 0.020000\n",
            "epoch 67 training time consumed: 144.53s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  133678 KB |    1044 MB |  245530 GB |  245530 GB |\n",
            "|       from large pool |  122880 KB |    1034 MB |  245332 GB |  245332 GB |\n",
            "|       from small pool |   10798 KB |      13 MB |     198 GB |     198 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  133678 KB |    1044 MB |  245530 GB |  245530 GB |\n",
            "|       from large pool |  122880 KB |    1034 MB |  245332 GB |  245332 GB |\n",
            "|       from small pool |   10798 KB |      13 MB |     198 GB |     198 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    1838 MB |    1838 MB |    1838 MB |       0 B  |\n",
            "|       from large pool |    1822 MB |    1822 MB |    1822 MB |       0 B  |\n",
            "|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  433618 KB |  778793 KB |  114162 GB |  114162 GB |\n",
            "|       from large pool |  432128 KB |  776192 KB |  113964 GB |  113963 GB |\n",
            "|       from small pool |    1490 KB |    3494 KB |     198 GB |     198 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     255    |     335    |    9933 K  |    9933 K  |\n",
            "|       from large pool |      24    |      65    |    4869 K  |    4869 K  |\n",
            "|       from small pool |     231    |     274    |    5064 K  |    5064 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     255    |     335    |    9933 K  |    9933 K  |\n",
            "|       from large pool |      24    |      65    |    4869 K  |    4869 K  |\n",
            "|       from small pool |     231    |     274    |    5064 K  |    5064 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      20    |      20    |      20    |       0    |\n",
            "|       from large pool |      12    |      12    |      12    |       0    |\n",
            "|       from small pool |       8    |       8    |       8    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      38    |      45    |    5750 K  |    5750 K  |\n",
            "|       from large pool |      10    |      15    |    2203 K  |    2203 K  |\n",
            "|       from small pool |      28    |      35    |    3546 K  |    3546 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 67, Average loss: 0.0085, Accuracy: 0.7139, Time consumed:9.02s\n",
            "\n",
            "Training Epoch: 68 [128/50000]\tLoss: 0.3176\tLR: 0.020000\n",
            "Training Epoch: 68 [256/50000]\tLoss: 0.2829\tLR: 0.020000\n",
            "Training Epoch: 68 [384/50000]\tLoss: 0.3428\tLR: 0.020000\n",
            "Training Epoch: 68 [512/50000]\tLoss: 0.3637\tLR: 0.020000\n",
            "Training Epoch: 68 [640/50000]\tLoss: 0.2489\tLR: 0.020000\n",
            "Training Epoch: 68 [768/50000]\tLoss: 0.3298\tLR: 0.020000\n",
            "Training Epoch: 68 [896/50000]\tLoss: 0.2737\tLR: 0.020000\n",
            "Training Epoch: 68 [1024/50000]\tLoss: 0.4924\tLR: 0.020000\n",
            "Training Epoch: 68 [1152/50000]\tLoss: 0.2140\tLR: 0.020000\n",
            "Training Epoch: 68 [1280/50000]\tLoss: 0.3412\tLR: 0.020000\n",
            "Training Epoch: 68 [1408/50000]\tLoss: 0.2951\tLR: 0.020000\n",
            "Training Epoch: 68 [1536/50000]\tLoss: 0.2533\tLR: 0.020000\n",
            "Training Epoch: 68 [1664/50000]\tLoss: 0.2094\tLR: 0.020000\n",
            "Training Epoch: 68 [1792/50000]\tLoss: 0.3880\tLR: 0.020000\n",
            "Training Epoch: 68 [1920/50000]\tLoss: 0.3451\tLR: 0.020000\n",
            "Training Epoch: 68 [2048/50000]\tLoss: 0.3279\tLR: 0.020000\n",
            "Training Epoch: 68 [2176/50000]\tLoss: 0.1621\tLR: 0.020000\n",
            "Training Epoch: 68 [2304/50000]\tLoss: 0.6051\tLR: 0.020000\n",
            "Training Epoch: 68 [2432/50000]\tLoss: 0.2929\tLR: 0.020000\n",
            "Training Epoch: 68 [2560/50000]\tLoss: 0.2638\tLR: 0.020000\n",
            "Training Epoch: 68 [2688/50000]\tLoss: 0.2985\tLR: 0.020000\n",
            "Training Epoch: 68 [2816/50000]\tLoss: 0.2397\tLR: 0.020000\n",
            "Training Epoch: 68 [2944/50000]\tLoss: 0.2308\tLR: 0.020000\n",
            "Training Epoch: 68 [3072/50000]\tLoss: 0.3126\tLR: 0.020000\n",
            "Training Epoch: 68 [3200/50000]\tLoss: 0.3796\tLR: 0.020000\n",
            "Training Epoch: 68 [3328/50000]\tLoss: 0.3370\tLR: 0.020000\n",
            "Training Epoch: 68 [3456/50000]\tLoss: 0.2655\tLR: 0.020000\n",
            "Training Epoch: 68 [3584/50000]\tLoss: 0.3191\tLR: 0.020000\n",
            "Training Epoch: 68 [3712/50000]\tLoss: 0.2492\tLR: 0.020000\n",
            "Training Epoch: 68 [3840/50000]\tLoss: 0.4332\tLR: 0.020000\n",
            "Training Epoch: 68 [3968/50000]\tLoss: 0.3382\tLR: 0.020000\n",
            "Training Epoch: 68 [4096/50000]\tLoss: 0.2930\tLR: 0.020000\n",
            "Training Epoch: 68 [4224/50000]\tLoss: 0.3262\tLR: 0.020000\n",
            "Training Epoch: 68 [4352/50000]\tLoss: 0.3776\tLR: 0.020000\n",
            "Training Epoch: 68 [4480/50000]\tLoss: 0.4062\tLR: 0.020000\n",
            "Training Epoch: 68 [4608/50000]\tLoss: 0.2731\tLR: 0.020000\n",
            "Training Epoch: 68 [4736/50000]\tLoss: 0.3629\tLR: 0.020000\n",
            "Training Epoch: 68 [4864/50000]\tLoss: 0.2160\tLR: 0.020000\n",
            "Training Epoch: 68 [4992/50000]\tLoss: 0.3121\tLR: 0.020000\n",
            "Training Epoch: 68 [5120/50000]\tLoss: 0.2964\tLR: 0.020000\n",
            "Training Epoch: 68 [5248/50000]\tLoss: 0.3112\tLR: 0.020000\n",
            "Training Epoch: 68 [5376/50000]\tLoss: 0.3187\tLR: 0.020000\n",
            "Training Epoch: 68 [5504/50000]\tLoss: 0.2534\tLR: 0.020000\n",
            "Training Epoch: 68 [5632/50000]\tLoss: 0.3006\tLR: 0.020000\n",
            "Training Epoch: 68 [5760/50000]\tLoss: 0.3650\tLR: 0.020000\n",
            "Training Epoch: 68 [5888/50000]\tLoss: 0.2848\tLR: 0.020000\n",
            "Training Epoch: 68 [6016/50000]\tLoss: 0.4362\tLR: 0.020000\n",
            "Training Epoch: 68 [6144/50000]\tLoss: 0.4253\tLR: 0.020000\n",
            "Training Epoch: 68 [6272/50000]\tLoss: 0.4392\tLR: 0.020000\n",
            "Training Epoch: 68 [6400/50000]\tLoss: 0.3063\tLR: 0.020000\n",
            "Training Epoch: 68 [6528/50000]\tLoss: 0.2699\tLR: 0.020000\n",
            "Training Epoch: 68 [6656/50000]\tLoss: 0.3364\tLR: 0.020000\n",
            "Training Epoch: 68 [6784/50000]\tLoss: 0.2967\tLR: 0.020000\n",
            "Training Epoch: 68 [6912/50000]\tLoss: 0.3713\tLR: 0.020000\n",
            "Training Epoch: 68 [7040/50000]\tLoss: 0.2541\tLR: 0.020000\n",
            "Training Epoch: 68 [7168/50000]\tLoss: 0.2690\tLR: 0.020000\n",
            "Training Epoch: 68 [7296/50000]\tLoss: 0.3941\tLR: 0.020000\n",
            "Training Epoch: 68 [7424/50000]\tLoss: 0.3091\tLR: 0.020000\n",
            "Training Epoch: 68 [7552/50000]\tLoss: 0.3428\tLR: 0.020000\n",
            "Training Epoch: 68 [7680/50000]\tLoss: 0.3779\tLR: 0.020000\n",
            "Training Epoch: 68 [7808/50000]\tLoss: 0.3759\tLR: 0.020000\n",
            "Training Epoch: 68 [7936/50000]\tLoss: 0.2674\tLR: 0.020000\n",
            "Training Epoch: 68 [8064/50000]\tLoss: 0.2726\tLR: 0.020000\n",
            "Training Epoch: 68 [8192/50000]\tLoss: 0.2642\tLR: 0.020000\n",
            "Training Epoch: 68 [8320/50000]\tLoss: 0.2673\tLR: 0.020000\n",
            "Training Epoch: 68 [8448/50000]\tLoss: 0.3841\tLR: 0.020000\n",
            "Training Epoch: 68 [8576/50000]\tLoss: 0.2511\tLR: 0.020000\n",
            "Training Epoch: 68 [8704/50000]\tLoss: 0.3499\tLR: 0.020000\n",
            "Training Epoch: 68 [8832/50000]\tLoss: 0.3039\tLR: 0.020000\n",
            "Training Epoch: 68 [8960/50000]\tLoss: 0.2721\tLR: 0.020000\n",
            "Training Epoch: 68 [9088/50000]\tLoss: 0.2037\tLR: 0.020000\n",
            "Training Epoch: 68 [9216/50000]\tLoss: 0.3658\tLR: 0.020000\n",
            "Training Epoch: 68 [9344/50000]\tLoss: 0.3725\tLR: 0.020000\n",
            "Training Epoch: 68 [9472/50000]\tLoss: 0.3486\tLR: 0.020000\n",
            "Training Epoch: 68 [9600/50000]\tLoss: 0.3833\tLR: 0.020000\n",
            "Training Epoch: 68 [9728/50000]\tLoss: 0.2431\tLR: 0.020000\n",
            "Training Epoch: 68 [9856/50000]\tLoss: 0.3147\tLR: 0.020000\n",
            "Training Epoch: 68 [9984/50000]\tLoss: 0.2882\tLR: 0.020000\n",
            "Training Epoch: 68 [10112/50000]\tLoss: 0.4208\tLR: 0.020000\n",
            "Training Epoch: 68 [10240/50000]\tLoss: 0.4106\tLR: 0.020000\n",
            "Training Epoch: 68 [10368/50000]\tLoss: 0.3077\tLR: 0.020000\n",
            "Training Epoch: 68 [10496/50000]\tLoss: 0.2593\tLR: 0.020000\n",
            "Training Epoch: 68 [10624/50000]\tLoss: 0.3613\tLR: 0.020000\n",
            "Training Epoch: 68 [10752/50000]\tLoss: 0.2785\tLR: 0.020000\n",
            "Training Epoch: 68 [10880/50000]\tLoss: 0.2912\tLR: 0.020000\n",
            "Training Epoch: 68 [11008/50000]\tLoss: 0.3272\tLR: 0.020000\n",
            "Training Epoch: 68 [11136/50000]\tLoss: 0.2936\tLR: 0.020000\n",
            "Training Epoch: 68 [11264/50000]\tLoss: 0.3174\tLR: 0.020000\n",
            "Training Epoch: 68 [11392/50000]\tLoss: 0.3959\tLR: 0.020000\n",
            "Training Epoch: 68 [11520/50000]\tLoss: 0.2860\tLR: 0.020000\n",
            "Training Epoch: 68 [11648/50000]\tLoss: 0.3736\tLR: 0.020000\n",
            "Training Epoch: 68 [11776/50000]\tLoss: 0.3759\tLR: 0.020000\n",
            "Training Epoch: 68 [11904/50000]\tLoss: 0.3504\tLR: 0.020000\n",
            "Training Epoch: 68 [12032/50000]\tLoss: 0.3519\tLR: 0.020000\n",
            "Training Epoch: 68 [12160/50000]\tLoss: 0.4355\tLR: 0.020000\n",
            "Training Epoch: 68 [12288/50000]\tLoss: 0.3573\tLR: 0.020000\n",
            "Training Epoch: 68 [12416/50000]\tLoss: 0.2435\tLR: 0.020000\n",
            "Training Epoch: 68 [12544/50000]\tLoss: 0.2793\tLR: 0.020000\n",
            "Training Epoch: 68 [12672/50000]\tLoss: 0.3062\tLR: 0.020000\n",
            "Training Epoch: 68 [12800/50000]\tLoss: 0.3478\tLR: 0.020000\n",
            "Training Epoch: 68 [12928/50000]\tLoss: 0.3210\tLR: 0.020000\n",
            "Training Epoch: 68 [13056/50000]\tLoss: 0.3305\tLR: 0.020000\n",
            "Training Epoch: 68 [13184/50000]\tLoss: 0.3890\tLR: 0.020000\n",
            "Training Epoch: 68 [13312/50000]\tLoss: 0.4573\tLR: 0.020000\n",
            "Training Epoch: 68 [13440/50000]\tLoss: 0.2120\tLR: 0.020000\n",
            "Training Epoch: 68 [13568/50000]\tLoss: 0.4854\tLR: 0.020000\n",
            "Training Epoch: 68 [13696/50000]\tLoss: 0.2765\tLR: 0.020000\n",
            "Training Epoch: 68 [13824/50000]\tLoss: 0.2808\tLR: 0.020000\n",
            "Training Epoch: 68 [13952/50000]\tLoss: 0.2634\tLR: 0.020000\n",
            "Training Epoch: 68 [14080/50000]\tLoss: 0.3396\tLR: 0.020000\n",
            "Training Epoch: 68 [14208/50000]\tLoss: 0.3766\tLR: 0.020000\n",
            "Training Epoch: 68 [14336/50000]\tLoss: 0.3167\tLR: 0.020000\n",
            "Training Epoch: 68 [14464/50000]\tLoss: 0.3618\tLR: 0.020000\n",
            "Training Epoch: 68 [14592/50000]\tLoss: 0.4041\tLR: 0.020000\n",
            "Training Epoch: 68 [14720/50000]\tLoss: 0.4246\tLR: 0.020000\n",
            "Training Epoch: 68 [14848/50000]\tLoss: 0.3074\tLR: 0.020000\n",
            "Training Epoch: 68 [14976/50000]\tLoss: 0.3176\tLR: 0.020000\n",
            "Training Epoch: 68 [15104/50000]\tLoss: 0.2713\tLR: 0.020000\n",
            "Training Epoch: 68 [15232/50000]\tLoss: 0.3459\tLR: 0.020000\n",
            "Training Epoch: 68 [15360/50000]\tLoss: 0.3587\tLR: 0.020000\n",
            "Training Epoch: 68 [15488/50000]\tLoss: 0.2998\tLR: 0.020000\n",
            "Training Epoch: 68 [15616/50000]\tLoss: 0.4193\tLR: 0.020000\n",
            "Training Epoch: 68 [15744/50000]\tLoss: 0.2872\tLR: 0.020000\n",
            "Training Epoch: 68 [15872/50000]\tLoss: 0.3069\tLR: 0.020000\n",
            "Training Epoch: 68 [16000/50000]\tLoss: 0.2928\tLR: 0.020000\n",
            "Training Epoch: 68 [16128/50000]\tLoss: 0.3599\tLR: 0.020000\n",
            "Training Epoch: 68 [16256/50000]\tLoss: 0.3571\tLR: 0.020000\n",
            "Training Epoch: 68 [16384/50000]\tLoss: 0.3005\tLR: 0.020000\n",
            "Training Epoch: 68 [16512/50000]\tLoss: 0.4465\tLR: 0.020000\n",
            "Training Epoch: 68 [16640/50000]\tLoss: 0.3136\tLR: 0.020000\n",
            "Training Epoch: 68 [16768/50000]\tLoss: 0.3390\tLR: 0.020000\n",
            "Training Epoch: 68 [16896/50000]\tLoss: 0.3977\tLR: 0.020000\n",
            "Training Epoch: 68 [17024/50000]\tLoss: 0.3408\tLR: 0.020000\n",
            "Training Epoch: 68 [17152/50000]\tLoss: 0.4499\tLR: 0.020000\n",
            "Training Epoch: 68 [17280/50000]\tLoss: 0.3390\tLR: 0.020000\n",
            "Training Epoch: 68 [17408/50000]\tLoss: 0.2428\tLR: 0.020000\n",
            "Training Epoch: 68 [17536/50000]\tLoss: 0.3219\tLR: 0.020000\n",
            "Training Epoch: 68 [17664/50000]\tLoss: 0.4129\tLR: 0.020000\n",
            "Training Epoch: 68 [17792/50000]\tLoss: 0.3197\tLR: 0.020000\n",
            "Training Epoch: 68 [17920/50000]\tLoss: 0.2610\tLR: 0.020000\n",
            "Training Epoch: 68 [18048/50000]\tLoss: 0.3846\tLR: 0.020000\n",
            "Training Epoch: 68 [18176/50000]\tLoss: 0.3709\tLR: 0.020000\n",
            "Training Epoch: 68 [18304/50000]\tLoss: 0.3324\tLR: 0.020000\n",
            "Training Epoch: 68 [18432/50000]\tLoss: 0.2790\tLR: 0.020000\n",
            "Training Epoch: 68 [18560/50000]\tLoss: 0.3467\tLR: 0.020000\n",
            "Training Epoch: 68 [18688/50000]\tLoss: 0.6025\tLR: 0.020000\n",
            "Training Epoch: 68 [18816/50000]\tLoss: 0.3686\tLR: 0.020000\n",
            "Training Epoch: 68 [18944/50000]\tLoss: 0.3616\tLR: 0.020000\n",
            "Training Epoch: 68 [19072/50000]\tLoss: 0.3777\tLR: 0.020000\n",
            "Training Epoch: 68 [19200/50000]\tLoss: 0.3509\tLR: 0.020000\n",
            "Training Epoch: 68 [19328/50000]\tLoss: 0.2979\tLR: 0.020000\n",
            "Training Epoch: 68 [19456/50000]\tLoss: 0.3214\tLR: 0.020000\n",
            "Training Epoch: 68 [19584/50000]\tLoss: 0.3136\tLR: 0.020000\n",
            "Training Epoch: 68 [19712/50000]\tLoss: 0.2972\tLR: 0.020000\n",
            "Training Epoch: 68 [19840/50000]\tLoss: 0.3089\tLR: 0.020000\n",
            "Training Epoch: 68 [19968/50000]\tLoss: 0.3887\tLR: 0.020000\n",
            "Training Epoch: 68 [20096/50000]\tLoss: 0.3252\tLR: 0.020000\n",
            "Training Epoch: 68 [20224/50000]\tLoss: 0.4026\tLR: 0.020000\n",
            "Training Epoch: 68 [20352/50000]\tLoss: 0.3893\tLR: 0.020000\n",
            "Training Epoch: 68 [20480/50000]\tLoss: 0.4798\tLR: 0.020000\n",
            "Training Epoch: 68 [20608/50000]\tLoss: 0.3900\tLR: 0.020000\n",
            "Training Epoch: 68 [20736/50000]\tLoss: 0.3826\tLR: 0.020000\n",
            "Training Epoch: 68 [20864/50000]\tLoss: 0.3346\tLR: 0.020000\n",
            "Training Epoch: 68 [20992/50000]\tLoss: 0.3989\tLR: 0.020000\n",
            "Training Epoch: 68 [21120/50000]\tLoss: 0.3586\tLR: 0.020000\n",
            "Training Epoch: 68 [21248/50000]\tLoss: 0.4843\tLR: 0.020000\n",
            "Training Epoch: 68 [21376/50000]\tLoss: 0.4504\tLR: 0.020000\n",
            "Training Epoch: 68 [21504/50000]\tLoss: 0.3056\tLR: 0.020000\n",
            "Training Epoch: 68 [21632/50000]\tLoss: 0.3658\tLR: 0.020000\n",
            "Training Epoch: 68 [21760/50000]\tLoss: 0.3810\tLR: 0.020000\n",
            "Training Epoch: 68 [21888/50000]\tLoss: 0.3472\tLR: 0.020000\n",
            "Training Epoch: 68 [22016/50000]\tLoss: 0.2820\tLR: 0.020000\n",
            "Training Epoch: 68 [22144/50000]\tLoss: 0.3091\tLR: 0.020000\n",
            "Training Epoch: 68 [22272/50000]\tLoss: 0.3125\tLR: 0.020000\n",
            "Training Epoch: 68 [22400/50000]\tLoss: 0.3531\tLR: 0.020000\n",
            "Training Epoch: 68 [22528/50000]\tLoss: 0.4218\tLR: 0.020000\n",
            "Training Epoch: 68 [22656/50000]\tLoss: 0.3660\tLR: 0.020000\n",
            "Training Epoch: 68 [22784/50000]\tLoss: 0.3528\tLR: 0.020000\n",
            "Training Epoch: 68 [22912/50000]\tLoss: 0.3439\tLR: 0.020000\n",
            "Training Epoch: 68 [23040/50000]\tLoss: 0.2940\tLR: 0.020000\n",
            "Training Epoch: 68 [23168/50000]\tLoss: 0.4143\tLR: 0.020000\n",
            "Training Epoch: 68 [23296/50000]\tLoss: 0.2837\tLR: 0.020000\n",
            "Training Epoch: 68 [23424/50000]\tLoss: 0.3987\tLR: 0.020000\n",
            "Training Epoch: 68 [23552/50000]\tLoss: 0.3736\tLR: 0.020000\n",
            "Training Epoch: 68 [23680/50000]\tLoss: 0.3438\tLR: 0.020000\n",
            "Training Epoch: 68 [23808/50000]\tLoss: 0.2834\tLR: 0.020000\n",
            "Training Epoch: 68 [23936/50000]\tLoss: 0.4953\tLR: 0.020000\n",
            "Training Epoch: 68 [24064/50000]\tLoss: 0.3593\tLR: 0.020000\n",
            "Training Epoch: 68 [24192/50000]\tLoss: 0.3009\tLR: 0.020000\n",
            "Training Epoch: 68 [24320/50000]\tLoss: 0.3717\tLR: 0.020000\n",
            "Training Epoch: 68 [24448/50000]\tLoss: 0.3389\tLR: 0.020000\n",
            "Training Epoch: 68 [24576/50000]\tLoss: 0.4153\tLR: 0.020000\n",
            "Training Epoch: 68 [24704/50000]\tLoss: 0.4950\tLR: 0.020000\n",
            "Training Epoch: 68 [24832/50000]\tLoss: 0.3188\tLR: 0.020000\n",
            "Training Epoch: 68 [24960/50000]\tLoss: 0.3896\tLR: 0.020000\n",
            "Training Epoch: 68 [25088/50000]\tLoss: 0.2463\tLR: 0.020000\n",
            "Training Epoch: 68 [25216/50000]\tLoss: 0.3632\tLR: 0.020000\n",
            "Training Epoch: 68 [25344/50000]\tLoss: 0.3331\tLR: 0.020000\n",
            "Training Epoch: 68 [25472/50000]\tLoss: 0.3238\tLR: 0.020000\n",
            "Training Epoch: 68 [25600/50000]\tLoss: 0.3418\tLR: 0.020000\n",
            "Training Epoch: 68 [25728/50000]\tLoss: 0.3899\tLR: 0.020000\n",
            "Training Epoch: 68 [25856/50000]\tLoss: 0.3208\tLR: 0.020000\n",
            "Training Epoch: 68 [25984/50000]\tLoss: 0.3640\tLR: 0.020000\n",
            "Training Epoch: 68 [26112/50000]\tLoss: 0.2868\tLR: 0.020000\n",
            "Training Epoch: 68 [26240/50000]\tLoss: 0.3506\tLR: 0.020000\n",
            "Training Epoch: 68 [26368/50000]\tLoss: 0.3465\tLR: 0.020000\n",
            "Training Epoch: 68 [26496/50000]\tLoss: 0.4587\tLR: 0.020000\n",
            "Training Epoch: 68 [26624/50000]\tLoss: 0.3346\tLR: 0.020000\n",
            "Training Epoch: 68 [26752/50000]\tLoss: 0.4029\tLR: 0.020000\n",
            "Training Epoch: 68 [26880/50000]\tLoss: 0.3639\tLR: 0.020000\n",
            "Training Epoch: 68 [27008/50000]\tLoss: 0.4212\tLR: 0.020000\n",
            "Training Epoch: 68 [27136/50000]\tLoss: 0.4570\tLR: 0.020000\n",
            "Training Epoch: 68 [27264/50000]\tLoss: 0.4001\tLR: 0.020000\n",
            "Training Epoch: 68 [27392/50000]\tLoss: 0.3207\tLR: 0.020000\n",
            "Training Epoch: 68 [27520/50000]\tLoss: 0.4686\tLR: 0.020000\n",
            "Training Epoch: 68 [27648/50000]\tLoss: 0.2717\tLR: 0.020000\n",
            "Training Epoch: 68 [27776/50000]\tLoss: 0.2943\tLR: 0.020000\n",
            "Training Epoch: 68 [27904/50000]\tLoss: 0.2908\tLR: 0.020000\n",
            "Training Epoch: 68 [28032/50000]\tLoss: 0.2721\tLR: 0.020000\n",
            "Training Epoch: 68 [28160/50000]\tLoss: 0.3910\tLR: 0.020000\n",
            "Training Epoch: 68 [28288/50000]\tLoss: 0.3024\tLR: 0.020000\n",
            "Training Epoch: 68 [28416/50000]\tLoss: 0.5274\tLR: 0.020000\n",
            "Training Epoch: 68 [28544/50000]\tLoss: 0.4662\tLR: 0.020000\n",
            "Training Epoch: 68 [28672/50000]\tLoss: 0.3779\tLR: 0.020000\n",
            "Training Epoch: 68 [28800/50000]\tLoss: 0.3316\tLR: 0.020000\n",
            "Training Epoch: 68 [28928/50000]\tLoss: 0.3379\tLR: 0.020000\n",
            "Training Epoch: 68 [29056/50000]\tLoss: 0.3164\tLR: 0.020000\n",
            "Training Epoch: 68 [29184/50000]\tLoss: 0.3828\tLR: 0.020000\n",
            "Training Epoch: 68 [29312/50000]\tLoss: 0.3993\tLR: 0.020000\n",
            "Training Epoch: 68 [29440/50000]\tLoss: 0.3499\tLR: 0.020000\n",
            "Training Epoch: 68 [29568/50000]\tLoss: 0.2450\tLR: 0.020000\n",
            "Training Epoch: 68 [29696/50000]\tLoss: 0.3140\tLR: 0.020000\n",
            "Training Epoch: 68 [29824/50000]\tLoss: 0.4152\tLR: 0.020000\n",
            "Training Epoch: 68 [29952/50000]\tLoss: 0.3243\tLR: 0.020000\n",
            "Training Epoch: 68 [30080/50000]\tLoss: 0.4243\tLR: 0.020000\n",
            "Training Epoch: 68 [30208/50000]\tLoss: 0.3034\tLR: 0.020000\n",
            "Training Epoch: 68 [30336/50000]\tLoss: 0.4345\tLR: 0.020000\n",
            "Training Epoch: 68 [30464/50000]\tLoss: 0.3878\tLR: 0.020000\n",
            "Training Epoch: 68 [30592/50000]\tLoss: 0.3808\tLR: 0.020000\n",
            "Training Epoch: 68 [30720/50000]\tLoss: 0.3653\tLR: 0.020000\n",
            "Training Epoch: 68 [30848/50000]\tLoss: 0.4701\tLR: 0.020000\n",
            "Training Epoch: 68 [30976/50000]\tLoss: 0.3888\tLR: 0.020000\n",
            "Training Epoch: 68 [31104/50000]\tLoss: 0.2887\tLR: 0.020000\n",
            "Training Epoch: 68 [31232/50000]\tLoss: 0.3598\tLR: 0.020000\n",
            "Training Epoch: 68 [31360/50000]\tLoss: 0.3852\tLR: 0.020000\n",
            "Training Epoch: 68 [31488/50000]\tLoss: 0.2966\tLR: 0.020000\n",
            "Training Epoch: 68 [31616/50000]\tLoss: 0.3151\tLR: 0.020000\n",
            "Training Epoch: 68 [31744/50000]\tLoss: 0.3516\tLR: 0.020000\n",
            "Training Epoch: 68 [31872/50000]\tLoss: 0.3024\tLR: 0.020000\n",
            "Training Epoch: 68 [32000/50000]\tLoss: 0.3564\tLR: 0.020000\n",
            "Training Epoch: 68 [32128/50000]\tLoss: 0.4747\tLR: 0.020000\n",
            "Training Epoch: 68 [32256/50000]\tLoss: 0.2949\tLR: 0.020000\n",
            "Training Epoch: 68 [32384/50000]\tLoss: 0.4663\tLR: 0.020000\n",
            "Training Epoch: 68 [32512/50000]\tLoss: 0.3344\tLR: 0.020000\n",
            "Training Epoch: 68 [32640/50000]\tLoss: 0.5279\tLR: 0.020000\n",
            "Training Epoch: 68 [32768/50000]\tLoss: 0.4348\tLR: 0.020000\n",
            "Training Epoch: 68 [32896/50000]\tLoss: 0.3159\tLR: 0.020000\n",
            "Training Epoch: 68 [33024/50000]\tLoss: 0.4150\tLR: 0.020000\n",
            "Training Epoch: 68 [33152/50000]\tLoss: 0.3351\tLR: 0.020000\n",
            "Training Epoch: 68 [33280/50000]\tLoss: 0.3786\tLR: 0.020000\n",
            "Training Epoch: 68 [33408/50000]\tLoss: 0.3754\tLR: 0.020000\n",
            "Training Epoch: 68 [33536/50000]\tLoss: 0.3286\tLR: 0.020000\n",
            "Training Epoch: 68 [33664/50000]\tLoss: 0.4762\tLR: 0.020000\n",
            "Training Epoch: 68 [33792/50000]\tLoss: 0.4128\tLR: 0.020000\n",
            "Training Epoch: 68 [33920/50000]\tLoss: 0.3116\tLR: 0.020000\n",
            "Training Epoch: 68 [34048/50000]\tLoss: 0.6484\tLR: 0.020000\n",
            "Training Epoch: 68 [34176/50000]\tLoss: 0.3288\tLR: 0.020000\n",
            "Training Epoch: 68 [34304/50000]\tLoss: 0.3229\tLR: 0.020000\n",
            "Training Epoch: 68 [34432/50000]\tLoss: 0.3039\tLR: 0.020000\n",
            "Training Epoch: 68 [34560/50000]\tLoss: 0.3552\tLR: 0.020000\n",
            "Training Epoch: 68 [34688/50000]\tLoss: 0.4743\tLR: 0.020000\n",
            "Training Epoch: 68 [34816/50000]\tLoss: 0.3694\tLR: 0.020000\n",
            "Training Epoch: 68 [34944/50000]\tLoss: 0.3651\tLR: 0.020000\n",
            "Training Epoch: 68 [35072/50000]\tLoss: 0.3173\tLR: 0.020000\n",
            "Training Epoch: 68 [35200/50000]\tLoss: 0.4333\tLR: 0.020000\n",
            "Training Epoch: 68 [35328/50000]\tLoss: 0.4333\tLR: 0.020000\n",
            "Training Epoch: 68 [35456/50000]\tLoss: 0.4020\tLR: 0.020000\n",
            "Training Epoch: 68 [35584/50000]\tLoss: 0.4603\tLR: 0.020000\n",
            "Training Epoch: 68 [35712/50000]\tLoss: 0.4013\tLR: 0.020000\n",
            "Training Epoch: 68 [35840/50000]\tLoss: 0.3467\tLR: 0.020000\n",
            "Training Epoch: 68 [35968/50000]\tLoss: 0.2915\tLR: 0.020000\n",
            "Training Epoch: 68 [36096/50000]\tLoss: 0.2638\tLR: 0.020000\n",
            "Training Epoch: 68 [36224/50000]\tLoss: 0.2904\tLR: 0.020000\n",
            "Training Epoch: 68 [36352/50000]\tLoss: 0.3824\tLR: 0.020000\n",
            "Training Epoch: 68 [36480/50000]\tLoss: 0.4170\tLR: 0.020000\n",
            "Training Epoch: 68 [36608/50000]\tLoss: 0.4429\tLR: 0.020000\n",
            "Training Epoch: 68 [36736/50000]\tLoss: 0.3365\tLR: 0.020000\n",
            "Training Epoch: 68 [36864/50000]\tLoss: 0.5169\tLR: 0.020000\n",
            "Training Epoch: 68 [36992/50000]\tLoss: 0.3799\tLR: 0.020000\n",
            "Training Epoch: 68 [37120/50000]\tLoss: 0.3860\tLR: 0.020000\n",
            "Training Epoch: 68 [37248/50000]\tLoss: 0.3750\tLR: 0.020000\n",
            "Training Epoch: 68 [37376/50000]\tLoss: 0.3352\tLR: 0.020000\n",
            "Training Epoch: 68 [37504/50000]\tLoss: 0.4904\tLR: 0.020000\n",
            "Training Epoch: 68 [37632/50000]\tLoss: 0.4622\tLR: 0.020000\n",
            "Training Epoch: 68 [37760/50000]\tLoss: 0.3755\tLR: 0.020000\n",
            "Training Epoch: 68 [37888/50000]\tLoss: 0.3641\tLR: 0.020000\n",
            "Training Epoch: 68 [38016/50000]\tLoss: 0.3104\tLR: 0.020000\n",
            "Training Epoch: 68 [38144/50000]\tLoss: 0.3604\tLR: 0.020000\n",
            "Training Epoch: 68 [38272/50000]\tLoss: 0.4731\tLR: 0.020000\n",
            "Training Epoch: 68 [38400/50000]\tLoss: 0.2707\tLR: 0.020000\n",
            "Training Epoch: 68 [38528/50000]\tLoss: 0.2651\tLR: 0.020000\n",
            "Training Epoch: 68 [38656/50000]\tLoss: 0.5380\tLR: 0.020000\n",
            "Training Epoch: 68 [38784/50000]\tLoss: 0.2907\tLR: 0.020000\n",
            "Training Epoch: 68 [38912/50000]\tLoss: 0.3895\tLR: 0.020000\n",
            "Training Epoch: 68 [39040/50000]\tLoss: 0.3530\tLR: 0.020000\n",
            "Training Epoch: 68 [39168/50000]\tLoss: 0.4022\tLR: 0.020000\n",
            "Training Epoch: 68 [39296/50000]\tLoss: 0.5281\tLR: 0.020000\n",
            "Training Epoch: 68 [39424/50000]\tLoss: 0.4426\tLR: 0.020000\n",
            "Training Epoch: 68 [39552/50000]\tLoss: 0.4512\tLR: 0.020000\n",
            "Training Epoch: 68 [39680/50000]\tLoss: 0.3515\tLR: 0.020000\n",
            "Training Epoch: 68 [39808/50000]\tLoss: 0.4886\tLR: 0.020000\n",
            "Training Epoch: 68 [39936/50000]\tLoss: 0.3615\tLR: 0.020000\n",
            "Training Epoch: 68 [40064/50000]\tLoss: 0.3085\tLR: 0.020000\n",
            "Training Epoch: 68 [40192/50000]\tLoss: 0.3733\tLR: 0.020000\n",
            "Training Epoch: 68 [40320/50000]\tLoss: 0.3361\tLR: 0.020000\n",
            "Training Epoch: 68 [40448/50000]\tLoss: 0.3289\tLR: 0.020000\n",
            "Training Epoch: 68 [40576/50000]\tLoss: 0.2802\tLR: 0.020000\n",
            "Training Epoch: 68 [40704/50000]\tLoss: 0.2910\tLR: 0.020000\n",
            "Training Epoch: 68 [40832/50000]\tLoss: 0.4814\tLR: 0.020000\n",
            "Training Epoch: 68 [40960/50000]\tLoss: 0.3182\tLR: 0.020000\n",
            "Training Epoch: 68 [41088/50000]\tLoss: 0.4133\tLR: 0.020000\n",
            "Training Epoch: 68 [41216/50000]\tLoss: 0.4307\tLR: 0.020000\n",
            "Training Epoch: 68 [41344/50000]\tLoss: 0.4775\tLR: 0.020000\n",
            "Training Epoch: 68 [41472/50000]\tLoss: 0.3555\tLR: 0.020000\n",
            "Training Epoch: 68 [41600/50000]\tLoss: 0.4215\tLR: 0.020000\n",
            "Training Epoch: 68 [41728/50000]\tLoss: 0.4423\tLR: 0.020000\n",
            "Training Epoch: 68 [41856/50000]\tLoss: 0.3291\tLR: 0.020000\n",
            "Training Epoch: 68 [41984/50000]\tLoss: 0.4070\tLR: 0.020000\n",
            "Training Epoch: 68 [42112/50000]\tLoss: 0.3233\tLR: 0.020000\n",
            "Training Epoch: 68 [42240/50000]\tLoss: 0.3769\tLR: 0.020000\n",
            "Training Epoch: 68 [42368/50000]\tLoss: 0.3375\tLR: 0.020000\n",
            "Training Epoch: 68 [42496/50000]\tLoss: 0.4274\tLR: 0.020000\n",
            "Training Epoch: 68 [42624/50000]\tLoss: 0.3061\tLR: 0.020000\n",
            "Training Epoch: 68 [42752/50000]\tLoss: 0.2536\tLR: 0.020000\n",
            "Training Epoch: 68 [42880/50000]\tLoss: 0.4108\tLR: 0.020000\n",
            "Training Epoch: 68 [43008/50000]\tLoss: 0.3298\tLR: 0.020000\n",
            "Training Epoch: 68 [43136/50000]\tLoss: 0.4687\tLR: 0.020000\n",
            "Training Epoch: 68 [43264/50000]\tLoss: 0.4637\tLR: 0.020000\n",
            "Training Epoch: 68 [43392/50000]\tLoss: 0.4768\tLR: 0.020000\n",
            "Training Epoch: 68 [43520/50000]\tLoss: 0.3590\tLR: 0.020000\n",
            "Training Epoch: 68 [43648/50000]\tLoss: 0.4761\tLR: 0.020000\n",
            "Training Epoch: 68 [43776/50000]\tLoss: 0.3733\tLR: 0.020000\n",
            "Training Epoch: 68 [43904/50000]\tLoss: 0.3067\tLR: 0.020000\n",
            "Training Epoch: 68 [44032/50000]\tLoss: 0.5593\tLR: 0.020000\n",
            "Training Epoch: 68 [44160/50000]\tLoss: 0.4236\tLR: 0.020000\n",
            "Training Epoch: 68 [44288/50000]\tLoss: 0.4150\tLR: 0.020000\n",
            "Training Epoch: 68 [44416/50000]\tLoss: 0.3684\tLR: 0.020000\n",
            "Training Epoch: 68 [44544/50000]\tLoss: 0.3607\tLR: 0.020000\n",
            "Training Epoch: 68 [44672/50000]\tLoss: 0.3227\tLR: 0.020000\n",
            "Training Epoch: 68 [44800/50000]\tLoss: 0.3268\tLR: 0.020000\n",
            "Training Epoch: 68 [44928/50000]\tLoss: 0.2876\tLR: 0.020000\n",
            "Training Epoch: 68 [45056/50000]\tLoss: 0.3268\tLR: 0.020000\n",
            "Training Epoch: 68 [45184/50000]\tLoss: 0.6001\tLR: 0.020000\n",
            "Training Epoch: 68 [45312/50000]\tLoss: 0.2931\tLR: 0.020000\n",
            "Training Epoch: 68 [45440/50000]\tLoss: 0.5222\tLR: 0.020000\n",
            "Training Epoch: 68 [45568/50000]\tLoss: 0.2915\tLR: 0.020000\n",
            "Training Epoch: 68 [45696/50000]\tLoss: 0.4434\tLR: 0.020000\n",
            "Training Epoch: 68 [45824/50000]\tLoss: 0.3294\tLR: 0.020000\n",
            "Training Epoch: 68 [45952/50000]\tLoss: 0.3445\tLR: 0.020000\n",
            "Training Epoch: 68 [46080/50000]\tLoss: 0.4098\tLR: 0.020000\n",
            "Training Epoch: 68 [46208/50000]\tLoss: 0.3705\tLR: 0.020000\n",
            "Training Epoch: 68 [46336/50000]\tLoss: 0.4305\tLR: 0.020000\n",
            "Training Epoch: 68 [46464/50000]\tLoss: 0.3394\tLR: 0.020000\n",
            "Training Epoch: 68 [46592/50000]\tLoss: 0.3234\tLR: 0.020000\n",
            "Training Epoch: 68 [46720/50000]\tLoss: 0.5510\tLR: 0.020000\n",
            "Training Epoch: 68 [46848/50000]\tLoss: 0.4340\tLR: 0.020000\n",
            "Training Epoch: 68 [46976/50000]\tLoss: 0.4983\tLR: 0.020000\n",
            "Training Epoch: 68 [47104/50000]\tLoss: 0.3577\tLR: 0.020000\n",
            "Training Epoch: 68 [47232/50000]\tLoss: 0.4706\tLR: 0.020000\n",
            "Training Epoch: 68 [47360/50000]\tLoss: 0.4174\tLR: 0.020000\n",
            "Training Epoch: 68 [47488/50000]\tLoss: 0.4941\tLR: 0.020000\n",
            "Training Epoch: 68 [47616/50000]\tLoss: 0.4705\tLR: 0.020000\n",
            "Training Epoch: 68 [47744/50000]\tLoss: 0.3818\tLR: 0.020000\n",
            "Training Epoch: 68 [47872/50000]\tLoss: 0.3223\tLR: 0.020000\n",
            "Training Epoch: 68 [48000/50000]\tLoss: 0.4669\tLR: 0.020000\n",
            "Training Epoch: 68 [48128/50000]\tLoss: 0.5462\tLR: 0.020000\n",
            "Training Epoch: 68 [48256/50000]\tLoss: 0.3894\tLR: 0.020000\n",
            "Training Epoch: 68 [48384/50000]\tLoss: 0.2621\tLR: 0.020000\n",
            "Training Epoch: 68 [48512/50000]\tLoss: 0.4371\tLR: 0.020000\n",
            "Training Epoch: 68 [48640/50000]\tLoss: 0.4158\tLR: 0.020000\n",
            "Training Epoch: 68 [48768/50000]\tLoss: 0.4765\tLR: 0.020000\n",
            "Training Epoch: 68 [48896/50000]\tLoss: 0.2395\tLR: 0.020000\n",
            "Training Epoch: 68 [49024/50000]\tLoss: 0.3506\tLR: 0.020000\n",
            "Training Epoch: 68 [49152/50000]\tLoss: 0.3698\tLR: 0.020000\n",
            "Training Epoch: 68 [49280/50000]\tLoss: 0.3234\tLR: 0.020000\n",
            "Training Epoch: 68 [49408/50000]\tLoss: 0.3600\tLR: 0.020000\n",
            "Training Epoch: 68 [49536/50000]\tLoss: 0.3000\tLR: 0.020000\n",
            "Training Epoch: 68 [49664/50000]\tLoss: 0.4347\tLR: 0.020000\n",
            "Training Epoch: 68 [49792/50000]\tLoss: 0.5079\tLR: 0.020000\n",
            "Training Epoch: 68 [49920/50000]\tLoss: 0.3427\tLR: 0.020000\n",
            "Training Epoch: 68 [50000/50000]\tLoss: 0.4004\tLR: 0.020000\n",
            "epoch 68 training time consumed: 144.82s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  133678 KB |    1044 MB |  249195 GB |  249195 GB |\n",
            "|       from large pool |  122880 KB |    1034 MB |  248993 GB |  248993 GB |\n",
            "|       from small pool |   10798 KB |      13 MB |     201 GB |     201 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  133678 KB |    1044 MB |  249195 GB |  249195 GB |\n",
            "|       from large pool |  122880 KB |    1034 MB |  248993 GB |  248993 GB |\n",
            "|       from small pool |   10798 KB |      13 MB |     201 GB |     201 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    1838 MB |    1838 MB |    1838 MB |       0 B  |\n",
            "|       from large pool |    1822 MB |    1822 MB |    1822 MB |       0 B  |\n",
            "|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  433618 KB |  778793 KB |  115866 GB |  115866 GB |\n",
            "|       from large pool |  432128 KB |  776192 KB |  115665 GB |  115664 GB |\n",
            "|       from small pool |    1490 KB |    3494 KB |     201 GB |     201 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     255    |     335    |   10081 K  |   10081 K  |\n",
            "|       from large pool |      24    |      65    |    4941 K  |    4941 K  |\n",
            "|       from small pool |     231    |     274    |    5140 K  |    5139 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     255    |     335    |   10081 K  |   10081 K  |\n",
            "|       from large pool |      24    |      65    |    4941 K  |    4941 K  |\n",
            "|       from small pool |     231    |     274    |    5140 K  |    5139 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      20    |      20    |      20    |       0    |\n",
            "|       from large pool |      12    |      12    |      12    |       0    |\n",
            "|       from small pool |       8    |       8    |       8    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      37    |      45    |    5836 K  |    5836 K  |\n",
            "|       from large pool |      10    |      15    |    2236 K  |    2236 K  |\n",
            "|       from small pool |      27    |      35    |    3599 K  |    3599 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 68, Average loss: 0.0091, Accuracy: 0.7043, Time consumed:9.04s\n",
            "\n",
            "Training Epoch: 69 [128/50000]\tLoss: 0.2374\tLR: 0.020000\n",
            "Training Epoch: 69 [256/50000]\tLoss: 0.3422\tLR: 0.020000\n",
            "Training Epoch: 69 [384/50000]\tLoss: 0.3056\tLR: 0.020000\n",
            "Training Epoch: 69 [512/50000]\tLoss: 0.2894\tLR: 0.020000\n",
            "Training Epoch: 69 [640/50000]\tLoss: 0.3709\tLR: 0.020000\n",
            "Training Epoch: 69 [768/50000]\tLoss: 0.2818\tLR: 0.020000\n",
            "Training Epoch: 69 [896/50000]\tLoss: 0.2801\tLR: 0.020000\n",
            "Training Epoch: 69 [1024/50000]\tLoss: 0.2789\tLR: 0.020000\n",
            "Training Epoch: 69 [1152/50000]\tLoss: 0.2240\tLR: 0.020000\n",
            "Training Epoch: 69 [1280/50000]\tLoss: 0.2720\tLR: 0.020000\n",
            "Training Epoch: 69 [1408/50000]\tLoss: 0.3107\tLR: 0.020000\n",
            "Training Epoch: 69 [1536/50000]\tLoss: 0.3146\tLR: 0.020000\n",
            "Training Epoch: 69 [1664/50000]\tLoss: 0.2660\tLR: 0.020000\n",
            "Training Epoch: 69 [1792/50000]\tLoss: 0.3500\tLR: 0.020000\n",
            "Training Epoch: 69 [1920/50000]\tLoss: 0.2755\tLR: 0.020000\n",
            "Training Epoch: 69 [2048/50000]\tLoss: 0.3076\tLR: 0.020000\n",
            "Training Epoch: 69 [2176/50000]\tLoss: 0.3821\tLR: 0.020000\n",
            "Training Epoch: 69 [2304/50000]\tLoss: 0.3779\tLR: 0.020000\n",
            "Training Epoch: 69 [2432/50000]\tLoss: 0.2622\tLR: 0.020000\n",
            "Training Epoch: 69 [2560/50000]\tLoss: 0.2714\tLR: 0.020000\n",
            "Training Epoch: 69 [2688/50000]\tLoss: 0.2436\tLR: 0.020000\n",
            "Training Epoch: 69 [2816/50000]\tLoss: 0.2638\tLR: 0.020000\n",
            "Training Epoch: 69 [2944/50000]\tLoss: 0.3158\tLR: 0.020000\n",
            "Training Epoch: 69 [3072/50000]\tLoss: 0.2750\tLR: 0.020000\n",
            "Training Epoch: 69 [3200/50000]\tLoss: 0.3747\tLR: 0.020000\n",
            "Training Epoch: 69 [3328/50000]\tLoss: 0.3080\tLR: 0.020000\n",
            "Training Epoch: 69 [3456/50000]\tLoss: 0.3398\tLR: 0.020000\n",
            "Training Epoch: 69 [3584/50000]\tLoss: 0.3085\tLR: 0.020000\n",
            "Training Epoch: 69 [3712/50000]\tLoss: 0.3205\tLR: 0.020000\n",
            "Training Epoch: 69 [3840/50000]\tLoss: 0.2874\tLR: 0.020000\n",
            "Training Epoch: 69 [3968/50000]\tLoss: 0.2762\tLR: 0.020000\n",
            "Training Epoch: 69 [4096/50000]\tLoss: 0.2705\tLR: 0.020000\n",
            "Training Epoch: 69 [4224/50000]\tLoss: 0.3878\tLR: 0.020000\n",
            "Training Epoch: 69 [4352/50000]\tLoss: 0.2652\tLR: 0.020000\n",
            "Training Epoch: 69 [4480/50000]\tLoss: 0.2845\tLR: 0.020000\n",
            "Training Epoch: 69 [4608/50000]\tLoss: 0.3615\tLR: 0.020000\n",
            "Training Epoch: 69 [4736/50000]\tLoss: 0.3021\tLR: 0.020000\n",
            "Training Epoch: 69 [4864/50000]\tLoss: 0.3119\tLR: 0.020000\n",
            "Training Epoch: 69 [4992/50000]\tLoss: 0.2274\tLR: 0.020000\n",
            "Training Epoch: 69 [5120/50000]\tLoss: 0.3478\tLR: 0.020000\n",
            "Training Epoch: 69 [5248/50000]\tLoss: 0.4048\tLR: 0.020000\n",
            "Training Epoch: 69 [5376/50000]\tLoss: 0.3381\tLR: 0.020000\n",
            "Training Epoch: 69 [5504/50000]\tLoss: 0.2982\tLR: 0.020000\n",
            "Training Epoch: 69 [5632/50000]\tLoss: 0.3820\tLR: 0.020000\n",
            "Training Epoch: 69 [5760/50000]\tLoss: 0.3422\tLR: 0.020000\n",
            "Training Epoch: 69 [5888/50000]\tLoss: 0.3839\tLR: 0.020000\n",
            "Training Epoch: 69 [6016/50000]\tLoss: 0.2444\tLR: 0.020000\n",
            "Training Epoch: 69 [6144/50000]\tLoss: 0.3832\tLR: 0.020000\n",
            "Training Epoch: 69 [6272/50000]\tLoss: 0.3312\tLR: 0.020000\n",
            "Training Epoch: 69 [6400/50000]\tLoss: 0.2568\tLR: 0.020000\n",
            "Training Epoch: 69 [6528/50000]\tLoss: 0.3648\tLR: 0.020000\n",
            "Training Epoch: 69 [6656/50000]\tLoss: 0.2284\tLR: 0.020000\n",
            "Training Epoch: 69 [6784/50000]\tLoss: 0.2604\tLR: 0.020000\n",
            "Training Epoch: 69 [6912/50000]\tLoss: 0.2745\tLR: 0.020000\n",
            "Training Epoch: 69 [7040/50000]\tLoss: 0.3376\tLR: 0.020000\n",
            "Training Epoch: 69 [7168/50000]\tLoss: 0.2600\tLR: 0.020000\n",
            "Training Epoch: 69 [7296/50000]\tLoss: 0.2103\tLR: 0.020000\n",
            "Training Epoch: 69 [7424/50000]\tLoss: 0.3068\tLR: 0.020000\n",
            "Training Epoch: 69 [7552/50000]\tLoss: 0.2884\tLR: 0.020000\n",
            "Training Epoch: 69 [7680/50000]\tLoss: 0.3251\tLR: 0.020000\n",
            "Training Epoch: 69 [7808/50000]\tLoss: 0.4151\tLR: 0.020000\n",
            "Training Epoch: 69 [7936/50000]\tLoss: 0.3829\tLR: 0.020000\n",
            "Training Epoch: 69 [8064/50000]\tLoss: 0.3038\tLR: 0.020000\n",
            "Training Epoch: 69 [8192/50000]\tLoss: 0.3593\tLR: 0.020000\n",
            "Training Epoch: 69 [8320/50000]\tLoss: 0.2323\tLR: 0.020000\n",
            "Training Epoch: 69 [8448/50000]\tLoss: 0.3787\tLR: 0.020000\n",
            "Training Epoch: 69 [8576/50000]\tLoss: 0.2647\tLR: 0.020000\n",
            "Training Epoch: 69 [8704/50000]\tLoss: 0.1802\tLR: 0.020000\n",
            "Training Epoch: 69 [8832/50000]\tLoss: 0.2393\tLR: 0.020000\n",
            "Training Epoch: 69 [8960/50000]\tLoss: 0.3521\tLR: 0.020000\n",
            "Training Epoch: 69 [9088/50000]\tLoss: 0.3364\tLR: 0.020000\n",
            "Training Epoch: 69 [9216/50000]\tLoss: 0.3257\tLR: 0.020000\n",
            "Training Epoch: 69 [9344/50000]\tLoss: 0.3434\tLR: 0.020000\n",
            "Training Epoch: 69 [9472/50000]\tLoss: 0.3797\tLR: 0.020000\n",
            "Training Epoch: 69 [9600/50000]\tLoss: 0.2161\tLR: 0.020000\n",
            "Training Epoch: 69 [9728/50000]\tLoss: 0.3227\tLR: 0.020000\n",
            "Training Epoch: 69 [9856/50000]\tLoss: 0.2617\tLR: 0.020000\n",
            "Training Epoch: 69 [9984/50000]\tLoss: 0.3241\tLR: 0.020000\n",
            "Training Epoch: 69 [10112/50000]\tLoss: 0.2460\tLR: 0.020000\n",
            "Training Epoch: 69 [10240/50000]\tLoss: 0.3046\tLR: 0.020000\n",
            "Training Epoch: 69 [10368/50000]\tLoss: 0.2242\tLR: 0.020000\n",
            "Training Epoch: 69 [10496/50000]\tLoss: 0.2030\tLR: 0.020000\n",
            "Training Epoch: 69 [10624/50000]\tLoss: 0.3912\tLR: 0.020000\n",
            "Training Epoch: 69 [10752/50000]\tLoss: 0.3572\tLR: 0.020000\n",
            "Training Epoch: 69 [10880/50000]\tLoss: 0.3430\tLR: 0.020000\n",
            "Training Epoch: 69 [11008/50000]\tLoss: 0.3279\tLR: 0.020000\n",
            "Training Epoch: 69 [11136/50000]\tLoss: 0.3033\tLR: 0.020000\n",
            "Training Epoch: 69 [11264/50000]\tLoss: 0.2332\tLR: 0.020000\n",
            "Training Epoch: 69 [11392/50000]\tLoss: 0.4221\tLR: 0.020000\n",
            "Training Epoch: 69 [11520/50000]\tLoss: 0.3561\tLR: 0.020000\n",
            "Training Epoch: 69 [11648/50000]\tLoss: 0.3204\tLR: 0.020000\n",
            "Training Epoch: 69 [11776/50000]\tLoss: 0.3622\tLR: 0.020000\n",
            "Training Epoch: 69 [11904/50000]\tLoss: 0.2880\tLR: 0.020000\n",
            "Training Epoch: 69 [12032/50000]\tLoss: 0.3222\tLR: 0.020000\n",
            "Training Epoch: 69 [12160/50000]\tLoss: 0.3324\tLR: 0.020000\n",
            "Training Epoch: 69 [12288/50000]\tLoss: 0.4436\tLR: 0.020000\n",
            "Training Epoch: 69 [12416/50000]\tLoss: 0.3864\tLR: 0.020000\n",
            "Training Epoch: 69 [12544/50000]\tLoss: 0.3321\tLR: 0.020000\n",
            "Training Epoch: 69 [12672/50000]\tLoss: 0.3156\tLR: 0.020000\n",
            "Training Epoch: 69 [12800/50000]\tLoss: 0.3125\tLR: 0.020000\n",
            "Training Epoch: 69 [12928/50000]\tLoss: 0.3167\tLR: 0.020000\n",
            "Training Epoch: 69 [13056/50000]\tLoss: 0.3013\tLR: 0.020000\n",
            "Training Epoch: 69 [13184/50000]\tLoss: 0.2894\tLR: 0.020000\n",
            "Training Epoch: 69 [13312/50000]\tLoss: 0.2856\tLR: 0.020000\n",
            "Training Epoch: 69 [13440/50000]\tLoss: 0.3157\tLR: 0.020000\n",
            "Training Epoch: 69 [13568/50000]\tLoss: 0.2803\tLR: 0.020000\n",
            "Training Epoch: 69 [13696/50000]\tLoss: 0.3617\tLR: 0.020000\n",
            "Training Epoch: 69 [13824/50000]\tLoss: 0.2531\tLR: 0.020000\n",
            "Training Epoch: 69 [13952/50000]\tLoss: 0.2625\tLR: 0.020000\n",
            "Training Epoch: 69 [14080/50000]\tLoss: 0.3775\tLR: 0.020000\n",
            "Training Epoch: 69 [14208/50000]\tLoss: 0.3240\tLR: 0.020000\n",
            "Training Epoch: 69 [14336/50000]\tLoss: 0.3716\tLR: 0.020000\n",
            "Training Epoch: 69 [14464/50000]\tLoss: 0.2772\tLR: 0.020000\n",
            "Training Epoch: 69 [14592/50000]\tLoss: 0.3462\tLR: 0.020000\n",
            "Training Epoch: 69 [14720/50000]\tLoss: 0.2755\tLR: 0.020000\n",
            "Training Epoch: 69 [14848/50000]\tLoss: 0.3329\tLR: 0.020000\n",
            "Training Epoch: 69 [14976/50000]\tLoss: 0.2300\tLR: 0.020000\n",
            "Training Epoch: 69 [15104/50000]\tLoss: 0.3574\tLR: 0.020000\n",
            "Training Epoch: 69 [15232/50000]\tLoss: 0.3648\tLR: 0.020000\n",
            "Training Epoch: 69 [15360/50000]\tLoss: 0.3644\tLR: 0.020000\n",
            "Training Epoch: 69 [15488/50000]\tLoss: 0.3139\tLR: 0.020000\n",
            "Training Epoch: 69 [15616/50000]\tLoss: 0.3142\tLR: 0.020000\n",
            "Training Epoch: 69 [15744/50000]\tLoss: 0.2557\tLR: 0.020000\n",
            "Training Epoch: 69 [15872/50000]\tLoss: 0.2762\tLR: 0.020000\n",
            "Training Epoch: 69 [16000/50000]\tLoss: 0.2858\tLR: 0.020000\n",
            "Training Epoch: 69 [16128/50000]\tLoss: 0.2494\tLR: 0.020000\n",
            "Training Epoch: 69 [16256/50000]\tLoss: 0.3157\tLR: 0.020000\n",
            "Training Epoch: 69 [16384/50000]\tLoss: 0.2683\tLR: 0.020000\n",
            "Training Epoch: 69 [16512/50000]\tLoss: 0.2664\tLR: 0.020000\n",
            "Training Epoch: 69 [16640/50000]\tLoss: 0.2735\tLR: 0.020000\n",
            "Training Epoch: 69 [16768/50000]\tLoss: 0.4671\tLR: 0.020000\n",
            "Training Epoch: 69 [16896/50000]\tLoss: 0.2786\tLR: 0.020000\n",
            "Training Epoch: 69 [17024/50000]\tLoss: 0.3777\tLR: 0.020000\n",
            "Training Epoch: 69 [17152/50000]\tLoss: 0.3763\tLR: 0.020000\n",
            "Training Epoch: 69 [17280/50000]\tLoss: 0.3789\tLR: 0.020000\n",
            "Training Epoch: 69 [17408/50000]\tLoss: 0.3020\tLR: 0.020000\n",
            "Training Epoch: 69 [17536/50000]\tLoss: 0.3360\tLR: 0.020000\n",
            "Training Epoch: 69 [17664/50000]\tLoss: 0.2995\tLR: 0.020000\n",
            "Training Epoch: 69 [17792/50000]\tLoss: 0.3616\tLR: 0.020000\n",
            "Training Epoch: 69 [17920/50000]\tLoss: 0.3849\tLR: 0.020000\n",
            "Training Epoch: 69 [18048/50000]\tLoss: 0.2829\tLR: 0.020000\n",
            "Training Epoch: 69 [18176/50000]\tLoss: 0.3130\tLR: 0.020000\n",
            "Training Epoch: 69 [18304/50000]\tLoss: 0.3481\tLR: 0.020000\n",
            "Training Epoch: 69 [18432/50000]\tLoss: 0.5319\tLR: 0.020000\n",
            "Training Epoch: 69 [18560/50000]\tLoss: 0.2644\tLR: 0.020000\n",
            "Training Epoch: 69 [18688/50000]\tLoss: 0.2269\tLR: 0.020000\n",
            "Training Epoch: 69 [18816/50000]\tLoss: 0.4545\tLR: 0.020000\n",
            "Training Epoch: 69 [18944/50000]\tLoss: 0.2650\tLR: 0.020000\n",
            "Training Epoch: 69 [19072/50000]\tLoss: 0.2344\tLR: 0.020000\n",
            "Training Epoch: 69 [19200/50000]\tLoss: 0.3387\tLR: 0.020000\n",
            "Training Epoch: 69 [19328/50000]\tLoss: 0.3133\tLR: 0.020000\n",
            "Training Epoch: 69 [19456/50000]\tLoss: 0.3157\tLR: 0.020000\n",
            "Training Epoch: 69 [19584/50000]\tLoss: 0.3253\tLR: 0.020000\n",
            "Training Epoch: 69 [19712/50000]\tLoss: 0.2696\tLR: 0.020000\n",
            "Training Epoch: 69 [19840/50000]\tLoss: 0.2594\tLR: 0.020000\n",
            "Training Epoch: 69 [19968/50000]\tLoss: 0.3434\tLR: 0.020000\n",
            "Training Epoch: 69 [20096/50000]\tLoss: 0.3975\tLR: 0.020000\n",
            "Training Epoch: 69 [20224/50000]\tLoss: 0.4812\tLR: 0.020000\n",
            "Training Epoch: 69 [20352/50000]\tLoss: 0.4029\tLR: 0.020000\n",
            "Training Epoch: 69 [20480/50000]\tLoss: 0.4192\tLR: 0.020000\n",
            "Training Epoch: 69 [20608/50000]\tLoss: 0.3625\tLR: 0.020000\n",
            "Training Epoch: 69 [20736/50000]\tLoss: 0.3452\tLR: 0.020000\n",
            "Training Epoch: 69 [20864/50000]\tLoss: 0.3300\tLR: 0.020000\n",
            "Training Epoch: 69 [20992/50000]\tLoss: 0.3299\tLR: 0.020000\n",
            "Training Epoch: 69 [21120/50000]\tLoss: 0.4054\tLR: 0.020000\n",
            "Training Epoch: 69 [21248/50000]\tLoss: 0.2679\tLR: 0.020000\n",
            "Training Epoch: 69 [21376/50000]\tLoss: 0.3560\tLR: 0.020000\n",
            "Training Epoch: 69 [21504/50000]\tLoss: 0.2713\tLR: 0.020000\n",
            "Training Epoch: 69 [21632/50000]\tLoss: 0.3804\tLR: 0.020000\n",
            "Training Epoch: 69 [21760/50000]\tLoss: 0.2567\tLR: 0.020000\n",
            "Training Epoch: 69 [21888/50000]\tLoss: 0.2345\tLR: 0.020000\n",
            "Training Epoch: 69 [22016/50000]\tLoss: 0.4063\tLR: 0.020000\n",
            "Training Epoch: 69 [22144/50000]\tLoss: 0.3375\tLR: 0.020000\n",
            "Training Epoch: 69 [22272/50000]\tLoss: 0.2993\tLR: 0.020000\n",
            "Training Epoch: 69 [22400/50000]\tLoss: 0.3675\tLR: 0.020000\n",
            "Training Epoch: 69 [22528/50000]\tLoss: 0.3048\tLR: 0.020000\n",
            "Training Epoch: 69 [22656/50000]\tLoss: 0.3681\tLR: 0.020000\n",
            "Training Epoch: 69 [22784/50000]\tLoss: 0.3555\tLR: 0.020000\n",
            "Training Epoch: 69 [22912/50000]\tLoss: 0.3613\tLR: 0.020000\n",
            "Training Epoch: 69 [23040/50000]\tLoss: 0.4483\tLR: 0.020000\n",
            "Training Epoch: 69 [23168/50000]\tLoss: 0.2484\tLR: 0.020000\n",
            "Training Epoch: 69 [23296/50000]\tLoss: 0.3858\tLR: 0.020000\n",
            "Training Epoch: 69 [23424/50000]\tLoss: 0.2803\tLR: 0.020000\n",
            "Training Epoch: 69 [23552/50000]\tLoss: 0.4419\tLR: 0.020000\n",
            "Training Epoch: 69 [23680/50000]\tLoss: 0.3248\tLR: 0.020000\n",
            "Training Epoch: 69 [23808/50000]\tLoss: 0.3292\tLR: 0.020000\n",
            "Training Epoch: 69 [23936/50000]\tLoss: 0.2569\tLR: 0.020000\n",
            "Training Epoch: 69 [24064/50000]\tLoss: 0.2999\tLR: 0.020000\n",
            "Training Epoch: 69 [24192/50000]\tLoss: 0.3201\tLR: 0.020000\n",
            "Training Epoch: 69 [24320/50000]\tLoss: 0.3880\tLR: 0.020000\n",
            "Training Epoch: 69 [24448/50000]\tLoss: 0.4226\tLR: 0.020000\n",
            "Training Epoch: 69 [24576/50000]\tLoss: 0.5038\tLR: 0.020000\n",
            "Training Epoch: 69 [24704/50000]\tLoss: 0.3211\tLR: 0.020000\n",
            "Training Epoch: 69 [24832/50000]\tLoss: 0.3567\tLR: 0.020000\n",
            "Training Epoch: 69 [24960/50000]\tLoss: 0.3424\tLR: 0.020000\n",
            "Training Epoch: 69 [25088/50000]\tLoss: 0.2649\tLR: 0.020000\n",
            "Training Epoch: 69 [25216/50000]\tLoss: 0.3573\tLR: 0.020000\n",
            "Training Epoch: 69 [25344/50000]\tLoss: 0.3463\tLR: 0.020000\n",
            "Training Epoch: 69 [25472/50000]\tLoss: 0.3395\tLR: 0.020000\n",
            "Training Epoch: 69 [25600/50000]\tLoss: 0.3207\tLR: 0.020000\n",
            "Training Epoch: 69 [25728/50000]\tLoss: 0.3507\tLR: 0.020000\n",
            "Training Epoch: 69 [25856/50000]\tLoss: 0.4442\tLR: 0.020000\n",
            "Training Epoch: 69 [25984/50000]\tLoss: 0.4460\tLR: 0.020000\n",
            "Training Epoch: 69 [26112/50000]\tLoss: 0.3741\tLR: 0.020000\n",
            "Training Epoch: 69 [26240/50000]\tLoss: 0.3818\tLR: 0.020000\n",
            "Training Epoch: 69 [26368/50000]\tLoss: 0.3459\tLR: 0.020000\n",
            "Training Epoch: 69 [26496/50000]\tLoss: 0.2855\tLR: 0.020000\n",
            "Training Epoch: 69 [26624/50000]\tLoss: 0.4360\tLR: 0.020000\n",
            "Training Epoch: 69 [26752/50000]\tLoss: 0.3830\tLR: 0.020000\n",
            "Training Epoch: 69 [26880/50000]\tLoss: 0.3716\tLR: 0.020000\n",
            "Training Epoch: 69 [27008/50000]\tLoss: 0.4162\tLR: 0.020000\n",
            "Training Epoch: 69 [27136/50000]\tLoss: 0.2881\tLR: 0.020000\n",
            "Training Epoch: 69 [27264/50000]\tLoss: 0.3162\tLR: 0.020000\n",
            "Training Epoch: 69 [27392/50000]\tLoss: 0.3045\tLR: 0.020000\n",
            "Training Epoch: 69 [27520/50000]\tLoss: 0.4257\tLR: 0.020000\n",
            "Training Epoch: 69 [27648/50000]\tLoss: 0.4111\tLR: 0.020000\n",
            "Training Epoch: 69 [27776/50000]\tLoss: 0.3630\tLR: 0.020000\n",
            "Training Epoch: 69 [27904/50000]\tLoss: 0.3646\tLR: 0.020000\n",
            "Training Epoch: 69 [28032/50000]\tLoss: 0.3148\tLR: 0.020000\n",
            "Training Epoch: 69 [28160/50000]\tLoss: 0.4906\tLR: 0.020000\n",
            "Training Epoch: 69 [28288/50000]\tLoss: 0.2901\tLR: 0.020000\n",
            "Training Epoch: 69 [28416/50000]\tLoss: 0.2920\tLR: 0.020000\n",
            "Training Epoch: 69 [28544/50000]\tLoss: 0.3012\tLR: 0.020000\n",
            "Training Epoch: 69 [28672/50000]\tLoss: 0.3341\tLR: 0.020000\n",
            "Training Epoch: 69 [28800/50000]\tLoss: 0.3625\tLR: 0.020000\n",
            "Training Epoch: 69 [28928/50000]\tLoss: 0.4026\tLR: 0.020000\n",
            "Training Epoch: 69 [29056/50000]\tLoss: 0.3637\tLR: 0.020000\n",
            "Training Epoch: 69 [29184/50000]\tLoss: 0.4126\tLR: 0.020000\n",
            "Training Epoch: 69 [29312/50000]\tLoss: 0.3059\tLR: 0.020000\n",
            "Training Epoch: 69 [29440/50000]\tLoss: 0.3876\tLR: 0.020000\n",
            "Training Epoch: 69 [29568/50000]\tLoss: 0.3056\tLR: 0.020000\n",
            "Training Epoch: 69 [29696/50000]\tLoss: 0.2728\tLR: 0.020000\n",
            "Training Epoch: 69 [29824/50000]\tLoss: 0.4833\tLR: 0.020000\n",
            "Training Epoch: 69 [29952/50000]\tLoss: 0.3182\tLR: 0.020000\n",
            "Training Epoch: 69 [30080/50000]\tLoss: 0.4076\tLR: 0.020000\n",
            "Training Epoch: 69 [30208/50000]\tLoss: 0.3661\tLR: 0.020000\n",
            "Training Epoch: 69 [30336/50000]\tLoss: 0.2948\tLR: 0.020000\n",
            "Training Epoch: 69 [30464/50000]\tLoss: 0.2594\tLR: 0.020000\n",
            "Training Epoch: 69 [30592/50000]\tLoss: 0.4669\tLR: 0.020000\n",
            "Training Epoch: 69 [30720/50000]\tLoss: 0.3673\tLR: 0.020000\n",
            "Training Epoch: 69 [30848/50000]\tLoss: 0.2584\tLR: 0.020000\n",
            "Training Epoch: 69 [30976/50000]\tLoss: 0.3554\tLR: 0.020000\n",
            "Training Epoch: 69 [31104/50000]\tLoss: 0.4148\tLR: 0.020000\n",
            "Training Epoch: 69 [31232/50000]\tLoss: 0.3579\tLR: 0.020000\n",
            "Training Epoch: 69 [31360/50000]\tLoss: 0.4308\tLR: 0.020000\n",
            "Training Epoch: 69 [31488/50000]\tLoss: 0.3077\tLR: 0.020000\n",
            "Training Epoch: 69 [31616/50000]\tLoss: 0.3911\tLR: 0.020000\n",
            "Training Epoch: 69 [31744/50000]\tLoss: 0.4989\tLR: 0.020000\n",
            "Training Epoch: 69 [31872/50000]\tLoss: 0.3378\tLR: 0.020000\n",
            "Training Epoch: 69 [32000/50000]\tLoss: 0.3474\tLR: 0.020000\n",
            "Training Epoch: 69 [32128/50000]\tLoss: 0.4120\tLR: 0.020000\n",
            "Training Epoch: 69 [32256/50000]\tLoss: 0.4482\tLR: 0.020000\n",
            "Training Epoch: 69 [32384/50000]\tLoss: 0.3935\tLR: 0.020000\n",
            "Training Epoch: 69 [32512/50000]\tLoss: 0.3926\tLR: 0.020000\n",
            "Training Epoch: 69 [32640/50000]\tLoss: 0.3060\tLR: 0.020000\n",
            "Training Epoch: 69 [32768/50000]\tLoss: 0.3361\tLR: 0.020000\n",
            "Training Epoch: 69 [32896/50000]\tLoss: 0.4571\tLR: 0.020000\n",
            "Training Epoch: 69 [33024/50000]\tLoss: 0.2408\tLR: 0.020000\n",
            "Training Epoch: 69 [33152/50000]\tLoss: 0.4295\tLR: 0.020000\n",
            "Training Epoch: 69 [33280/50000]\tLoss: 0.3189\tLR: 0.020000\n",
            "Training Epoch: 69 [33408/50000]\tLoss: 0.3255\tLR: 0.020000\n",
            "Training Epoch: 69 [33536/50000]\tLoss: 0.4047\tLR: 0.020000\n",
            "Training Epoch: 69 [33664/50000]\tLoss: 0.4195\tLR: 0.020000\n",
            "Training Epoch: 69 [33792/50000]\tLoss: 0.3269\tLR: 0.020000\n",
            "Training Epoch: 69 [33920/50000]\tLoss: 0.4469\tLR: 0.020000\n",
            "Training Epoch: 69 [34048/50000]\tLoss: 0.3676\tLR: 0.020000\n",
            "Training Epoch: 69 [34176/50000]\tLoss: 0.2779\tLR: 0.020000\n",
            "Training Epoch: 69 [34304/50000]\tLoss: 0.3397\tLR: 0.020000\n",
            "Training Epoch: 69 [34432/50000]\tLoss: 0.3693\tLR: 0.020000\n",
            "Training Epoch: 69 [34560/50000]\tLoss: 0.3419\tLR: 0.020000\n",
            "Training Epoch: 69 [34688/50000]\tLoss: 0.4599\tLR: 0.020000\n",
            "Training Epoch: 69 [34816/50000]\tLoss: 0.4457\tLR: 0.020000\n",
            "Training Epoch: 69 [34944/50000]\tLoss: 0.4199\tLR: 0.020000\n",
            "Training Epoch: 69 [35072/50000]\tLoss: 0.4221\tLR: 0.020000\n",
            "Training Epoch: 69 [35200/50000]\tLoss: 0.3356\tLR: 0.020000\n",
            "Training Epoch: 69 [35328/50000]\tLoss: 0.2761\tLR: 0.020000\n",
            "Training Epoch: 69 [35456/50000]\tLoss: 0.4787\tLR: 0.020000\n",
            "Training Epoch: 69 [35584/50000]\tLoss: 0.3598\tLR: 0.020000\n",
            "Training Epoch: 69 [35712/50000]\tLoss: 0.3209\tLR: 0.020000\n",
            "Training Epoch: 69 [35840/50000]\tLoss: 0.4385\tLR: 0.020000\n",
            "Training Epoch: 69 [35968/50000]\tLoss: 0.4886\tLR: 0.020000\n",
            "Training Epoch: 69 [36096/50000]\tLoss: 0.4045\tLR: 0.020000\n",
            "Training Epoch: 69 [36224/50000]\tLoss: 0.3551\tLR: 0.020000\n",
            "Training Epoch: 69 [36352/50000]\tLoss: 0.3961\tLR: 0.020000\n",
            "Training Epoch: 69 [36480/50000]\tLoss: 0.3828\tLR: 0.020000\n",
            "Training Epoch: 69 [36608/50000]\tLoss: 0.3796\tLR: 0.020000\n",
            "Training Epoch: 69 [36736/50000]\tLoss: 0.3232\tLR: 0.020000\n",
            "Training Epoch: 69 [36864/50000]\tLoss: 0.2807\tLR: 0.020000\n",
            "Training Epoch: 69 [36992/50000]\tLoss: 0.3373\tLR: 0.020000\n",
            "Training Epoch: 69 [37120/50000]\tLoss: 0.3091\tLR: 0.020000\n",
            "Training Epoch: 69 [37248/50000]\tLoss: 0.3486\tLR: 0.020000\n",
            "Training Epoch: 69 [37376/50000]\tLoss: 0.5078\tLR: 0.020000\n",
            "Training Epoch: 69 [37504/50000]\tLoss: 0.3819\tLR: 0.020000\n",
            "Training Epoch: 69 [37632/50000]\tLoss: 0.4180\tLR: 0.020000\n",
            "Training Epoch: 69 [37760/50000]\tLoss: 0.3116\tLR: 0.020000\n",
            "Training Epoch: 69 [37888/50000]\tLoss: 0.2730\tLR: 0.020000\n",
            "Training Epoch: 69 [38016/50000]\tLoss: 0.3871\tLR: 0.020000\n",
            "Training Epoch: 69 [38144/50000]\tLoss: 0.4345\tLR: 0.020000\n",
            "Training Epoch: 69 [38272/50000]\tLoss: 0.4450\tLR: 0.020000\n",
            "Training Epoch: 69 [38400/50000]\tLoss: 0.3648\tLR: 0.020000\n",
            "Training Epoch: 69 [38528/50000]\tLoss: 0.3677\tLR: 0.020000\n",
            "Training Epoch: 69 [38656/50000]\tLoss: 0.3376\tLR: 0.020000\n",
            "Training Epoch: 69 [38784/50000]\tLoss: 0.3322\tLR: 0.020000\n",
            "Training Epoch: 69 [38912/50000]\tLoss: 0.3653\tLR: 0.020000\n",
            "Training Epoch: 69 [39040/50000]\tLoss: 0.3631\tLR: 0.020000\n",
            "Training Epoch: 69 [39168/50000]\tLoss: 0.2547\tLR: 0.020000\n",
            "Training Epoch: 69 [39296/50000]\tLoss: 0.5052\tLR: 0.020000\n",
            "Training Epoch: 69 [39424/50000]\tLoss: 0.3261\tLR: 0.020000\n",
            "Training Epoch: 69 [39552/50000]\tLoss: 0.3828\tLR: 0.020000\n",
            "Training Epoch: 69 [39680/50000]\tLoss: 0.3205\tLR: 0.020000\n",
            "Training Epoch: 69 [39808/50000]\tLoss: 0.3583\tLR: 0.020000\n",
            "Training Epoch: 69 [39936/50000]\tLoss: 0.4338\tLR: 0.020000\n",
            "Training Epoch: 69 [40064/50000]\tLoss: 0.3073\tLR: 0.020000\n",
            "Training Epoch: 69 [40192/50000]\tLoss: 0.3530\tLR: 0.020000\n",
            "Training Epoch: 69 [40320/50000]\tLoss: 0.2992\tLR: 0.020000\n",
            "Training Epoch: 69 [40448/50000]\tLoss: 0.4299\tLR: 0.020000\n",
            "Training Epoch: 69 [40576/50000]\tLoss: 0.3411\tLR: 0.020000\n",
            "Training Epoch: 69 [40704/50000]\tLoss: 0.2966\tLR: 0.020000\n",
            "Training Epoch: 69 [40832/50000]\tLoss: 0.3878\tLR: 0.020000\n",
            "Training Epoch: 69 [40960/50000]\tLoss: 0.3752\tLR: 0.020000\n",
            "Training Epoch: 69 [41088/50000]\tLoss: 0.2832\tLR: 0.020000\n",
            "Training Epoch: 69 [41216/50000]\tLoss: 0.5656\tLR: 0.020000\n",
            "Training Epoch: 69 [41344/50000]\tLoss: 0.3005\tLR: 0.020000\n",
            "Training Epoch: 69 [41472/50000]\tLoss: 0.3268\tLR: 0.020000\n",
            "Training Epoch: 69 [41600/50000]\tLoss: 0.4041\tLR: 0.020000\n",
            "Training Epoch: 69 [41728/50000]\tLoss: 0.3523\tLR: 0.020000\n",
            "Training Epoch: 69 [41856/50000]\tLoss: 0.5859\tLR: 0.020000\n",
            "Training Epoch: 69 [41984/50000]\tLoss: 0.4453\tLR: 0.020000\n",
            "Training Epoch: 69 [42112/50000]\tLoss: 0.2713\tLR: 0.020000\n",
            "Training Epoch: 69 [42240/50000]\tLoss: 0.3575\tLR: 0.020000\n",
            "Training Epoch: 69 [42368/50000]\tLoss: 0.5012\tLR: 0.020000\n",
            "Training Epoch: 69 [42496/50000]\tLoss: 0.3364\tLR: 0.020000\n",
            "Training Epoch: 69 [42624/50000]\tLoss: 0.3550\tLR: 0.020000\n",
            "Training Epoch: 69 [42752/50000]\tLoss: 0.4390\tLR: 0.020000\n",
            "Training Epoch: 69 [42880/50000]\tLoss: 0.3415\tLR: 0.020000\n",
            "Training Epoch: 69 [43008/50000]\tLoss: 0.3676\tLR: 0.020000\n",
            "Training Epoch: 69 [43136/50000]\tLoss: 0.4982\tLR: 0.020000\n",
            "Training Epoch: 69 [43264/50000]\tLoss: 0.3227\tLR: 0.020000\n",
            "Training Epoch: 69 [43392/50000]\tLoss: 0.4732\tLR: 0.020000\n",
            "Training Epoch: 69 [43520/50000]\tLoss: 0.4322\tLR: 0.020000\n",
            "Training Epoch: 69 [43648/50000]\tLoss: 0.4882\tLR: 0.020000\n",
            "Training Epoch: 69 [43776/50000]\tLoss: 0.6191\tLR: 0.020000\n",
            "Training Epoch: 69 [43904/50000]\tLoss: 0.3647\tLR: 0.020000\n",
            "Training Epoch: 69 [44032/50000]\tLoss: 0.4545\tLR: 0.020000\n",
            "Training Epoch: 69 [44160/50000]\tLoss: 0.3052\tLR: 0.020000\n",
            "Training Epoch: 69 [44288/50000]\tLoss: 0.4860\tLR: 0.020000\n",
            "Training Epoch: 69 [44416/50000]\tLoss: 0.4916\tLR: 0.020000\n",
            "Training Epoch: 69 [44544/50000]\tLoss: 0.3409\tLR: 0.020000\n",
            "Training Epoch: 69 [44672/50000]\tLoss: 0.3135\tLR: 0.020000\n",
            "Training Epoch: 69 [44800/50000]\tLoss: 0.3035\tLR: 0.020000\n",
            "Training Epoch: 69 [44928/50000]\tLoss: 0.3316\tLR: 0.020000\n",
            "Training Epoch: 69 [45056/50000]\tLoss: 0.4025\tLR: 0.020000\n",
            "Training Epoch: 69 [45184/50000]\tLoss: 0.3907\tLR: 0.020000\n",
            "Training Epoch: 69 [45312/50000]\tLoss: 0.4986\tLR: 0.020000\n",
            "Training Epoch: 69 [45440/50000]\tLoss: 0.3039\tLR: 0.020000\n",
            "Training Epoch: 69 [45568/50000]\tLoss: 0.3244\tLR: 0.020000\n",
            "Training Epoch: 69 [45696/50000]\tLoss: 0.5027\tLR: 0.020000\n",
            "Training Epoch: 69 [45824/50000]\tLoss: 0.3384\tLR: 0.020000\n",
            "Training Epoch: 69 [45952/50000]\tLoss: 0.3439\tLR: 0.020000\n",
            "Training Epoch: 69 [46080/50000]\tLoss: 0.3898\tLR: 0.020000\n",
            "Training Epoch: 69 [46208/50000]\tLoss: 0.3393\tLR: 0.020000\n",
            "Training Epoch: 69 [46336/50000]\tLoss: 0.5249\tLR: 0.020000\n",
            "Training Epoch: 69 [46464/50000]\tLoss: 0.3920\tLR: 0.020000\n",
            "Training Epoch: 69 [46592/50000]\tLoss: 0.3656\tLR: 0.020000\n",
            "Training Epoch: 69 [46720/50000]\tLoss: 0.3751\tLR: 0.020000\n",
            "Training Epoch: 69 [46848/50000]\tLoss: 0.3998\tLR: 0.020000\n",
            "Training Epoch: 69 [46976/50000]\tLoss: 0.3541\tLR: 0.020000\n",
            "Training Epoch: 69 [47104/50000]\tLoss: 0.3662\tLR: 0.020000\n",
            "Training Epoch: 69 [47232/50000]\tLoss: 0.3555\tLR: 0.020000\n",
            "Training Epoch: 69 [47360/50000]\tLoss: 0.4459\tLR: 0.020000\n",
            "Training Epoch: 69 [47488/50000]\tLoss: 0.3746\tLR: 0.020000\n",
            "Training Epoch: 69 [47616/50000]\tLoss: 0.3321\tLR: 0.020000\n",
            "Training Epoch: 69 [47744/50000]\tLoss: 0.3952\tLR: 0.020000\n",
            "Training Epoch: 69 [47872/50000]\tLoss: 0.5808\tLR: 0.020000\n",
            "Training Epoch: 69 [48000/50000]\tLoss: 0.3376\tLR: 0.020000\n",
            "Training Epoch: 69 [48128/50000]\tLoss: 0.3364\tLR: 0.020000\n",
            "Training Epoch: 69 [48256/50000]\tLoss: 0.2732\tLR: 0.020000\n",
            "Training Epoch: 69 [48384/50000]\tLoss: 0.5040\tLR: 0.020000\n",
            "Training Epoch: 69 [48512/50000]\tLoss: 0.4224\tLR: 0.020000\n",
            "Training Epoch: 69 [48640/50000]\tLoss: 0.3538\tLR: 0.020000\n",
            "Training Epoch: 69 [48768/50000]\tLoss: 0.5639\tLR: 0.020000\n",
            "Training Epoch: 69 [48896/50000]\tLoss: 0.4406\tLR: 0.020000\n",
            "Training Epoch: 69 [49024/50000]\tLoss: 0.4991\tLR: 0.020000\n",
            "Training Epoch: 69 [49152/50000]\tLoss: 0.2594\tLR: 0.020000\n",
            "Training Epoch: 69 [49280/50000]\tLoss: 0.4617\tLR: 0.020000\n",
            "Training Epoch: 69 [49408/50000]\tLoss: 0.3784\tLR: 0.020000\n",
            "Training Epoch: 69 [49536/50000]\tLoss: 0.5222\tLR: 0.020000\n",
            "Training Epoch: 69 [49664/50000]\tLoss: 0.4019\tLR: 0.020000\n",
            "Training Epoch: 69 [49792/50000]\tLoss: 0.2816\tLR: 0.020000\n",
            "Training Epoch: 69 [49920/50000]\tLoss: 0.4557\tLR: 0.020000\n",
            "Training Epoch: 69 [50000/50000]\tLoss: 0.5782\tLR: 0.020000\n",
            "epoch 69 training time consumed: 144.54s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  133678 KB |    1044 MB |  252859 GB |  252859 GB |\n",
            "|       from large pool |  122880 KB |    1034 MB |  252655 GB |  252655 GB |\n",
            "|       from small pool |   10798 KB |      13 MB |     204 GB |     204 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  133678 KB |    1044 MB |  252859 GB |  252859 GB |\n",
            "|       from large pool |  122880 KB |    1034 MB |  252655 GB |  252655 GB |\n",
            "|       from small pool |   10798 KB |      13 MB |     204 GB |     204 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    1838 MB |    1838 MB |    1838 MB |       0 B  |\n",
            "|       from large pool |    1822 MB |    1822 MB |    1822 MB |       0 B  |\n",
            "|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  433618 KB |  778793 KB |  117570 GB |  117570 GB |\n",
            "|       from large pool |  432128 KB |  776192 KB |  117366 GB |  117365 GB |\n",
            "|       from small pool |    1490 KB |    3494 KB |     204 GB |     204 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     255    |     335    |   10230 K  |   10229 K  |\n",
            "|       from large pool |      24    |      65    |    5014 K  |    5014 K  |\n",
            "|       from small pool |     231    |     274    |    5215 K  |    5215 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     255    |     335    |   10230 K  |   10229 K  |\n",
            "|       from large pool |      24    |      65    |    5014 K  |    5014 K  |\n",
            "|       from small pool |     231    |     274    |    5215 K  |    5215 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      20    |      20    |      20    |       0    |\n",
            "|       from large pool |      12    |      12    |      12    |       0    |\n",
            "|       from small pool |       8    |       8    |       8    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      36    |      45    |    5922 K  |    5922 K  |\n",
            "|       from large pool |      10    |      15    |    2269 K  |    2269 K  |\n",
            "|       from small pool |      26    |      35    |    3652 K  |    3652 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 69, Average loss: 0.0092, Accuracy: 0.7011, Time consumed:9.00s\n",
            "\n",
            "Training Epoch: 70 [128/50000]\tLoss: 0.4074\tLR: 0.020000\n",
            "Training Epoch: 70 [256/50000]\tLoss: 0.2967\tLR: 0.020000\n",
            "Training Epoch: 70 [384/50000]\tLoss: 0.2471\tLR: 0.020000\n",
            "Training Epoch: 70 [512/50000]\tLoss: 0.2475\tLR: 0.020000\n",
            "Training Epoch: 70 [640/50000]\tLoss: 0.2198\tLR: 0.020000\n",
            "Training Epoch: 70 [768/50000]\tLoss: 0.3047\tLR: 0.020000\n",
            "Training Epoch: 70 [896/50000]\tLoss: 0.3268\tLR: 0.020000\n",
            "Training Epoch: 70 [1024/50000]\tLoss: 0.3262\tLR: 0.020000\n",
            "Training Epoch: 70 [1152/50000]\tLoss: 0.2784\tLR: 0.020000\n",
            "Training Epoch: 70 [1280/50000]\tLoss: 0.2966\tLR: 0.020000\n",
            "Training Epoch: 70 [1408/50000]\tLoss: 0.3362\tLR: 0.020000\n",
            "Training Epoch: 70 [1536/50000]\tLoss: 0.2352\tLR: 0.020000\n",
            "Training Epoch: 70 [1664/50000]\tLoss: 0.3746\tLR: 0.020000\n",
            "Training Epoch: 70 [1792/50000]\tLoss: 0.2288\tLR: 0.020000\n",
            "Training Epoch: 70 [1920/50000]\tLoss: 0.1687\tLR: 0.020000\n",
            "Training Epoch: 70 [2048/50000]\tLoss: 0.2169\tLR: 0.020000\n",
            "Training Epoch: 70 [2176/50000]\tLoss: 0.2364\tLR: 0.020000\n",
            "Training Epoch: 70 [2304/50000]\tLoss: 0.3688\tLR: 0.020000\n",
            "Training Epoch: 70 [2432/50000]\tLoss: 0.4108\tLR: 0.020000\n",
            "Training Epoch: 70 [2560/50000]\tLoss: 0.2446\tLR: 0.020000\n",
            "Training Epoch: 70 [2688/50000]\tLoss: 0.2604\tLR: 0.020000\n",
            "Training Epoch: 70 [2816/50000]\tLoss: 0.3767\tLR: 0.020000\n",
            "Training Epoch: 70 [2944/50000]\tLoss: 0.2815\tLR: 0.020000\n",
            "Training Epoch: 70 [3072/50000]\tLoss: 0.2984\tLR: 0.020000\n",
            "Training Epoch: 70 [3200/50000]\tLoss: 0.3441\tLR: 0.020000\n",
            "Training Epoch: 70 [3328/50000]\tLoss: 0.2566\tLR: 0.020000\n",
            "Training Epoch: 70 [3456/50000]\tLoss: 0.3307\tLR: 0.020000\n",
            "Training Epoch: 70 [3584/50000]\tLoss: 0.2780\tLR: 0.020000\n",
            "Training Epoch: 70 [3712/50000]\tLoss: 0.2816\tLR: 0.020000\n",
            "Training Epoch: 70 [3840/50000]\tLoss: 0.3055\tLR: 0.020000\n",
            "Training Epoch: 70 [3968/50000]\tLoss: 0.2897\tLR: 0.020000\n",
            "Training Epoch: 70 [4096/50000]\tLoss: 0.3149\tLR: 0.020000\n",
            "Training Epoch: 70 [4224/50000]\tLoss: 0.2967\tLR: 0.020000\n",
            "Training Epoch: 70 [4352/50000]\tLoss: 0.2978\tLR: 0.020000\n",
            "Training Epoch: 70 [4480/50000]\tLoss: 0.3433\tLR: 0.020000\n",
            "Training Epoch: 70 [4608/50000]\tLoss: 0.3488\tLR: 0.020000\n",
            "Training Epoch: 70 [4736/50000]\tLoss: 0.3214\tLR: 0.020000\n",
            "Training Epoch: 70 [4864/50000]\tLoss: 0.2605\tLR: 0.020000\n",
            "Training Epoch: 70 [4992/50000]\tLoss: 0.2705\tLR: 0.020000\n",
            "Training Epoch: 70 [5120/50000]\tLoss: 0.3038\tLR: 0.020000\n",
            "Training Epoch: 70 [5248/50000]\tLoss: 0.2824\tLR: 0.020000\n",
            "Training Epoch: 70 [5376/50000]\tLoss: 0.3726\tLR: 0.020000\n",
            "Training Epoch: 70 [5504/50000]\tLoss: 0.2417\tLR: 0.020000\n",
            "Training Epoch: 70 [5632/50000]\tLoss: 0.3044\tLR: 0.020000\n",
            "Training Epoch: 70 [5760/50000]\tLoss: 0.4006\tLR: 0.020000\n",
            "Training Epoch: 70 [5888/50000]\tLoss: 0.1654\tLR: 0.020000\n",
            "Training Epoch: 70 [6016/50000]\tLoss: 0.2483\tLR: 0.020000\n",
            "Training Epoch: 70 [6144/50000]\tLoss: 0.3867\tLR: 0.020000\n",
            "Training Epoch: 70 [6272/50000]\tLoss: 0.4382\tLR: 0.020000\n",
            "Training Epoch: 70 [6400/50000]\tLoss: 0.2695\tLR: 0.020000\n",
            "Training Epoch: 70 [6528/50000]\tLoss: 0.2784\tLR: 0.020000\n",
            "Training Epoch: 70 [6656/50000]\tLoss: 0.1795\tLR: 0.020000\n",
            "Training Epoch: 70 [6784/50000]\tLoss: 0.2735\tLR: 0.020000\n",
            "Training Epoch: 70 [6912/50000]\tLoss: 0.2615\tLR: 0.020000\n",
            "Training Epoch: 70 [7040/50000]\tLoss: 0.3306\tLR: 0.020000\n",
            "Training Epoch: 70 [7168/50000]\tLoss: 0.3035\tLR: 0.020000\n",
            "Training Epoch: 70 [7296/50000]\tLoss: 0.3726\tLR: 0.020000\n",
            "Training Epoch: 70 [7424/50000]\tLoss: 0.3841\tLR: 0.020000\n",
            "Training Epoch: 70 [7552/50000]\tLoss: 0.3616\tLR: 0.020000\n",
            "Training Epoch: 70 [7680/50000]\tLoss: 0.3116\tLR: 0.020000\n",
            "Training Epoch: 70 [7808/50000]\tLoss: 0.3119\tLR: 0.020000\n",
            "Training Epoch: 70 [7936/50000]\tLoss: 0.2664\tLR: 0.020000\n",
            "Training Epoch: 70 [8064/50000]\tLoss: 0.2964\tLR: 0.020000\n",
            "Training Epoch: 70 [8192/50000]\tLoss: 0.3185\tLR: 0.020000\n",
            "Training Epoch: 70 [8320/50000]\tLoss: 0.2857\tLR: 0.020000\n",
            "Training Epoch: 70 [8448/50000]\tLoss: 0.2773\tLR: 0.020000\n",
            "Training Epoch: 70 [8576/50000]\tLoss: 0.2742\tLR: 0.020000\n",
            "Training Epoch: 70 [8704/50000]\tLoss: 0.3758\tLR: 0.020000\n",
            "Training Epoch: 70 [8832/50000]\tLoss: 0.2754\tLR: 0.020000\n",
            "Training Epoch: 70 [8960/50000]\tLoss: 0.3372\tLR: 0.020000\n",
            "Training Epoch: 70 [9088/50000]\tLoss: 0.2140\tLR: 0.020000\n",
            "Training Epoch: 70 [9216/50000]\tLoss: 0.3679\tLR: 0.020000\n",
            "Training Epoch: 70 [9344/50000]\tLoss: 0.2465\tLR: 0.020000\n",
            "Training Epoch: 70 [9472/50000]\tLoss: 0.2858\tLR: 0.020000\n",
            "Training Epoch: 70 [9600/50000]\tLoss: 0.3800\tLR: 0.020000\n",
            "Training Epoch: 70 [9728/50000]\tLoss: 0.3344\tLR: 0.020000\n",
            "Training Epoch: 70 [9856/50000]\tLoss: 0.2194\tLR: 0.020000\n",
            "Training Epoch: 70 [9984/50000]\tLoss: 0.3255\tLR: 0.020000\n",
            "Training Epoch: 70 [10112/50000]\tLoss: 0.2327\tLR: 0.020000\n",
            "Training Epoch: 70 [10240/50000]\tLoss: 0.2636\tLR: 0.020000\n",
            "Training Epoch: 70 [10368/50000]\tLoss: 0.4293\tLR: 0.020000\n",
            "Training Epoch: 70 [10496/50000]\tLoss: 0.3464\tLR: 0.020000\n",
            "Training Epoch: 70 [10624/50000]\tLoss: 0.4141\tLR: 0.020000\n",
            "Training Epoch: 70 [10752/50000]\tLoss: 0.3104\tLR: 0.020000\n",
            "Training Epoch: 70 [10880/50000]\tLoss: 0.3047\tLR: 0.020000\n",
            "Training Epoch: 70 [11008/50000]\tLoss: 0.3431\tLR: 0.020000\n",
            "Training Epoch: 70 [11136/50000]\tLoss: 0.2321\tLR: 0.020000\n",
            "Training Epoch: 70 [11264/50000]\tLoss: 0.2352\tLR: 0.020000\n",
            "Training Epoch: 70 [11392/50000]\tLoss: 0.2889\tLR: 0.020000\n",
            "Training Epoch: 70 [11520/50000]\tLoss: 0.1989\tLR: 0.020000\n",
            "Training Epoch: 70 [11648/50000]\tLoss: 0.3008\tLR: 0.020000\n",
            "Training Epoch: 70 [11776/50000]\tLoss: 0.3307\tLR: 0.020000\n",
            "Training Epoch: 70 [11904/50000]\tLoss: 0.3054\tLR: 0.020000\n",
            "Training Epoch: 70 [12032/50000]\tLoss: 0.2420\tLR: 0.020000\n",
            "Training Epoch: 70 [12160/50000]\tLoss: 0.3327\tLR: 0.020000\n",
            "Training Epoch: 70 [12288/50000]\tLoss: 0.2926\tLR: 0.020000\n",
            "Training Epoch: 70 [12416/50000]\tLoss: 0.2683\tLR: 0.020000\n",
            "Training Epoch: 70 [12544/50000]\tLoss: 0.3274\tLR: 0.020000\n",
            "Training Epoch: 70 [12672/50000]\tLoss: 0.2293\tLR: 0.020000\n",
            "Training Epoch: 70 [12800/50000]\tLoss: 0.2406\tLR: 0.020000\n",
            "Training Epoch: 70 [12928/50000]\tLoss: 0.3166\tLR: 0.020000\n",
            "Training Epoch: 70 [13056/50000]\tLoss: 0.2151\tLR: 0.020000\n",
            "Training Epoch: 70 [13184/50000]\tLoss: 0.3435\tLR: 0.020000\n",
            "Training Epoch: 70 [13312/50000]\tLoss: 0.2843\tLR: 0.020000\n",
            "Training Epoch: 70 [13440/50000]\tLoss: 0.3842\tLR: 0.020000\n",
            "Training Epoch: 70 [13568/50000]\tLoss: 0.3086\tLR: 0.020000\n",
            "Training Epoch: 70 [13696/50000]\tLoss: 0.3615\tLR: 0.020000\n",
            "Training Epoch: 70 [13824/50000]\tLoss: 0.3390\tLR: 0.020000\n",
            "Training Epoch: 70 [13952/50000]\tLoss: 0.3378\tLR: 0.020000\n",
            "Training Epoch: 70 [14080/50000]\tLoss: 0.3179\tLR: 0.020000\n",
            "Training Epoch: 70 [14208/50000]\tLoss: 0.2871\tLR: 0.020000\n",
            "Training Epoch: 70 [14336/50000]\tLoss: 0.4150\tLR: 0.020000\n",
            "Training Epoch: 70 [14464/50000]\tLoss: 0.3156\tLR: 0.020000\n",
            "Training Epoch: 70 [14592/50000]\tLoss: 0.4573\tLR: 0.020000\n",
            "Training Epoch: 70 [14720/50000]\tLoss: 0.1975\tLR: 0.020000\n",
            "Training Epoch: 70 [14848/50000]\tLoss: 0.1839\tLR: 0.020000\n",
            "Training Epoch: 70 [14976/50000]\tLoss: 0.3593\tLR: 0.020000\n",
            "Training Epoch: 70 [15104/50000]\tLoss: 0.3497\tLR: 0.020000\n",
            "Training Epoch: 70 [15232/50000]\tLoss: 0.3372\tLR: 0.020000\n",
            "Training Epoch: 70 [15360/50000]\tLoss: 0.2720\tLR: 0.020000\n",
            "Training Epoch: 70 [15488/50000]\tLoss: 0.2700\tLR: 0.020000\n",
            "Training Epoch: 70 [15616/50000]\tLoss: 0.2527\tLR: 0.020000\n",
            "Training Epoch: 70 [15744/50000]\tLoss: 0.3800\tLR: 0.020000\n",
            "Training Epoch: 70 [15872/50000]\tLoss: 0.2684\tLR: 0.020000\n",
            "Training Epoch: 70 [16000/50000]\tLoss: 0.3421\tLR: 0.020000\n",
            "Training Epoch: 70 [16128/50000]\tLoss: 0.2996\tLR: 0.020000\n",
            "Training Epoch: 70 [16256/50000]\tLoss: 0.2391\tLR: 0.020000\n",
            "Training Epoch: 70 [16384/50000]\tLoss: 0.3027\tLR: 0.020000\n",
            "Training Epoch: 70 [16512/50000]\tLoss: 0.3593\tLR: 0.020000\n",
            "Training Epoch: 70 [16640/50000]\tLoss: 0.2327\tLR: 0.020000\n",
            "Training Epoch: 70 [16768/50000]\tLoss: 0.3020\tLR: 0.020000\n",
            "Training Epoch: 70 [16896/50000]\tLoss: 0.2975\tLR: 0.020000\n",
            "Training Epoch: 70 [17024/50000]\tLoss: 0.3312\tLR: 0.020000\n",
            "Training Epoch: 70 [17152/50000]\tLoss: 0.3721\tLR: 0.020000\n",
            "Training Epoch: 70 [17280/50000]\tLoss: 0.3512\tLR: 0.020000\n",
            "Training Epoch: 70 [17408/50000]\tLoss: 0.3841\tLR: 0.020000\n",
            "Training Epoch: 70 [17536/50000]\tLoss: 0.2743\tLR: 0.020000\n",
            "Training Epoch: 70 [17664/50000]\tLoss: 0.3249\tLR: 0.020000\n",
            "Training Epoch: 70 [17792/50000]\tLoss: 0.2679\tLR: 0.020000\n",
            "Training Epoch: 70 [17920/50000]\tLoss: 0.4028\tLR: 0.020000\n",
            "Training Epoch: 70 [18048/50000]\tLoss: 0.3801\tLR: 0.020000\n",
            "Training Epoch: 70 [18176/50000]\tLoss: 0.4386\tLR: 0.020000\n",
            "Training Epoch: 70 [18304/50000]\tLoss: 0.2627\tLR: 0.020000\n",
            "Training Epoch: 70 [18432/50000]\tLoss: 0.3825\tLR: 0.020000\n",
            "Training Epoch: 70 [18560/50000]\tLoss: 0.2832\tLR: 0.020000\n",
            "Training Epoch: 70 [18688/50000]\tLoss: 0.2548\tLR: 0.020000\n",
            "Training Epoch: 70 [18816/50000]\tLoss: 0.3951\tLR: 0.020000\n",
            "Training Epoch: 70 [18944/50000]\tLoss: 0.2615\tLR: 0.020000\n",
            "Training Epoch: 70 [19072/50000]\tLoss: 0.2809\tLR: 0.020000\n",
            "Training Epoch: 70 [19200/50000]\tLoss: 0.3364\tLR: 0.020000\n",
            "Training Epoch: 70 [19328/50000]\tLoss: 0.4065\tLR: 0.020000\n",
            "Training Epoch: 70 [19456/50000]\tLoss: 0.2923\tLR: 0.020000\n",
            "Training Epoch: 70 [19584/50000]\tLoss: 0.3721\tLR: 0.020000\n",
            "Training Epoch: 70 [19712/50000]\tLoss: 0.2906\tLR: 0.020000\n",
            "Training Epoch: 70 [19840/50000]\tLoss: 0.2971\tLR: 0.020000\n",
            "Training Epoch: 70 [19968/50000]\tLoss: 0.3199\tLR: 0.020000\n",
            "Training Epoch: 70 [20096/50000]\tLoss: 0.3034\tLR: 0.020000\n",
            "Training Epoch: 70 [20224/50000]\tLoss: 0.2797\tLR: 0.020000\n",
            "Training Epoch: 70 [20352/50000]\tLoss: 0.3287\tLR: 0.020000\n",
            "Training Epoch: 70 [20480/50000]\tLoss: 0.2880\tLR: 0.020000\n",
            "Training Epoch: 70 [20608/50000]\tLoss: 0.3001\tLR: 0.020000\n",
            "Training Epoch: 70 [20736/50000]\tLoss: 0.3077\tLR: 0.020000\n",
            "Training Epoch: 70 [20864/50000]\tLoss: 0.3412\tLR: 0.020000\n",
            "Training Epoch: 70 [20992/50000]\tLoss: 0.3182\tLR: 0.020000\n",
            "Training Epoch: 70 [21120/50000]\tLoss: 0.2553\tLR: 0.020000\n",
            "Training Epoch: 70 [21248/50000]\tLoss: 0.3328\tLR: 0.020000\n",
            "Training Epoch: 70 [21376/50000]\tLoss: 0.2953\tLR: 0.020000\n",
            "Training Epoch: 70 [21504/50000]\tLoss: 0.3628\tLR: 0.020000\n",
            "Training Epoch: 70 [21632/50000]\tLoss: 0.4035\tLR: 0.020000\n",
            "Training Epoch: 70 [21760/50000]\tLoss: 0.4857\tLR: 0.020000\n",
            "Training Epoch: 70 [21888/50000]\tLoss: 0.4016\tLR: 0.020000\n",
            "Training Epoch: 70 [22016/50000]\tLoss: 0.3078\tLR: 0.020000\n",
            "Training Epoch: 70 [22144/50000]\tLoss: 0.4977\tLR: 0.020000\n",
            "Training Epoch: 70 [22272/50000]\tLoss: 0.3178\tLR: 0.020000\n",
            "Training Epoch: 70 [22400/50000]\tLoss: 0.3081\tLR: 0.020000\n",
            "Training Epoch: 70 [22528/50000]\tLoss: 0.3820\tLR: 0.020000\n",
            "Training Epoch: 70 [22656/50000]\tLoss: 0.3755\tLR: 0.020000\n",
            "Training Epoch: 70 [22784/50000]\tLoss: 0.2519\tLR: 0.020000\n",
            "Training Epoch: 70 [22912/50000]\tLoss: 0.2753\tLR: 0.020000\n",
            "Training Epoch: 70 [23040/50000]\tLoss: 0.3775\tLR: 0.020000\n",
            "Training Epoch: 70 [23168/50000]\tLoss: 0.4237\tLR: 0.020000\n",
            "Training Epoch: 70 [23296/50000]\tLoss: 0.4201\tLR: 0.020000\n",
            "Training Epoch: 70 [23424/50000]\tLoss: 0.3754\tLR: 0.020000\n",
            "Training Epoch: 70 [23552/50000]\tLoss: 0.3592\tLR: 0.020000\n",
            "Training Epoch: 70 [23680/50000]\tLoss: 0.3899\tLR: 0.020000\n",
            "Training Epoch: 70 [23808/50000]\tLoss: 0.3897\tLR: 0.020000\n",
            "Training Epoch: 70 [23936/50000]\tLoss: 0.2608\tLR: 0.020000\n",
            "Training Epoch: 70 [24064/50000]\tLoss: 0.2725\tLR: 0.020000\n",
            "Training Epoch: 70 [24192/50000]\tLoss: 0.3227\tLR: 0.020000\n",
            "Training Epoch: 70 [24320/50000]\tLoss: 0.3369\tLR: 0.020000\n",
            "Training Epoch: 70 [24448/50000]\tLoss: 0.3486\tLR: 0.020000\n",
            "Training Epoch: 70 [24576/50000]\tLoss: 0.3677\tLR: 0.020000\n",
            "Training Epoch: 70 [24704/50000]\tLoss: 0.3727\tLR: 0.020000\n",
            "Training Epoch: 70 [24832/50000]\tLoss: 0.4248\tLR: 0.020000\n",
            "Training Epoch: 70 [24960/50000]\tLoss: 0.2960\tLR: 0.020000\n",
            "Training Epoch: 70 [25088/50000]\tLoss: 0.2888\tLR: 0.020000\n",
            "Training Epoch: 70 [25216/50000]\tLoss: 0.3091\tLR: 0.020000\n",
            "Training Epoch: 70 [25344/50000]\tLoss: 0.3885\tLR: 0.020000\n",
            "Training Epoch: 70 [25472/50000]\tLoss: 0.3267\tLR: 0.020000\n",
            "Training Epoch: 70 [25600/50000]\tLoss: 0.3300\tLR: 0.020000\n",
            "Training Epoch: 70 [25728/50000]\tLoss: 0.3152\tLR: 0.020000\n",
            "Training Epoch: 70 [25856/50000]\tLoss: 0.4453\tLR: 0.020000\n",
            "Training Epoch: 70 [25984/50000]\tLoss: 0.4556\tLR: 0.020000\n",
            "Training Epoch: 70 [26112/50000]\tLoss: 0.3753\tLR: 0.020000\n",
            "Training Epoch: 70 [26240/50000]\tLoss: 0.3759\tLR: 0.020000\n",
            "Training Epoch: 70 [26368/50000]\tLoss: 0.2481\tLR: 0.020000\n",
            "Training Epoch: 70 [26496/50000]\tLoss: 0.3573\tLR: 0.020000\n",
            "Training Epoch: 70 [26624/50000]\tLoss: 0.3468\tLR: 0.020000\n",
            "Training Epoch: 70 [26752/50000]\tLoss: 0.2961\tLR: 0.020000\n",
            "Training Epoch: 70 [26880/50000]\tLoss: 0.2289\tLR: 0.020000\n",
            "Training Epoch: 70 [27008/50000]\tLoss: 0.4411\tLR: 0.020000\n",
            "Training Epoch: 70 [27136/50000]\tLoss: 0.4142\tLR: 0.020000\n",
            "Training Epoch: 70 [27264/50000]\tLoss: 0.3376\tLR: 0.020000\n",
            "Training Epoch: 70 [27392/50000]\tLoss: 0.3182\tLR: 0.020000\n",
            "Training Epoch: 70 [27520/50000]\tLoss: 0.4845\tLR: 0.020000\n",
            "Training Epoch: 70 [27648/50000]\tLoss: 0.2602\tLR: 0.020000\n",
            "Training Epoch: 70 [27776/50000]\tLoss: 0.3649\tLR: 0.020000\n",
            "Training Epoch: 70 [27904/50000]\tLoss: 0.4316\tLR: 0.020000\n",
            "Training Epoch: 70 [28032/50000]\tLoss: 0.3105\tLR: 0.020000\n",
            "Training Epoch: 70 [28160/50000]\tLoss: 0.4509\tLR: 0.020000\n",
            "Training Epoch: 70 [28288/50000]\tLoss: 0.3359\tLR: 0.020000\n",
            "Training Epoch: 70 [28416/50000]\tLoss: 0.2566\tLR: 0.020000\n",
            "Training Epoch: 70 [28544/50000]\tLoss: 0.3038\tLR: 0.020000\n",
            "Training Epoch: 70 [28672/50000]\tLoss: 0.4145\tLR: 0.020000\n",
            "Training Epoch: 70 [28800/50000]\tLoss: 0.4344\tLR: 0.020000\n",
            "Training Epoch: 70 [28928/50000]\tLoss: 0.4616\tLR: 0.020000\n",
            "Training Epoch: 70 [29056/50000]\tLoss: 0.3155\tLR: 0.020000\n",
            "Training Epoch: 70 [29184/50000]\tLoss: 0.3034\tLR: 0.020000\n",
            "Training Epoch: 70 [29312/50000]\tLoss: 0.2334\tLR: 0.020000\n",
            "Training Epoch: 70 [29440/50000]\tLoss: 0.2984\tLR: 0.020000\n",
            "Training Epoch: 70 [29568/50000]\tLoss: 0.3751\tLR: 0.020000\n",
            "Training Epoch: 70 [29696/50000]\tLoss: 0.3500\tLR: 0.020000\n",
            "Training Epoch: 70 [29824/50000]\tLoss: 0.3078\tLR: 0.020000\n",
            "Training Epoch: 70 [29952/50000]\tLoss: 0.3025\tLR: 0.020000\n",
            "Training Epoch: 70 [30080/50000]\tLoss: 0.3081\tLR: 0.020000\n",
            "Training Epoch: 70 [30208/50000]\tLoss: 0.4478\tLR: 0.020000\n",
            "Training Epoch: 70 [30336/50000]\tLoss: 0.3215\tLR: 0.020000\n",
            "Training Epoch: 70 [30464/50000]\tLoss: 0.4273\tLR: 0.020000\n",
            "Training Epoch: 70 [30592/50000]\tLoss: 0.3912\tLR: 0.020000\n",
            "Training Epoch: 70 [30720/50000]\tLoss: 0.4275\tLR: 0.020000\n",
            "Training Epoch: 70 [30848/50000]\tLoss: 0.4478\tLR: 0.020000\n",
            "Training Epoch: 70 [30976/50000]\tLoss: 0.4514\tLR: 0.020000\n",
            "Training Epoch: 70 [31104/50000]\tLoss: 0.3727\tLR: 0.020000\n",
            "Training Epoch: 70 [31232/50000]\tLoss: 0.3727\tLR: 0.020000\n",
            "Training Epoch: 70 [31360/50000]\tLoss: 0.4604\tLR: 0.020000\n",
            "Training Epoch: 70 [31488/50000]\tLoss: 0.4172\tLR: 0.020000\n",
            "Training Epoch: 70 [31616/50000]\tLoss: 0.3783\tLR: 0.020000\n",
            "Training Epoch: 70 [31744/50000]\tLoss: 0.3955\tLR: 0.020000\n",
            "Training Epoch: 70 [31872/50000]\tLoss: 0.3871\tLR: 0.020000\n",
            "Training Epoch: 70 [32000/50000]\tLoss: 0.2817\tLR: 0.020000\n",
            "Training Epoch: 70 [32128/50000]\tLoss: 0.4460\tLR: 0.020000\n",
            "Training Epoch: 70 [32256/50000]\tLoss: 0.3627\tLR: 0.020000\n",
            "Training Epoch: 70 [32384/50000]\tLoss: 0.4282\tLR: 0.020000\n",
            "Training Epoch: 70 [32512/50000]\tLoss: 0.3333\tLR: 0.020000\n",
            "Training Epoch: 70 [32640/50000]\tLoss: 0.5100\tLR: 0.020000\n",
            "Training Epoch: 70 [32768/50000]\tLoss: 0.4210\tLR: 0.020000\n",
            "Training Epoch: 70 [32896/50000]\tLoss: 0.3801\tLR: 0.020000\n",
            "Training Epoch: 70 [33024/50000]\tLoss: 0.3833\tLR: 0.020000\n",
            "Training Epoch: 70 [33152/50000]\tLoss: 0.3960\tLR: 0.020000\n",
            "Training Epoch: 70 [33280/50000]\tLoss: 0.3629\tLR: 0.020000\n",
            "Training Epoch: 70 [33408/50000]\tLoss: 0.3326\tLR: 0.020000\n",
            "Training Epoch: 70 [33536/50000]\tLoss: 0.3280\tLR: 0.020000\n",
            "Training Epoch: 70 [33664/50000]\tLoss: 0.4108\tLR: 0.020000\n",
            "Training Epoch: 70 [33792/50000]\tLoss: 0.3752\tLR: 0.020000\n",
            "Training Epoch: 70 [33920/50000]\tLoss: 0.2945\tLR: 0.020000\n",
            "Training Epoch: 70 [34048/50000]\tLoss: 0.3793\tLR: 0.020000\n",
            "Training Epoch: 70 [34176/50000]\tLoss: 0.3011\tLR: 0.020000\n",
            "Training Epoch: 70 [34304/50000]\tLoss: 0.2821\tLR: 0.020000\n",
            "Training Epoch: 70 [34432/50000]\tLoss: 0.4300\tLR: 0.020000\n",
            "Training Epoch: 70 [34560/50000]\tLoss: 0.3978\tLR: 0.020000\n",
            "Training Epoch: 70 [34688/50000]\tLoss: 0.3941\tLR: 0.020000\n",
            "Training Epoch: 70 [34816/50000]\tLoss: 0.4026\tLR: 0.020000\n",
            "Training Epoch: 70 [34944/50000]\tLoss: 0.3538\tLR: 0.020000\n",
            "Training Epoch: 70 [35072/50000]\tLoss: 0.3605\tLR: 0.020000\n",
            "Training Epoch: 70 [35200/50000]\tLoss: 0.4357\tLR: 0.020000\n",
            "Training Epoch: 70 [35328/50000]\tLoss: 0.3786\tLR: 0.020000\n",
            "Training Epoch: 70 [35456/50000]\tLoss: 0.3350\tLR: 0.020000\n",
            "Training Epoch: 70 [35584/50000]\tLoss: 0.4407\tLR: 0.020000\n",
            "Training Epoch: 70 [35712/50000]\tLoss: 0.3315\tLR: 0.020000\n",
            "Training Epoch: 70 [35840/50000]\tLoss: 0.3772\tLR: 0.020000\n",
            "Training Epoch: 70 [35968/50000]\tLoss: 0.3329\tLR: 0.020000\n",
            "Training Epoch: 70 [36096/50000]\tLoss: 0.2794\tLR: 0.020000\n",
            "Training Epoch: 70 [36224/50000]\tLoss: 0.3255\tLR: 0.020000\n",
            "Training Epoch: 70 [36352/50000]\tLoss: 0.3769\tLR: 0.020000\n",
            "Training Epoch: 70 [36480/50000]\tLoss: 0.3401\tLR: 0.020000\n",
            "Training Epoch: 70 [36608/50000]\tLoss: 0.5240\tLR: 0.020000\n",
            "Training Epoch: 70 [36736/50000]\tLoss: 0.3440\tLR: 0.020000\n",
            "Training Epoch: 70 [36864/50000]\tLoss: 0.4812\tLR: 0.020000\n",
            "Training Epoch: 70 [36992/50000]\tLoss: 0.4994\tLR: 0.020000\n",
            "Training Epoch: 70 [37120/50000]\tLoss: 0.3635\tLR: 0.020000\n",
            "Training Epoch: 70 [37248/50000]\tLoss: 0.5292\tLR: 0.020000\n",
            "Training Epoch: 70 [37376/50000]\tLoss: 0.3171\tLR: 0.020000\n",
            "Training Epoch: 70 [37504/50000]\tLoss: 0.2851\tLR: 0.020000\n",
            "Training Epoch: 70 [37632/50000]\tLoss: 0.4057\tLR: 0.020000\n",
            "Training Epoch: 70 [37760/50000]\tLoss: 0.5532\tLR: 0.020000\n",
            "Training Epoch: 70 [37888/50000]\tLoss: 0.3518\tLR: 0.020000\n",
            "Training Epoch: 70 [38016/50000]\tLoss: 0.3693\tLR: 0.020000\n",
            "Training Epoch: 70 [38144/50000]\tLoss: 0.3557\tLR: 0.020000\n",
            "Training Epoch: 70 [38272/50000]\tLoss: 0.3660\tLR: 0.020000\n",
            "Training Epoch: 70 [38400/50000]\tLoss: 0.3989\tLR: 0.020000\n",
            "Training Epoch: 70 [38528/50000]\tLoss: 0.3847\tLR: 0.020000\n",
            "Training Epoch: 70 [38656/50000]\tLoss: 0.4384\tLR: 0.020000\n",
            "Training Epoch: 70 [38784/50000]\tLoss: 0.4179\tLR: 0.020000\n",
            "Training Epoch: 70 [38912/50000]\tLoss: 0.3425\tLR: 0.020000\n",
            "Training Epoch: 70 [39040/50000]\tLoss: 0.3051\tLR: 0.020000\n",
            "Training Epoch: 70 [39168/50000]\tLoss: 0.2344\tLR: 0.020000\n",
            "Training Epoch: 70 [39296/50000]\tLoss: 0.4122\tLR: 0.020000\n",
            "Training Epoch: 70 [39424/50000]\tLoss: 0.3966\tLR: 0.020000\n",
            "Training Epoch: 70 [39552/50000]\tLoss: 0.4284\tLR: 0.020000\n",
            "Training Epoch: 70 [39680/50000]\tLoss: 0.4173\tLR: 0.020000\n",
            "Training Epoch: 70 [39808/50000]\tLoss: 0.5146\tLR: 0.020000\n",
            "Training Epoch: 70 [39936/50000]\tLoss: 0.3125\tLR: 0.020000\n",
            "Training Epoch: 70 [40064/50000]\tLoss: 0.4459\tLR: 0.020000\n",
            "Training Epoch: 70 [40192/50000]\tLoss: 0.4027\tLR: 0.020000\n",
            "Training Epoch: 70 [40320/50000]\tLoss: 0.4014\tLR: 0.020000\n",
            "Training Epoch: 70 [40448/50000]\tLoss: 0.3497\tLR: 0.020000\n",
            "Training Epoch: 70 [40576/50000]\tLoss: 0.3539\tLR: 0.020000\n",
            "Training Epoch: 70 [40704/50000]\tLoss: 0.4013\tLR: 0.020000\n",
            "Training Epoch: 70 [40832/50000]\tLoss: 0.3309\tLR: 0.020000\n",
            "Training Epoch: 70 [40960/50000]\tLoss: 0.4875\tLR: 0.020000\n",
            "Training Epoch: 70 [41088/50000]\tLoss: 0.4242\tLR: 0.020000\n",
            "Training Epoch: 70 [41216/50000]\tLoss: 0.3840\tLR: 0.020000\n",
            "Training Epoch: 70 [41344/50000]\tLoss: 0.4482\tLR: 0.020000\n",
            "Training Epoch: 70 [41472/50000]\tLoss: 0.3247\tLR: 0.020000\n",
            "Training Epoch: 70 [41600/50000]\tLoss: 0.4319\tLR: 0.020000\n",
            "Training Epoch: 70 [41728/50000]\tLoss: 0.4242\tLR: 0.020000\n",
            "Training Epoch: 70 [41856/50000]\tLoss: 0.3210\tLR: 0.020000\n",
            "Training Epoch: 70 [41984/50000]\tLoss: 0.4040\tLR: 0.020000\n",
            "Training Epoch: 70 [42112/50000]\tLoss: 0.3539\tLR: 0.020000\n",
            "Training Epoch: 70 [42240/50000]\tLoss: 0.3863\tLR: 0.020000\n",
            "Training Epoch: 70 [42368/50000]\tLoss: 0.5005\tLR: 0.020000\n",
            "Training Epoch: 70 [42496/50000]\tLoss: 0.3481\tLR: 0.020000\n",
            "Training Epoch: 70 [42624/50000]\tLoss: 0.3789\tLR: 0.020000\n",
            "Training Epoch: 70 [42752/50000]\tLoss: 0.3553\tLR: 0.020000\n",
            "Training Epoch: 70 [42880/50000]\tLoss: 0.4378\tLR: 0.020000\n",
            "Training Epoch: 70 [43008/50000]\tLoss: 0.4335\tLR: 0.020000\n",
            "Training Epoch: 70 [43136/50000]\tLoss: 0.3627\tLR: 0.020000\n",
            "Training Epoch: 70 [43264/50000]\tLoss: 0.4526\tLR: 0.020000\n",
            "Training Epoch: 70 [43392/50000]\tLoss: 0.3661\tLR: 0.020000\n",
            "Training Epoch: 70 [43520/50000]\tLoss: 0.3903\tLR: 0.020000\n",
            "Training Epoch: 70 [43648/50000]\tLoss: 0.5107\tLR: 0.020000\n",
            "Training Epoch: 70 [43776/50000]\tLoss: 0.4634\tLR: 0.020000\n",
            "Training Epoch: 70 [43904/50000]\tLoss: 0.4648\tLR: 0.020000\n",
            "Training Epoch: 70 [44032/50000]\tLoss: 0.5284\tLR: 0.020000\n",
            "Training Epoch: 70 [44160/50000]\tLoss: 0.5544\tLR: 0.020000\n",
            "Training Epoch: 70 [44288/50000]\tLoss: 0.3750\tLR: 0.020000\n",
            "Training Epoch: 70 [44416/50000]\tLoss: 0.3353\tLR: 0.020000\n",
            "Training Epoch: 70 [44544/50000]\tLoss: 0.4418\tLR: 0.020000\n",
            "Training Epoch: 70 [44672/50000]\tLoss: 0.4373\tLR: 0.020000\n",
            "Training Epoch: 70 [44800/50000]\tLoss: 0.3598\tLR: 0.020000\n",
            "Training Epoch: 70 [44928/50000]\tLoss: 0.4849\tLR: 0.020000\n",
            "Training Epoch: 70 [45056/50000]\tLoss: 0.4318\tLR: 0.020000\n",
            "Training Epoch: 70 [45184/50000]\tLoss: 0.4532\tLR: 0.020000\n",
            "Training Epoch: 70 [45312/50000]\tLoss: 0.5271\tLR: 0.020000\n",
            "Training Epoch: 70 [45440/50000]\tLoss: 0.3652\tLR: 0.020000\n",
            "Training Epoch: 70 [45568/50000]\tLoss: 0.5264\tLR: 0.020000\n",
            "Training Epoch: 70 [45696/50000]\tLoss: 0.4301\tLR: 0.020000\n",
            "Training Epoch: 70 [45824/50000]\tLoss: 0.5417\tLR: 0.020000\n",
            "Training Epoch: 70 [45952/50000]\tLoss: 0.4335\tLR: 0.020000\n",
            "Training Epoch: 70 [46080/50000]\tLoss: 0.4889\tLR: 0.020000\n",
            "Training Epoch: 70 [46208/50000]\tLoss: 0.3438\tLR: 0.020000\n",
            "Training Epoch: 70 [46336/50000]\tLoss: 0.3048\tLR: 0.020000\n",
            "Training Epoch: 70 [46464/50000]\tLoss: 0.3553\tLR: 0.020000\n",
            "Training Epoch: 70 [46592/50000]\tLoss: 0.3369\tLR: 0.020000\n",
            "Training Epoch: 70 [46720/50000]\tLoss: 0.4764\tLR: 0.020000\n",
            "Training Epoch: 70 [46848/50000]\tLoss: 0.4727\tLR: 0.020000\n",
            "Training Epoch: 70 [46976/50000]\tLoss: 0.4696\tLR: 0.020000\n",
            "Training Epoch: 70 [47104/50000]\tLoss: 0.3977\tLR: 0.020000\n",
            "Training Epoch: 70 [47232/50000]\tLoss: 0.5375\tLR: 0.020000\n",
            "Training Epoch: 70 [47360/50000]\tLoss: 0.3494\tLR: 0.020000\n",
            "Training Epoch: 70 [47488/50000]\tLoss: 0.4633\tLR: 0.020000\n",
            "Training Epoch: 70 [47616/50000]\tLoss: 0.3534\tLR: 0.020000\n",
            "Training Epoch: 70 [47744/50000]\tLoss: 0.4009\tLR: 0.020000\n",
            "Training Epoch: 70 [47872/50000]\tLoss: 0.4980\tLR: 0.020000\n",
            "Training Epoch: 70 [48000/50000]\tLoss: 0.4533\tLR: 0.020000\n",
            "Training Epoch: 70 [48128/50000]\tLoss: 0.5039\tLR: 0.020000\n",
            "Training Epoch: 70 [48256/50000]\tLoss: 0.4312\tLR: 0.020000\n",
            "Training Epoch: 70 [48384/50000]\tLoss: 0.4481\tLR: 0.020000\n",
            "Training Epoch: 70 [48512/50000]\tLoss: 0.4246\tLR: 0.020000\n",
            "Training Epoch: 70 [48640/50000]\tLoss: 0.3394\tLR: 0.020000\n",
            "Training Epoch: 70 [48768/50000]\tLoss: 0.4220\tLR: 0.020000\n",
            "Training Epoch: 70 [48896/50000]\tLoss: 0.4736\tLR: 0.020000\n",
            "Training Epoch: 70 [49024/50000]\tLoss: 0.2900\tLR: 0.020000\n",
            "Training Epoch: 70 [49152/50000]\tLoss: 0.4024\tLR: 0.020000\n",
            "Training Epoch: 70 [49280/50000]\tLoss: 0.3588\tLR: 0.020000\n",
            "Training Epoch: 70 [49408/50000]\tLoss: 0.3766\tLR: 0.020000\n",
            "Training Epoch: 70 [49536/50000]\tLoss: 0.3845\tLR: 0.020000\n",
            "Training Epoch: 70 [49664/50000]\tLoss: 0.3494\tLR: 0.020000\n",
            "Training Epoch: 70 [49792/50000]\tLoss: 0.3967\tLR: 0.020000\n",
            "Training Epoch: 70 [49920/50000]\tLoss: 0.4765\tLR: 0.020000\n",
            "Training Epoch: 70 [50000/50000]\tLoss: 0.5822\tLR: 0.020000\n",
            "epoch 70 training time consumed: 144.52s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  133678 KB |    1044 MB |  256524 GB |  256524 GB |\n",
            "|       from large pool |  122880 KB |    1034 MB |  256317 GB |  256317 GB |\n",
            "|       from small pool |   10798 KB |      13 MB |     207 GB |     207 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  133678 KB |    1044 MB |  256524 GB |  256524 GB |\n",
            "|       from large pool |  122880 KB |    1034 MB |  256317 GB |  256317 GB |\n",
            "|       from small pool |   10798 KB |      13 MB |     207 GB |     207 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    1838 MB |    1838 MB |    1838 MB |       0 B  |\n",
            "|       from large pool |    1822 MB |    1822 MB |    1822 MB |       0 B  |\n",
            "|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  433618 KB |  778793 KB |  119274 GB |  119274 GB |\n",
            "|       from large pool |  432128 KB |  776192 KB |  119067 GB |  119066 GB |\n",
            "|       from small pool |    1490 KB |    3494 KB |     207 GB |     207 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     255    |     335    |   10378 K  |   10378 K  |\n",
            "|       from large pool |      24    |      65    |    5087 K  |    5087 K  |\n",
            "|       from small pool |     231    |     274    |    5291 K  |    5290 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     255    |     335    |   10378 K  |   10378 K  |\n",
            "|       from large pool |      24    |      65    |    5087 K  |    5087 K  |\n",
            "|       from small pool |     231    |     274    |    5291 K  |    5290 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      20    |      20    |      20    |       0    |\n",
            "|       from large pool |      12    |      12    |      12    |       0    |\n",
            "|       from small pool |       8    |       8    |       8    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      36    |      45    |    6008 K  |    6007 K  |\n",
            "|       from large pool |      10    |      15    |    2302 K  |    2302 K  |\n",
            "|       from small pool |      26    |      35    |    3705 K  |    3705 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 70, Average loss: 0.0091, Accuracy: 0.7021, Time consumed:9.01s\n",
            "\n",
            "saving weights file to checkpoint/resnet18/Monday_14_February_2022_17h_59m_02s/resnet18-70-regular.pth\n",
            "Training Epoch: 71 [128/50000]\tLoss: 0.4092\tLR: 0.020000\n",
            "Training Epoch: 71 [256/50000]\tLoss: 0.4451\tLR: 0.020000\n",
            "Training Epoch: 71 [384/50000]\tLoss: 0.3011\tLR: 0.020000\n",
            "Training Epoch: 71 [512/50000]\tLoss: 0.2985\tLR: 0.020000\n",
            "Training Epoch: 71 [640/50000]\tLoss: 0.2733\tLR: 0.020000\n",
            "Training Epoch: 71 [768/50000]\tLoss: 0.3443\tLR: 0.020000\n",
            "Training Epoch: 71 [896/50000]\tLoss: 0.3215\tLR: 0.020000\n",
            "Training Epoch: 71 [1024/50000]\tLoss: 0.2989\tLR: 0.020000\n",
            "Training Epoch: 71 [1152/50000]\tLoss: 0.4648\tLR: 0.020000\n",
            "Training Epoch: 71 [1280/50000]\tLoss: 0.3789\tLR: 0.020000\n",
            "Training Epoch: 71 [1408/50000]\tLoss: 0.4191\tLR: 0.020000\n",
            "Training Epoch: 71 [1536/50000]\tLoss: 0.2746\tLR: 0.020000\n",
            "Training Epoch: 71 [1664/50000]\tLoss: 0.3587\tLR: 0.020000\n",
            "Training Epoch: 71 [1792/50000]\tLoss: 0.3260\tLR: 0.020000\n",
            "Training Epoch: 71 [1920/50000]\tLoss: 0.2394\tLR: 0.020000\n",
            "Training Epoch: 71 [2048/50000]\tLoss: 0.2777\tLR: 0.020000\n",
            "Training Epoch: 71 [2176/50000]\tLoss: 0.2984\tLR: 0.020000\n",
            "Training Epoch: 71 [2304/50000]\tLoss: 0.3798\tLR: 0.020000\n",
            "Training Epoch: 71 [2432/50000]\tLoss: 0.2405\tLR: 0.020000\n",
            "Training Epoch: 71 [2560/50000]\tLoss: 0.3138\tLR: 0.020000\n",
            "Training Epoch: 71 [2688/50000]\tLoss: 0.2387\tLR: 0.020000\n",
            "Training Epoch: 71 [2816/50000]\tLoss: 0.3008\tLR: 0.020000\n",
            "Training Epoch: 71 [2944/50000]\tLoss: 0.3168\tLR: 0.020000\n",
            "Training Epoch: 71 [3072/50000]\tLoss: 0.2964\tLR: 0.020000\n",
            "Training Epoch: 71 [3200/50000]\tLoss: 0.3147\tLR: 0.020000\n",
            "Training Epoch: 71 [3328/50000]\tLoss: 0.2320\tLR: 0.020000\n",
            "Training Epoch: 71 [3456/50000]\tLoss: 0.3611\tLR: 0.020000\n",
            "Training Epoch: 71 [3584/50000]\tLoss: 0.3512\tLR: 0.020000\n",
            "Training Epoch: 71 [3712/50000]\tLoss: 0.3461\tLR: 0.020000\n",
            "Training Epoch: 71 [3840/50000]\tLoss: 0.3352\tLR: 0.020000\n",
            "Training Epoch: 71 [3968/50000]\tLoss: 0.3364\tLR: 0.020000\n",
            "Training Epoch: 71 [4096/50000]\tLoss: 0.2267\tLR: 0.020000\n",
            "Training Epoch: 71 [4224/50000]\tLoss: 0.2949\tLR: 0.020000\n",
            "Training Epoch: 71 [4352/50000]\tLoss: 0.2964\tLR: 0.020000\n",
            "Training Epoch: 71 [4480/50000]\tLoss: 0.3563\tLR: 0.020000\n",
            "Training Epoch: 71 [4608/50000]\tLoss: 0.2930\tLR: 0.020000\n",
            "Training Epoch: 71 [4736/50000]\tLoss: 0.3296\tLR: 0.020000\n",
            "Training Epoch: 71 [4864/50000]\tLoss: 0.3093\tLR: 0.020000\n",
            "Training Epoch: 71 [4992/50000]\tLoss: 0.3771\tLR: 0.020000\n",
            "Training Epoch: 71 [5120/50000]\tLoss: 0.3057\tLR: 0.020000\n",
            "Training Epoch: 71 [5248/50000]\tLoss: 0.2795\tLR: 0.020000\n",
            "Training Epoch: 71 [5376/50000]\tLoss: 0.4568\tLR: 0.020000\n",
            "Training Epoch: 71 [5504/50000]\tLoss: 0.2091\tLR: 0.020000\n",
            "Training Epoch: 71 [5632/50000]\tLoss: 0.2970\tLR: 0.020000\n",
            "Training Epoch: 71 [5760/50000]\tLoss: 0.4050\tLR: 0.020000\n",
            "Training Epoch: 71 [5888/50000]\tLoss: 0.3312\tLR: 0.020000\n",
            "Training Epoch: 71 [6016/50000]\tLoss: 0.1854\tLR: 0.020000\n",
            "Training Epoch: 71 [6144/50000]\tLoss: 0.3398\tLR: 0.020000\n",
            "Training Epoch: 71 [6272/50000]\tLoss: 0.3145\tLR: 0.020000\n",
            "Training Epoch: 71 [6400/50000]\tLoss: 0.3062\tLR: 0.020000\n",
            "Training Epoch: 71 [6528/50000]\tLoss: 0.4003\tLR: 0.020000\n",
            "Training Epoch: 71 [6656/50000]\tLoss: 0.2555\tLR: 0.020000\n",
            "Training Epoch: 71 [6784/50000]\tLoss: 0.4028\tLR: 0.020000\n",
            "Training Epoch: 71 [6912/50000]\tLoss: 0.3129\tLR: 0.020000\n",
            "Training Epoch: 71 [7040/50000]\tLoss: 0.2881\tLR: 0.020000\n",
            "Training Epoch: 71 [7168/50000]\tLoss: 0.3692\tLR: 0.020000\n",
            "Training Epoch: 71 [7296/50000]\tLoss: 0.3243\tLR: 0.020000\n",
            "Training Epoch: 71 [7424/50000]\tLoss: 0.3003\tLR: 0.020000\n",
            "Training Epoch: 71 [7552/50000]\tLoss: 0.3505\tLR: 0.020000\n",
            "Training Epoch: 71 [7680/50000]\tLoss: 0.2506\tLR: 0.020000\n",
            "Training Epoch: 71 [7808/50000]\tLoss: 0.3058\tLR: 0.020000\n",
            "Training Epoch: 71 [7936/50000]\tLoss: 0.3175\tLR: 0.020000\n",
            "Training Epoch: 71 [8064/50000]\tLoss: 0.2795\tLR: 0.020000\n",
            "Training Epoch: 71 [8192/50000]\tLoss: 0.3914\tLR: 0.020000\n",
            "Training Epoch: 71 [8320/50000]\tLoss: 0.4944\tLR: 0.020000\n",
            "Training Epoch: 71 [8448/50000]\tLoss: 0.2506\tLR: 0.020000\n",
            "Training Epoch: 71 [8576/50000]\tLoss: 0.3168\tLR: 0.020000\n",
            "Training Epoch: 71 [8704/50000]\tLoss: 0.2968\tLR: 0.020000\n",
            "Training Epoch: 71 [8832/50000]\tLoss: 0.3021\tLR: 0.020000\n",
            "Training Epoch: 71 [8960/50000]\tLoss: 0.2519\tLR: 0.020000\n",
            "Training Epoch: 71 [9088/50000]\tLoss: 0.2538\tLR: 0.020000\n",
            "Training Epoch: 71 [9216/50000]\tLoss: 0.3653\tLR: 0.020000\n",
            "Training Epoch: 71 [9344/50000]\tLoss: 0.3453\tLR: 0.020000\n",
            "Training Epoch: 71 [9472/50000]\tLoss: 0.3962\tLR: 0.020000\n",
            "Training Epoch: 71 [9600/50000]\tLoss: 0.2295\tLR: 0.020000\n",
            "Training Epoch: 71 [9728/50000]\tLoss: 0.3532\tLR: 0.020000\n",
            "Training Epoch: 71 [9856/50000]\tLoss: 0.3106\tLR: 0.020000\n",
            "Training Epoch: 71 [9984/50000]\tLoss: 0.4637\tLR: 0.020000\n",
            "Training Epoch: 71 [10112/50000]\tLoss: 0.3579\tLR: 0.020000\n",
            "Training Epoch: 71 [10240/50000]\tLoss: 0.3747\tLR: 0.020000\n",
            "Training Epoch: 71 [10368/50000]\tLoss: 0.3781\tLR: 0.020000\n",
            "Training Epoch: 71 [10496/50000]\tLoss: 0.3010\tLR: 0.020000\n",
            "Training Epoch: 71 [10624/50000]\tLoss: 0.2858\tLR: 0.020000\n",
            "Training Epoch: 71 [10752/50000]\tLoss: 0.3016\tLR: 0.020000\n",
            "Training Epoch: 71 [10880/50000]\tLoss: 0.2526\tLR: 0.020000\n",
            "Training Epoch: 71 [11008/50000]\tLoss: 0.3634\tLR: 0.020000\n",
            "Training Epoch: 71 [11136/50000]\tLoss: 0.2575\tLR: 0.020000\n",
            "Training Epoch: 71 [11264/50000]\tLoss: 0.2604\tLR: 0.020000\n",
            "Training Epoch: 71 [11392/50000]\tLoss: 0.3283\tLR: 0.020000\n",
            "Training Epoch: 71 [11520/50000]\tLoss: 0.4099\tLR: 0.020000\n",
            "Training Epoch: 71 [11648/50000]\tLoss: 0.3465\tLR: 0.020000\n",
            "Training Epoch: 71 [11776/50000]\tLoss: 0.2784\tLR: 0.020000\n",
            "Training Epoch: 71 [11904/50000]\tLoss: 0.2889\tLR: 0.020000\n",
            "Training Epoch: 71 [12032/50000]\tLoss: 0.3092\tLR: 0.020000\n",
            "Training Epoch: 71 [12160/50000]\tLoss: 0.3104\tLR: 0.020000\n",
            "Training Epoch: 71 [12288/50000]\tLoss: 0.3531\tLR: 0.020000\n",
            "Training Epoch: 71 [12416/50000]\tLoss: 0.2221\tLR: 0.020000\n",
            "Training Epoch: 71 [12544/50000]\tLoss: 0.3359\tLR: 0.020000\n",
            "Training Epoch: 71 [12672/50000]\tLoss: 0.2187\tLR: 0.020000\n",
            "Training Epoch: 71 [12800/50000]\tLoss: 0.2303\tLR: 0.020000\n",
            "Training Epoch: 71 [12928/50000]\tLoss: 0.4159\tLR: 0.020000\n",
            "Training Epoch: 71 [13056/50000]\tLoss: 0.3291\tLR: 0.020000\n",
            "Training Epoch: 71 [13184/50000]\tLoss: 0.2068\tLR: 0.020000\n",
            "Training Epoch: 71 [13312/50000]\tLoss: 0.2730\tLR: 0.020000\n",
            "Training Epoch: 71 [13440/50000]\tLoss: 0.3160\tLR: 0.020000\n",
            "Training Epoch: 71 [13568/50000]\tLoss: 0.3571\tLR: 0.020000\n",
            "Training Epoch: 71 [13696/50000]\tLoss: 0.3298\tLR: 0.020000\n",
            "Training Epoch: 71 [13824/50000]\tLoss: 0.2455\tLR: 0.020000\n",
            "Training Epoch: 71 [13952/50000]\tLoss: 0.3674\tLR: 0.020000\n",
            "Training Epoch: 71 [14080/50000]\tLoss: 0.1862\tLR: 0.020000\n",
            "Training Epoch: 71 [14208/50000]\tLoss: 0.2960\tLR: 0.020000\n",
            "Training Epoch: 71 [14336/50000]\tLoss: 0.3396\tLR: 0.020000\n",
            "Training Epoch: 71 [14464/50000]\tLoss: 0.3112\tLR: 0.020000\n",
            "Training Epoch: 71 [14592/50000]\tLoss: 0.3888\tLR: 0.020000\n",
            "Training Epoch: 71 [14720/50000]\tLoss: 0.4135\tLR: 0.020000\n",
            "Training Epoch: 71 [14848/50000]\tLoss: 0.2898\tLR: 0.020000\n",
            "Training Epoch: 71 [14976/50000]\tLoss: 0.3193\tLR: 0.020000\n",
            "Training Epoch: 71 [15104/50000]\tLoss: 0.2392\tLR: 0.020000\n",
            "Training Epoch: 71 [15232/50000]\tLoss: 0.2248\tLR: 0.020000\n",
            "Training Epoch: 71 [15360/50000]\tLoss: 0.3172\tLR: 0.020000\n",
            "Training Epoch: 71 [15488/50000]\tLoss: 0.2748\tLR: 0.020000\n",
            "Training Epoch: 71 [15616/50000]\tLoss: 0.2833\tLR: 0.020000\n",
            "Training Epoch: 71 [15744/50000]\tLoss: 0.3810\tLR: 0.020000\n",
            "Training Epoch: 71 [15872/50000]\tLoss: 0.3077\tLR: 0.020000\n",
            "Training Epoch: 71 [16000/50000]\tLoss: 0.2885\tLR: 0.020000\n",
            "Training Epoch: 71 [16128/50000]\tLoss: 0.2540\tLR: 0.020000\n",
            "Training Epoch: 71 [16256/50000]\tLoss: 0.5548\tLR: 0.020000\n",
            "Training Epoch: 71 [16384/50000]\tLoss: 0.3179\tLR: 0.020000\n",
            "Training Epoch: 71 [16512/50000]\tLoss: 0.2802\tLR: 0.020000\n",
            "Training Epoch: 71 [16640/50000]\tLoss: 0.3226\tLR: 0.020000\n",
            "Training Epoch: 71 [16768/50000]\tLoss: 0.2880\tLR: 0.020000\n",
            "Training Epoch: 71 [16896/50000]\tLoss: 0.2451\tLR: 0.020000\n",
            "Training Epoch: 71 [17024/50000]\tLoss: 0.2286\tLR: 0.020000\n",
            "Training Epoch: 71 [17152/50000]\tLoss: 0.3778\tLR: 0.020000\n",
            "Training Epoch: 71 [17280/50000]\tLoss: 0.3366\tLR: 0.020000\n",
            "Training Epoch: 71 [17408/50000]\tLoss: 0.4306\tLR: 0.020000\n",
            "Training Epoch: 71 [17536/50000]\tLoss: 0.4122\tLR: 0.020000\n",
            "Training Epoch: 71 [17664/50000]\tLoss: 0.3623\tLR: 0.020000\n",
            "Training Epoch: 71 [17792/50000]\tLoss: 0.3376\tLR: 0.020000\n",
            "Training Epoch: 71 [17920/50000]\tLoss: 0.3441\tLR: 0.020000\n",
            "Training Epoch: 71 [18048/50000]\tLoss: 0.2848\tLR: 0.020000\n",
            "Training Epoch: 71 [18176/50000]\tLoss: 0.3716\tLR: 0.020000\n",
            "Training Epoch: 71 [18304/50000]\tLoss: 0.3684\tLR: 0.020000\n",
            "Training Epoch: 71 [18432/50000]\tLoss: 0.3901\tLR: 0.020000\n",
            "Training Epoch: 71 [18560/50000]\tLoss: 0.2331\tLR: 0.020000\n",
            "Training Epoch: 71 [18688/50000]\tLoss: 0.3410\tLR: 0.020000\n",
            "Training Epoch: 71 [18816/50000]\tLoss: 0.3616\tLR: 0.020000\n",
            "Training Epoch: 71 [18944/50000]\tLoss: 0.2858\tLR: 0.020000\n",
            "Training Epoch: 71 [19072/50000]\tLoss: 0.4512\tLR: 0.020000\n",
            "Training Epoch: 71 [19200/50000]\tLoss: 0.2768\tLR: 0.020000\n",
            "Training Epoch: 71 [19328/50000]\tLoss: 0.2707\tLR: 0.020000\n",
            "Training Epoch: 71 [19456/50000]\tLoss: 0.3367\tLR: 0.020000\n",
            "Training Epoch: 71 [19584/50000]\tLoss: 0.3174\tLR: 0.020000\n",
            "Training Epoch: 71 [19712/50000]\tLoss: 0.3864\tLR: 0.020000\n",
            "Training Epoch: 71 [19840/50000]\tLoss: 0.3650\tLR: 0.020000\n",
            "Training Epoch: 71 [19968/50000]\tLoss: 0.3245\tLR: 0.020000\n",
            "Training Epoch: 71 [20096/50000]\tLoss: 0.2815\tLR: 0.020000\n",
            "Training Epoch: 71 [20224/50000]\tLoss: 0.4061\tLR: 0.020000\n",
            "Training Epoch: 71 [20352/50000]\tLoss: 0.3868\tLR: 0.020000\n",
            "Training Epoch: 71 [20480/50000]\tLoss: 0.3278\tLR: 0.020000\n",
            "Training Epoch: 71 [20608/50000]\tLoss: 0.2806\tLR: 0.020000\n",
            "Training Epoch: 71 [20736/50000]\tLoss: 0.3469\tLR: 0.020000\n",
            "Training Epoch: 71 [20864/50000]\tLoss: 0.4233\tLR: 0.020000\n",
            "Training Epoch: 71 [20992/50000]\tLoss: 0.4432\tLR: 0.020000\n",
            "Training Epoch: 71 [21120/50000]\tLoss: 0.3242\tLR: 0.020000\n",
            "Training Epoch: 71 [21248/50000]\tLoss: 0.3554\tLR: 0.020000\n",
            "Training Epoch: 71 [21376/50000]\tLoss: 0.4275\tLR: 0.020000\n",
            "Training Epoch: 71 [21504/50000]\tLoss: 0.3057\tLR: 0.020000\n",
            "Training Epoch: 71 [21632/50000]\tLoss: 0.3706\tLR: 0.020000\n",
            "Training Epoch: 71 [21760/50000]\tLoss: 0.2715\tLR: 0.020000\n",
            "Training Epoch: 71 [21888/50000]\tLoss: 0.3426\tLR: 0.020000\n",
            "Training Epoch: 71 [22016/50000]\tLoss: 0.3183\tLR: 0.020000\n",
            "Training Epoch: 71 [22144/50000]\tLoss: 0.2735\tLR: 0.020000\n",
            "Training Epoch: 71 [22272/50000]\tLoss: 0.3010\tLR: 0.020000\n",
            "Training Epoch: 71 [22400/50000]\tLoss: 0.2883\tLR: 0.020000\n",
            "Training Epoch: 71 [22528/50000]\tLoss: 0.3170\tLR: 0.020000\n",
            "Training Epoch: 71 [22656/50000]\tLoss: 0.3359\tLR: 0.020000\n",
            "Training Epoch: 71 [22784/50000]\tLoss: 0.3205\tLR: 0.020000\n",
            "Training Epoch: 71 [22912/50000]\tLoss: 0.4755\tLR: 0.020000\n",
            "Training Epoch: 71 [23040/50000]\tLoss: 0.3423\tLR: 0.020000\n",
            "Training Epoch: 71 [23168/50000]\tLoss: 0.3200\tLR: 0.020000\n",
            "Training Epoch: 71 [23296/50000]\tLoss: 0.4430\tLR: 0.020000\n",
            "Training Epoch: 71 [23424/50000]\tLoss: 0.3161\tLR: 0.020000\n",
            "Training Epoch: 71 [23552/50000]\tLoss: 0.3830\tLR: 0.020000\n",
            "Training Epoch: 71 [23680/50000]\tLoss: 0.4364\tLR: 0.020000\n",
            "Training Epoch: 71 [23808/50000]\tLoss: 0.2909\tLR: 0.020000\n",
            "Training Epoch: 71 [23936/50000]\tLoss: 0.3307\tLR: 0.020000\n",
            "Training Epoch: 71 [24064/50000]\tLoss: 0.3050\tLR: 0.020000\n",
            "Training Epoch: 71 [24192/50000]\tLoss: 0.4248\tLR: 0.020000\n",
            "Training Epoch: 71 [24320/50000]\tLoss: 0.3659\tLR: 0.020000\n",
            "Training Epoch: 71 [24448/50000]\tLoss: 0.2882\tLR: 0.020000\n",
            "Training Epoch: 71 [24576/50000]\tLoss: 0.2953\tLR: 0.020000\n",
            "Training Epoch: 71 [24704/50000]\tLoss: 0.3321\tLR: 0.020000\n",
            "Training Epoch: 71 [24832/50000]\tLoss: 0.3286\tLR: 0.020000\n",
            "Training Epoch: 71 [24960/50000]\tLoss: 0.4512\tLR: 0.020000\n",
            "Training Epoch: 71 [25088/50000]\tLoss: 0.4464\tLR: 0.020000\n",
            "Training Epoch: 71 [25216/50000]\tLoss: 0.4028\tLR: 0.020000\n",
            "Training Epoch: 71 [25344/50000]\tLoss: 0.2878\tLR: 0.020000\n",
            "Training Epoch: 71 [25472/50000]\tLoss: 0.4531\tLR: 0.020000\n",
            "Training Epoch: 71 [25600/50000]\tLoss: 0.3798\tLR: 0.020000\n",
            "Training Epoch: 71 [25728/50000]\tLoss: 0.3991\tLR: 0.020000\n",
            "Training Epoch: 71 [25856/50000]\tLoss: 0.3669\tLR: 0.020000\n",
            "Training Epoch: 71 [25984/50000]\tLoss: 0.3842\tLR: 0.020000\n",
            "Training Epoch: 71 [26112/50000]\tLoss: 0.4252\tLR: 0.020000\n",
            "Training Epoch: 71 [26240/50000]\tLoss: 0.3201\tLR: 0.020000\n",
            "Training Epoch: 71 [26368/50000]\tLoss: 0.2980\tLR: 0.020000\n",
            "Training Epoch: 71 [26496/50000]\tLoss: 0.3854\tLR: 0.020000\n",
            "Training Epoch: 71 [26624/50000]\tLoss: 0.3269\tLR: 0.020000\n",
            "Training Epoch: 71 [26752/50000]\tLoss: 0.4030\tLR: 0.020000\n",
            "Training Epoch: 71 [26880/50000]\tLoss: 0.3144\tLR: 0.020000\n",
            "Training Epoch: 71 [27008/50000]\tLoss: 0.2623\tLR: 0.020000\n",
            "Training Epoch: 71 [27136/50000]\tLoss: 0.3616\tLR: 0.020000\n",
            "Training Epoch: 71 [27264/50000]\tLoss: 0.3161\tLR: 0.020000\n",
            "Training Epoch: 71 [27392/50000]\tLoss: 0.4404\tLR: 0.020000\n",
            "Training Epoch: 71 [27520/50000]\tLoss: 0.3469\tLR: 0.020000\n",
            "Training Epoch: 71 [27648/50000]\tLoss: 0.2525\tLR: 0.020000\n",
            "Training Epoch: 71 [27776/50000]\tLoss: 0.3430\tLR: 0.020000\n",
            "Training Epoch: 71 [27904/50000]\tLoss: 0.3260\tLR: 0.020000\n",
            "Training Epoch: 71 [28032/50000]\tLoss: 0.2753\tLR: 0.020000\n",
            "Training Epoch: 71 [28160/50000]\tLoss: 0.3363\tLR: 0.020000\n",
            "Training Epoch: 71 [28288/50000]\tLoss: 0.4622\tLR: 0.020000\n",
            "Training Epoch: 71 [28416/50000]\tLoss: 0.2754\tLR: 0.020000\n",
            "Training Epoch: 71 [28544/50000]\tLoss: 0.2845\tLR: 0.020000\n",
            "Training Epoch: 71 [28672/50000]\tLoss: 0.3378\tLR: 0.020000\n",
            "Training Epoch: 71 [28800/50000]\tLoss: 0.3352\tLR: 0.020000\n",
            "Training Epoch: 71 [28928/50000]\tLoss: 0.3715\tLR: 0.020000\n",
            "Training Epoch: 71 [29056/50000]\tLoss: 0.3024\tLR: 0.020000\n",
            "Training Epoch: 71 [29184/50000]\tLoss: 0.3795\tLR: 0.020000\n",
            "Training Epoch: 71 [29312/50000]\tLoss: 0.2748\tLR: 0.020000\n",
            "Training Epoch: 71 [29440/50000]\tLoss: 0.3487\tLR: 0.020000\n",
            "Training Epoch: 71 [29568/50000]\tLoss: 0.3506\tLR: 0.020000\n",
            "Training Epoch: 71 [29696/50000]\tLoss: 0.4895\tLR: 0.020000\n",
            "Training Epoch: 71 [29824/50000]\tLoss: 0.3646\tLR: 0.020000\n",
            "Training Epoch: 71 [29952/50000]\tLoss: 0.2279\tLR: 0.020000\n",
            "Training Epoch: 71 [30080/50000]\tLoss: 0.4047\tLR: 0.020000\n",
            "Training Epoch: 71 [30208/50000]\tLoss: 0.4389\tLR: 0.020000\n",
            "Training Epoch: 71 [30336/50000]\tLoss: 0.4339\tLR: 0.020000\n",
            "Training Epoch: 71 [30464/50000]\tLoss: 0.4107\tLR: 0.020000\n",
            "Training Epoch: 71 [30592/50000]\tLoss: 0.4541\tLR: 0.020000\n",
            "Training Epoch: 71 [30720/50000]\tLoss: 0.2985\tLR: 0.020000\n",
            "Training Epoch: 71 [30848/50000]\tLoss: 0.3506\tLR: 0.020000\n",
            "Training Epoch: 71 [30976/50000]\tLoss: 0.3042\tLR: 0.020000\n",
            "Training Epoch: 71 [31104/50000]\tLoss: 0.5101\tLR: 0.020000\n",
            "Training Epoch: 71 [31232/50000]\tLoss: 0.4591\tLR: 0.020000\n",
            "Training Epoch: 71 [31360/50000]\tLoss: 0.3334\tLR: 0.020000\n",
            "Training Epoch: 71 [31488/50000]\tLoss: 0.4018\tLR: 0.020000\n",
            "Training Epoch: 71 [31616/50000]\tLoss: 0.3654\tLR: 0.020000\n",
            "Training Epoch: 71 [31744/50000]\tLoss: 0.3414\tLR: 0.020000\n",
            "Training Epoch: 71 [31872/50000]\tLoss: 0.3537\tLR: 0.020000\n",
            "Training Epoch: 71 [32000/50000]\tLoss: 0.2148\tLR: 0.020000\n",
            "Training Epoch: 71 [32128/50000]\tLoss: 0.4849\tLR: 0.020000\n",
            "Training Epoch: 71 [32256/50000]\tLoss: 0.3256\tLR: 0.020000\n",
            "Training Epoch: 71 [32384/50000]\tLoss: 0.3587\tLR: 0.020000\n",
            "Training Epoch: 71 [32512/50000]\tLoss: 0.3904\tLR: 0.020000\n",
            "Training Epoch: 71 [32640/50000]\tLoss: 0.4965\tLR: 0.020000\n",
            "Training Epoch: 71 [32768/50000]\tLoss: 0.3133\tLR: 0.020000\n",
            "Training Epoch: 71 [32896/50000]\tLoss: 0.3285\tLR: 0.020000\n",
            "Training Epoch: 71 [33024/50000]\tLoss: 0.4008\tLR: 0.020000\n",
            "Training Epoch: 71 [33152/50000]\tLoss: 0.3716\tLR: 0.020000\n",
            "Training Epoch: 71 [33280/50000]\tLoss: 0.3849\tLR: 0.020000\n",
            "Training Epoch: 71 [33408/50000]\tLoss: 0.3681\tLR: 0.020000\n",
            "Training Epoch: 71 [33536/50000]\tLoss: 0.4040\tLR: 0.020000\n",
            "Training Epoch: 71 [33664/50000]\tLoss: 0.3778\tLR: 0.020000\n",
            "Training Epoch: 71 [33792/50000]\tLoss: 0.4265\tLR: 0.020000\n",
            "Training Epoch: 71 [33920/50000]\tLoss: 0.3808\tLR: 0.020000\n",
            "Training Epoch: 71 [34048/50000]\tLoss: 0.5731\tLR: 0.020000\n",
            "Training Epoch: 71 [34176/50000]\tLoss: 0.3134\tLR: 0.020000\n",
            "Training Epoch: 71 [34304/50000]\tLoss: 0.4150\tLR: 0.020000\n",
            "Training Epoch: 71 [34432/50000]\tLoss: 0.2561\tLR: 0.020000\n",
            "Training Epoch: 71 [34560/50000]\tLoss: 0.2749\tLR: 0.020000\n",
            "Training Epoch: 71 [34688/50000]\tLoss: 0.4550\tLR: 0.020000\n",
            "Training Epoch: 71 [34816/50000]\tLoss: 0.3514\tLR: 0.020000\n",
            "Training Epoch: 71 [34944/50000]\tLoss: 0.3646\tLR: 0.020000\n",
            "Training Epoch: 71 [35072/50000]\tLoss: 0.3960\tLR: 0.020000\n",
            "Training Epoch: 71 [35200/50000]\tLoss: 0.3691\tLR: 0.020000\n",
            "Training Epoch: 71 [35328/50000]\tLoss: 0.4597\tLR: 0.020000\n",
            "Training Epoch: 71 [35456/50000]\tLoss: 0.3629\tLR: 0.020000\n",
            "Training Epoch: 71 [35584/50000]\tLoss: 0.3280\tLR: 0.020000\n",
            "Training Epoch: 71 [35712/50000]\tLoss: 0.4132\tLR: 0.020000\n",
            "Training Epoch: 71 [35840/50000]\tLoss: 0.4107\tLR: 0.020000\n",
            "Training Epoch: 71 [35968/50000]\tLoss: 0.4554\tLR: 0.020000\n",
            "Training Epoch: 71 [36096/50000]\tLoss: 0.4011\tLR: 0.020000\n",
            "Training Epoch: 71 [36224/50000]\tLoss: 0.2558\tLR: 0.020000\n",
            "Training Epoch: 71 [36352/50000]\tLoss: 0.3584\tLR: 0.020000\n",
            "Training Epoch: 71 [36480/50000]\tLoss: 0.3012\tLR: 0.020000\n",
            "Training Epoch: 71 [36608/50000]\tLoss: 0.3683\tLR: 0.020000\n",
            "Training Epoch: 71 [36736/50000]\tLoss: 0.4041\tLR: 0.020000\n",
            "Training Epoch: 71 [36864/50000]\tLoss: 0.3406\tLR: 0.020000\n",
            "Training Epoch: 71 [36992/50000]\tLoss: 0.3198\tLR: 0.020000\n",
            "Training Epoch: 71 [37120/50000]\tLoss: 0.5131\tLR: 0.020000\n",
            "Training Epoch: 71 [37248/50000]\tLoss: 0.4618\tLR: 0.020000\n",
            "Training Epoch: 71 [37376/50000]\tLoss: 0.3371\tLR: 0.020000\n",
            "Training Epoch: 71 [37504/50000]\tLoss: 0.3398\tLR: 0.020000\n",
            "Training Epoch: 71 [37632/50000]\tLoss: 0.3872\tLR: 0.020000\n",
            "Training Epoch: 71 [37760/50000]\tLoss: 0.3745\tLR: 0.020000\n",
            "Training Epoch: 71 [37888/50000]\tLoss: 0.5204\tLR: 0.020000\n",
            "Training Epoch: 71 [38016/50000]\tLoss: 0.4915\tLR: 0.020000\n",
            "Training Epoch: 71 [38144/50000]\tLoss: 0.4569\tLR: 0.020000\n",
            "Training Epoch: 71 [38272/50000]\tLoss: 0.3546\tLR: 0.020000\n",
            "Training Epoch: 71 [38400/50000]\tLoss: 0.3469\tLR: 0.020000\n",
            "Training Epoch: 71 [38528/50000]\tLoss: 0.3738\tLR: 0.020000\n",
            "Training Epoch: 71 [38656/50000]\tLoss: 0.3334\tLR: 0.020000\n",
            "Training Epoch: 71 [38784/50000]\tLoss: 0.2559\tLR: 0.020000\n",
            "Training Epoch: 71 [38912/50000]\tLoss: 0.4168\tLR: 0.020000\n",
            "Training Epoch: 71 [39040/50000]\tLoss: 0.3594\tLR: 0.020000\n",
            "Training Epoch: 71 [39168/50000]\tLoss: 0.2884\tLR: 0.020000\n",
            "Training Epoch: 71 [39296/50000]\tLoss: 0.4731\tLR: 0.020000\n",
            "Training Epoch: 71 [39424/50000]\tLoss: 0.4337\tLR: 0.020000\n",
            "Training Epoch: 71 [39552/50000]\tLoss: 0.3843\tLR: 0.020000\n",
            "Training Epoch: 71 [39680/50000]\tLoss: 0.3217\tLR: 0.020000\n",
            "Training Epoch: 71 [39808/50000]\tLoss: 0.4275\tLR: 0.020000\n",
            "Training Epoch: 71 [39936/50000]\tLoss: 0.3809\tLR: 0.020000\n",
            "Training Epoch: 71 [40064/50000]\tLoss: 0.3041\tLR: 0.020000\n",
            "Training Epoch: 71 [40192/50000]\tLoss: 0.3561\tLR: 0.020000\n",
            "Training Epoch: 71 [40320/50000]\tLoss: 0.3160\tLR: 0.020000\n",
            "Training Epoch: 71 [40448/50000]\tLoss: 0.3812\tLR: 0.020000\n",
            "Training Epoch: 71 [40576/50000]\tLoss: 0.4852\tLR: 0.020000\n",
            "Training Epoch: 71 [40704/50000]\tLoss: 0.5384\tLR: 0.020000\n",
            "Training Epoch: 71 [40832/50000]\tLoss: 0.3725\tLR: 0.020000\n",
            "Training Epoch: 71 [40960/50000]\tLoss: 0.3175\tLR: 0.020000\n",
            "Training Epoch: 71 [41088/50000]\tLoss: 0.4157\tLR: 0.020000\n",
            "Training Epoch: 71 [41216/50000]\tLoss: 0.3755\tLR: 0.020000\n",
            "Training Epoch: 71 [41344/50000]\tLoss: 0.4435\tLR: 0.020000\n",
            "Training Epoch: 71 [41472/50000]\tLoss: 0.5574\tLR: 0.020000\n",
            "Training Epoch: 71 [41600/50000]\tLoss: 0.3440\tLR: 0.020000\n",
            "Training Epoch: 71 [41728/50000]\tLoss: 0.3604\tLR: 0.020000\n",
            "Training Epoch: 71 [41856/50000]\tLoss: 0.4259\tLR: 0.020000\n",
            "Training Epoch: 71 [41984/50000]\tLoss: 0.3904\tLR: 0.020000\n",
            "Training Epoch: 71 [42112/50000]\tLoss: 0.4148\tLR: 0.020000\n",
            "Training Epoch: 71 [42240/50000]\tLoss: 0.2892\tLR: 0.020000\n",
            "Training Epoch: 71 [42368/50000]\tLoss: 0.4034\tLR: 0.020000\n",
            "Training Epoch: 71 [42496/50000]\tLoss: 0.5205\tLR: 0.020000\n",
            "Training Epoch: 71 [42624/50000]\tLoss: 0.3242\tLR: 0.020000\n",
            "Training Epoch: 71 [42752/50000]\tLoss: 0.3824\tLR: 0.020000\n",
            "Training Epoch: 71 [42880/50000]\tLoss: 0.4799\tLR: 0.020000\n",
            "Training Epoch: 71 [43008/50000]\tLoss: 0.5253\tLR: 0.020000\n",
            "Training Epoch: 71 [43136/50000]\tLoss: 0.4005\tLR: 0.020000\n",
            "Training Epoch: 71 [43264/50000]\tLoss: 0.5614\tLR: 0.020000\n",
            "Training Epoch: 71 [43392/50000]\tLoss: 0.3317\tLR: 0.020000\n",
            "Training Epoch: 71 [43520/50000]\tLoss: 0.4066\tLR: 0.020000\n",
            "Training Epoch: 71 [43648/50000]\tLoss: 0.3938\tLR: 0.020000\n",
            "Training Epoch: 71 [43776/50000]\tLoss: 0.3903\tLR: 0.020000\n",
            "Training Epoch: 71 [43904/50000]\tLoss: 0.3162\tLR: 0.020000\n",
            "Training Epoch: 71 [44032/50000]\tLoss: 0.4100\tLR: 0.020000\n",
            "Training Epoch: 71 [44160/50000]\tLoss: 0.5764\tLR: 0.020000\n",
            "Training Epoch: 71 [44288/50000]\tLoss: 0.4325\tLR: 0.020000\n",
            "Training Epoch: 71 [44416/50000]\tLoss: 0.4607\tLR: 0.020000\n",
            "Training Epoch: 71 [44544/50000]\tLoss: 0.5696\tLR: 0.020000\n",
            "Training Epoch: 71 [44672/50000]\tLoss: 0.3439\tLR: 0.020000\n",
            "Training Epoch: 71 [44800/50000]\tLoss: 0.4399\tLR: 0.020000\n",
            "Training Epoch: 71 [44928/50000]\tLoss: 0.5294\tLR: 0.020000\n",
            "Training Epoch: 71 [45056/50000]\tLoss: 0.4041\tLR: 0.020000\n",
            "Training Epoch: 71 [45184/50000]\tLoss: 0.3454\tLR: 0.020000\n",
            "Training Epoch: 71 [45312/50000]\tLoss: 0.3411\tLR: 0.020000\n",
            "Training Epoch: 71 [45440/50000]\tLoss: 0.4660\tLR: 0.020000\n",
            "Training Epoch: 71 [45568/50000]\tLoss: 0.4989\tLR: 0.020000\n",
            "Training Epoch: 71 [45696/50000]\tLoss: 0.2896\tLR: 0.020000\n",
            "Training Epoch: 71 [45824/50000]\tLoss: 0.2799\tLR: 0.020000\n",
            "Training Epoch: 71 [45952/50000]\tLoss: 0.4083\tLR: 0.020000\n",
            "Training Epoch: 71 [46080/50000]\tLoss: 0.5167\tLR: 0.020000\n",
            "Training Epoch: 71 [46208/50000]\tLoss: 0.4146\tLR: 0.020000\n",
            "Training Epoch: 71 [46336/50000]\tLoss: 0.4473\tLR: 0.020000\n",
            "Training Epoch: 71 [46464/50000]\tLoss: 0.3280\tLR: 0.020000\n",
            "Training Epoch: 71 [46592/50000]\tLoss: 0.5721\tLR: 0.020000\n",
            "Training Epoch: 71 [46720/50000]\tLoss: 0.4677\tLR: 0.020000\n",
            "Training Epoch: 71 [46848/50000]\tLoss: 0.3596\tLR: 0.020000\n",
            "Training Epoch: 71 [46976/50000]\tLoss: 0.4936\tLR: 0.020000\n",
            "Training Epoch: 71 [47104/50000]\tLoss: 0.3936\tLR: 0.020000\n",
            "Training Epoch: 71 [47232/50000]\tLoss: 0.4284\tLR: 0.020000\n",
            "Training Epoch: 71 [47360/50000]\tLoss: 0.4726\tLR: 0.020000\n",
            "Training Epoch: 71 [47488/50000]\tLoss: 0.4450\tLR: 0.020000\n",
            "Training Epoch: 71 [47616/50000]\tLoss: 0.3310\tLR: 0.020000\n",
            "Training Epoch: 71 [47744/50000]\tLoss: 0.3440\tLR: 0.020000\n",
            "Training Epoch: 71 [47872/50000]\tLoss: 0.4444\tLR: 0.020000\n",
            "Training Epoch: 71 [48000/50000]\tLoss: 0.3472\tLR: 0.020000\n",
            "Training Epoch: 71 [48128/50000]\tLoss: 0.4576\tLR: 0.020000\n",
            "Training Epoch: 71 [48256/50000]\tLoss: 0.4288\tLR: 0.020000\n",
            "Training Epoch: 71 [48384/50000]\tLoss: 0.3918\tLR: 0.020000\n",
            "Training Epoch: 71 [48512/50000]\tLoss: 0.6081\tLR: 0.020000\n",
            "Training Epoch: 71 [48640/50000]\tLoss: 0.5139\tLR: 0.020000\n",
            "Training Epoch: 71 [48768/50000]\tLoss: 0.4267\tLR: 0.020000\n",
            "Training Epoch: 71 [48896/50000]\tLoss: 0.3058\tLR: 0.020000\n",
            "Training Epoch: 71 [49024/50000]\tLoss: 0.4402\tLR: 0.020000\n",
            "Training Epoch: 71 [49152/50000]\tLoss: 0.3855\tLR: 0.020000\n",
            "Training Epoch: 71 [49280/50000]\tLoss: 0.3892\tLR: 0.020000\n",
            "Training Epoch: 71 [49408/50000]\tLoss: 0.4108\tLR: 0.020000\n",
            "Training Epoch: 71 [49536/50000]\tLoss: 0.3841\tLR: 0.020000\n",
            "Training Epoch: 71 [49664/50000]\tLoss: 0.4751\tLR: 0.020000\n",
            "Training Epoch: 71 [49792/50000]\tLoss: 0.3890\tLR: 0.020000\n",
            "Training Epoch: 71 [49920/50000]\tLoss: 0.5156\tLR: 0.020000\n",
            "Training Epoch: 71 [50000/50000]\tLoss: 0.3878\tLR: 0.020000\n",
            "epoch 71 training time consumed: 144.65s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  133678 KB |    1044 MB |  260189 GB |  260188 GB |\n",
            "|       from large pool |  122880 KB |    1034 MB |  259978 GB |  259978 GB |\n",
            "|       from small pool |   10798 KB |      13 MB |     210 GB |     210 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  133678 KB |    1044 MB |  260189 GB |  260188 GB |\n",
            "|       from large pool |  122880 KB |    1034 MB |  259978 GB |  259978 GB |\n",
            "|       from small pool |   10798 KB |      13 MB |     210 GB |     210 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    1838 MB |    1838 MB |    1838 MB |       0 B  |\n",
            "|       from large pool |    1822 MB |    1822 MB |    1822 MB |       0 B  |\n",
            "|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  433618 KB |  778793 KB |  120978 GB |  120977 GB |\n",
            "|       from large pool |  432128 KB |  776192 KB |  120768 GB |  120767 GB |\n",
            "|       from small pool |    1490 KB |    3494 KB |     210 GB |     210 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     255    |     335    |   10526 K  |   10526 K  |\n",
            "|       from large pool |      24    |      65    |    5159 K  |    5159 K  |\n",
            "|       from small pool |     231    |     274    |    5366 K  |    5366 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     255    |     335    |   10526 K  |   10526 K  |\n",
            "|       from large pool |      24    |      65    |    5159 K  |    5159 K  |\n",
            "|       from small pool |     231    |     274    |    5366 K  |    5366 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      20    |      20    |      20    |       0    |\n",
            "|       from large pool |      12    |      12    |      12    |       0    |\n",
            "|       from small pool |       8    |       8    |       8    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      34    |      45    |    6093 K  |    6093 K  |\n",
            "|       from large pool |      10    |      15    |    2335 K  |    2335 K  |\n",
            "|       from small pool |      24    |      35    |    3758 K  |    3758 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 71, Average loss: 0.0096, Accuracy: 0.6949, Time consumed:8.98s\n",
            "\n",
            "Training Epoch: 72 [128/50000]\tLoss: 0.3274\tLR: 0.020000\n",
            "Training Epoch: 72 [256/50000]\tLoss: 0.3369\tLR: 0.020000\n",
            "Training Epoch: 72 [384/50000]\tLoss: 0.3262\tLR: 0.020000\n",
            "Training Epoch: 72 [512/50000]\tLoss: 0.3528\tLR: 0.020000\n",
            "Training Epoch: 72 [640/50000]\tLoss: 0.2855\tLR: 0.020000\n",
            "Training Epoch: 72 [768/50000]\tLoss: 0.3679\tLR: 0.020000\n",
            "Training Epoch: 72 [896/50000]\tLoss: 0.3378\tLR: 0.020000\n",
            "Training Epoch: 72 [1024/50000]\tLoss: 0.2176\tLR: 0.020000\n",
            "Training Epoch: 72 [1152/50000]\tLoss: 0.1772\tLR: 0.020000\n",
            "Training Epoch: 72 [1280/50000]\tLoss: 0.4483\tLR: 0.020000\n",
            "Training Epoch: 72 [1408/50000]\tLoss: 0.3333\tLR: 0.020000\n",
            "Training Epoch: 72 [1536/50000]\tLoss: 0.3584\tLR: 0.020000\n",
            "Training Epoch: 72 [1664/50000]\tLoss: 0.3609\tLR: 0.020000\n",
            "Training Epoch: 72 [1792/50000]\tLoss: 0.2482\tLR: 0.020000\n",
            "Training Epoch: 72 [1920/50000]\tLoss: 0.3803\tLR: 0.020000\n",
            "Training Epoch: 72 [2048/50000]\tLoss: 0.2972\tLR: 0.020000\n",
            "Training Epoch: 72 [2176/50000]\tLoss: 0.3099\tLR: 0.020000\n",
            "Training Epoch: 72 [2304/50000]\tLoss: 0.3159\tLR: 0.020000\n",
            "Training Epoch: 72 [2432/50000]\tLoss: 0.2990\tLR: 0.020000\n",
            "Training Epoch: 72 [2560/50000]\tLoss: 0.2372\tLR: 0.020000\n",
            "Training Epoch: 72 [2688/50000]\tLoss: 0.2950\tLR: 0.020000\n",
            "Training Epoch: 72 [2816/50000]\tLoss: 0.2889\tLR: 0.020000\n",
            "Training Epoch: 72 [2944/50000]\tLoss: 0.2779\tLR: 0.020000\n",
            "Training Epoch: 72 [3072/50000]\tLoss: 0.3965\tLR: 0.020000\n",
            "Training Epoch: 72 [3200/50000]\tLoss: 0.3535\tLR: 0.020000\n",
            "Training Epoch: 72 [3328/50000]\tLoss: 0.3465\tLR: 0.020000\n",
            "Training Epoch: 72 [3456/50000]\tLoss: 0.3474\tLR: 0.020000\n",
            "Training Epoch: 72 [3584/50000]\tLoss: 0.2633\tLR: 0.020000\n",
            "Training Epoch: 72 [3712/50000]\tLoss: 0.3667\tLR: 0.020000\n",
            "Training Epoch: 72 [3840/50000]\tLoss: 0.4054\tLR: 0.020000\n",
            "Training Epoch: 72 [3968/50000]\tLoss: 0.3375\tLR: 0.020000\n",
            "Training Epoch: 72 [4096/50000]\tLoss: 0.4494\tLR: 0.020000\n",
            "Training Epoch: 72 [4224/50000]\tLoss: 0.3632\tLR: 0.020000\n",
            "Training Epoch: 72 [4352/50000]\tLoss: 0.3036\tLR: 0.020000\n",
            "Training Epoch: 72 [4480/50000]\tLoss: 0.3499\tLR: 0.020000\n",
            "Training Epoch: 72 [4608/50000]\tLoss: 0.3506\tLR: 0.020000\n",
            "Training Epoch: 72 [4736/50000]\tLoss: 0.3822\tLR: 0.020000\n",
            "Training Epoch: 72 [4864/50000]\tLoss: 0.2805\tLR: 0.020000\n",
            "Training Epoch: 72 [4992/50000]\tLoss: 0.2345\tLR: 0.020000\n",
            "Training Epoch: 72 [5120/50000]\tLoss: 0.2777\tLR: 0.020000\n",
            "Training Epoch: 72 [5248/50000]\tLoss: 0.3887\tLR: 0.020000\n",
            "Training Epoch: 72 [5376/50000]\tLoss: 0.3927\tLR: 0.020000\n",
            "Training Epoch: 72 [5504/50000]\tLoss: 0.4334\tLR: 0.020000\n",
            "Training Epoch: 72 [5632/50000]\tLoss: 0.2535\tLR: 0.020000\n",
            "Training Epoch: 72 [5760/50000]\tLoss: 0.2227\tLR: 0.020000\n",
            "Training Epoch: 72 [5888/50000]\tLoss: 0.3170\tLR: 0.020000\n",
            "Training Epoch: 72 [6016/50000]\tLoss: 0.3259\tLR: 0.020000\n",
            "Training Epoch: 72 [6144/50000]\tLoss: 0.3308\tLR: 0.020000\n",
            "Training Epoch: 72 [6272/50000]\tLoss: 0.2288\tLR: 0.020000\n",
            "Training Epoch: 72 [6400/50000]\tLoss: 0.3043\tLR: 0.020000\n",
            "Training Epoch: 72 [6528/50000]\tLoss: 0.2918\tLR: 0.020000\n",
            "Training Epoch: 72 [6656/50000]\tLoss: 0.2761\tLR: 0.020000\n",
            "Training Epoch: 72 [6784/50000]\tLoss: 0.2831\tLR: 0.020000\n",
            "Training Epoch: 72 [6912/50000]\tLoss: 0.3398\tLR: 0.020000\n",
            "Training Epoch: 72 [7040/50000]\tLoss: 0.3224\tLR: 0.020000\n",
            "Training Epoch: 72 [7168/50000]\tLoss: 0.3242\tLR: 0.020000\n",
            "Training Epoch: 72 [7296/50000]\tLoss: 0.2699\tLR: 0.020000\n",
            "Training Epoch: 72 [7424/50000]\tLoss: 0.2697\tLR: 0.020000\n",
            "Training Epoch: 72 [7552/50000]\tLoss: 0.3603\tLR: 0.020000\n",
            "Training Epoch: 72 [7680/50000]\tLoss: 0.3414\tLR: 0.020000\n",
            "Training Epoch: 72 [7808/50000]\tLoss: 0.4116\tLR: 0.020000\n",
            "Training Epoch: 72 [7936/50000]\tLoss: 0.2479\tLR: 0.020000\n",
            "Training Epoch: 72 [8064/50000]\tLoss: 0.3367\tLR: 0.020000\n",
            "Training Epoch: 72 [8192/50000]\tLoss: 0.3302\tLR: 0.020000\n",
            "Training Epoch: 72 [8320/50000]\tLoss: 0.3129\tLR: 0.020000\n",
            "Training Epoch: 72 [8448/50000]\tLoss: 0.1563\tLR: 0.020000\n",
            "Training Epoch: 72 [8576/50000]\tLoss: 0.2996\tLR: 0.020000\n",
            "Training Epoch: 72 [8704/50000]\tLoss: 0.2281\tLR: 0.020000\n",
            "Training Epoch: 72 [8832/50000]\tLoss: 0.3044\tLR: 0.020000\n",
            "Training Epoch: 72 [8960/50000]\tLoss: 0.3703\tLR: 0.020000\n",
            "Training Epoch: 72 [9088/50000]\tLoss: 0.4049\tLR: 0.020000\n",
            "Training Epoch: 72 [9216/50000]\tLoss: 0.3136\tLR: 0.020000\n",
            "Training Epoch: 72 [9344/50000]\tLoss: 0.2312\tLR: 0.020000\n",
            "Training Epoch: 72 [9472/50000]\tLoss: 0.3975\tLR: 0.020000\n",
            "Training Epoch: 72 [9600/50000]\tLoss: 0.3141\tLR: 0.020000\n",
            "Training Epoch: 72 [9728/50000]\tLoss: 0.3208\tLR: 0.020000\n",
            "Training Epoch: 72 [9856/50000]\tLoss: 0.2769\tLR: 0.020000\n",
            "Training Epoch: 72 [9984/50000]\tLoss: 0.3349\tLR: 0.020000\n",
            "Training Epoch: 72 [10112/50000]\tLoss: 0.4684\tLR: 0.020000\n",
            "Training Epoch: 72 [10240/50000]\tLoss: 0.3807\tLR: 0.020000\n",
            "Training Epoch: 72 [10368/50000]\tLoss: 0.3134\tLR: 0.020000\n",
            "Training Epoch: 72 [10496/50000]\tLoss: 0.3312\tLR: 0.020000\n",
            "Training Epoch: 72 [10624/50000]\tLoss: 0.2725\tLR: 0.020000\n",
            "Training Epoch: 72 [10752/50000]\tLoss: 0.3637\tLR: 0.020000\n",
            "Training Epoch: 72 [10880/50000]\tLoss: 0.4250\tLR: 0.020000\n",
            "Training Epoch: 72 [11008/50000]\tLoss: 0.2961\tLR: 0.020000\n",
            "Training Epoch: 72 [11136/50000]\tLoss: 0.3634\tLR: 0.020000\n",
            "Training Epoch: 72 [11264/50000]\tLoss: 0.4352\tLR: 0.020000\n",
            "Training Epoch: 72 [11392/50000]\tLoss: 0.2405\tLR: 0.020000\n",
            "Training Epoch: 72 [11520/50000]\tLoss: 0.3624\tLR: 0.020000\n",
            "Training Epoch: 72 [11648/50000]\tLoss: 0.3270\tLR: 0.020000\n",
            "Training Epoch: 72 [11776/50000]\tLoss: 0.3147\tLR: 0.020000\n",
            "Training Epoch: 72 [11904/50000]\tLoss: 0.4279\tLR: 0.020000\n",
            "Training Epoch: 72 [12032/50000]\tLoss: 0.3770\tLR: 0.020000\n",
            "Training Epoch: 72 [12160/50000]\tLoss: 0.2545\tLR: 0.020000\n",
            "Training Epoch: 72 [12288/50000]\tLoss: 0.2246\tLR: 0.020000\n",
            "Training Epoch: 72 [12416/50000]\tLoss: 0.3548\tLR: 0.020000\n",
            "Training Epoch: 72 [12544/50000]\tLoss: 0.2653\tLR: 0.020000\n",
            "Training Epoch: 72 [12672/50000]\tLoss: 0.3194\tLR: 0.020000\n",
            "Training Epoch: 72 [12800/50000]\tLoss: 0.2277\tLR: 0.020000\n",
            "Training Epoch: 72 [12928/50000]\tLoss: 0.4413\tLR: 0.020000\n",
            "Training Epoch: 72 [13056/50000]\tLoss: 0.3941\tLR: 0.020000\n",
            "Training Epoch: 72 [13184/50000]\tLoss: 0.3403\tLR: 0.020000\n",
            "Training Epoch: 72 [13312/50000]\tLoss: 0.3540\tLR: 0.020000\n",
            "Training Epoch: 72 [13440/50000]\tLoss: 0.2494\tLR: 0.020000\n",
            "Training Epoch: 72 [13568/50000]\tLoss: 0.3759\tLR: 0.020000\n",
            "Training Epoch: 72 [13696/50000]\tLoss: 0.2297\tLR: 0.020000\n",
            "Training Epoch: 72 [13824/50000]\tLoss: 0.3416\tLR: 0.020000\n",
            "Training Epoch: 72 [13952/50000]\tLoss: 0.3677\tLR: 0.020000\n",
            "Training Epoch: 72 [14080/50000]\tLoss: 0.3579\tLR: 0.020000\n",
            "Training Epoch: 72 [14208/50000]\tLoss: 0.2688\tLR: 0.020000\n",
            "Training Epoch: 72 [14336/50000]\tLoss: 0.3519\tLR: 0.020000\n",
            "Training Epoch: 72 [14464/50000]\tLoss: 0.2738\tLR: 0.020000\n",
            "Training Epoch: 72 [14592/50000]\tLoss: 0.3626\tLR: 0.020000\n",
            "Training Epoch: 72 [14720/50000]\tLoss: 0.3000\tLR: 0.020000\n",
            "Training Epoch: 72 [14848/50000]\tLoss: 0.3848\tLR: 0.020000\n",
            "Training Epoch: 72 [14976/50000]\tLoss: 0.3482\tLR: 0.020000\n",
            "Training Epoch: 72 [15104/50000]\tLoss: 0.2666\tLR: 0.020000\n",
            "Training Epoch: 72 [15232/50000]\tLoss: 0.3939\tLR: 0.020000\n",
            "Training Epoch: 72 [15360/50000]\tLoss: 0.2133\tLR: 0.020000\n",
            "Training Epoch: 72 [15488/50000]\tLoss: 0.4088\tLR: 0.020000\n",
            "Training Epoch: 72 [15616/50000]\tLoss: 0.3641\tLR: 0.020000\n",
            "Training Epoch: 72 [15744/50000]\tLoss: 0.4632\tLR: 0.020000\n",
            "Training Epoch: 72 [15872/50000]\tLoss: 0.4094\tLR: 0.020000\n",
            "Training Epoch: 72 [16000/50000]\tLoss: 0.3260\tLR: 0.020000\n",
            "Training Epoch: 72 [16128/50000]\tLoss: 0.3606\tLR: 0.020000\n",
            "Training Epoch: 72 [16256/50000]\tLoss: 0.3423\tLR: 0.020000\n",
            "Training Epoch: 72 [16384/50000]\tLoss: 0.3887\tLR: 0.020000\n",
            "Training Epoch: 72 [16512/50000]\tLoss: 0.3970\tLR: 0.020000\n",
            "Training Epoch: 72 [16640/50000]\tLoss: 0.3549\tLR: 0.020000\n",
            "Training Epoch: 72 [16768/50000]\tLoss: 0.3303\tLR: 0.020000\n",
            "Training Epoch: 72 [16896/50000]\tLoss: 0.2694\tLR: 0.020000\n",
            "Training Epoch: 72 [17024/50000]\tLoss: 0.3144\tLR: 0.020000\n",
            "Training Epoch: 72 [17152/50000]\tLoss: 0.3292\tLR: 0.020000\n",
            "Training Epoch: 72 [17280/50000]\tLoss: 0.3581\tLR: 0.020000\n",
            "Training Epoch: 72 [17408/50000]\tLoss: 0.4268\tLR: 0.020000\n",
            "Training Epoch: 72 [17536/50000]\tLoss: 0.3979\tLR: 0.020000\n",
            "Training Epoch: 72 [17664/50000]\tLoss: 0.4103\tLR: 0.020000\n",
            "Training Epoch: 72 [17792/50000]\tLoss: 0.3786\tLR: 0.020000\n",
            "Training Epoch: 72 [17920/50000]\tLoss: 0.2602\tLR: 0.020000\n",
            "Training Epoch: 72 [18048/50000]\tLoss: 0.3396\tLR: 0.020000\n",
            "Training Epoch: 72 [18176/50000]\tLoss: 0.3120\tLR: 0.020000\n",
            "Training Epoch: 72 [18304/50000]\tLoss: 0.2638\tLR: 0.020000\n",
            "Training Epoch: 72 [18432/50000]\tLoss: 0.2524\tLR: 0.020000\n",
            "Training Epoch: 72 [18560/50000]\tLoss: 0.3207\tLR: 0.020000\n",
            "Training Epoch: 72 [18688/50000]\tLoss: 0.3868\tLR: 0.020000\n",
            "Training Epoch: 72 [18816/50000]\tLoss: 0.3620\tLR: 0.020000\n",
            "Training Epoch: 72 [18944/50000]\tLoss: 0.5848\tLR: 0.020000\n",
            "Training Epoch: 72 [19072/50000]\tLoss: 0.4566\tLR: 0.020000\n",
            "Training Epoch: 72 [19200/50000]\tLoss: 0.3975\tLR: 0.020000\n",
            "Training Epoch: 72 [19328/50000]\tLoss: 0.3553\tLR: 0.020000\n",
            "Training Epoch: 72 [19456/50000]\tLoss: 0.2547\tLR: 0.020000\n",
            "Training Epoch: 72 [19584/50000]\tLoss: 0.3303\tLR: 0.020000\n",
            "Training Epoch: 72 [19712/50000]\tLoss: 0.3399\tLR: 0.020000\n",
            "Training Epoch: 72 [19840/50000]\tLoss: 0.4169\tLR: 0.020000\n",
            "Training Epoch: 72 [19968/50000]\tLoss: 0.3615\tLR: 0.020000\n",
            "Training Epoch: 72 [20096/50000]\tLoss: 0.3948\tLR: 0.020000\n",
            "Training Epoch: 72 [20224/50000]\tLoss: 0.5236\tLR: 0.020000\n",
            "Training Epoch: 72 [20352/50000]\tLoss: 0.2375\tLR: 0.020000\n",
            "Training Epoch: 72 [20480/50000]\tLoss: 0.3464\tLR: 0.020000\n",
            "Training Epoch: 72 [20608/50000]\tLoss: 0.2311\tLR: 0.020000\n",
            "Training Epoch: 72 [20736/50000]\tLoss: 0.2736\tLR: 0.020000\n",
            "Training Epoch: 72 [20864/50000]\tLoss: 0.2846\tLR: 0.020000\n",
            "Training Epoch: 72 [20992/50000]\tLoss: 0.2906\tLR: 0.020000\n",
            "Training Epoch: 72 [21120/50000]\tLoss: 0.3980\tLR: 0.020000\n",
            "Training Epoch: 72 [21248/50000]\tLoss: 0.4932\tLR: 0.020000\n",
            "Training Epoch: 72 [21376/50000]\tLoss: 0.2680\tLR: 0.020000\n",
            "Training Epoch: 72 [21504/50000]\tLoss: 0.3197\tLR: 0.020000\n",
            "Training Epoch: 72 [21632/50000]\tLoss: 0.2925\tLR: 0.020000\n",
            "Training Epoch: 72 [21760/50000]\tLoss: 0.3519\tLR: 0.020000\n",
            "Training Epoch: 72 [21888/50000]\tLoss: 0.3679\tLR: 0.020000\n",
            "Training Epoch: 72 [22016/50000]\tLoss: 0.2645\tLR: 0.020000\n",
            "Training Epoch: 72 [22144/50000]\tLoss: 0.3939\tLR: 0.020000\n",
            "Training Epoch: 72 [22272/50000]\tLoss: 0.3505\tLR: 0.020000\n",
            "Training Epoch: 72 [22400/50000]\tLoss: 0.3378\tLR: 0.020000\n",
            "Training Epoch: 72 [22528/50000]\tLoss: 0.4264\tLR: 0.020000\n",
            "Training Epoch: 72 [22656/50000]\tLoss: 0.3556\tLR: 0.020000\n",
            "Training Epoch: 72 [22784/50000]\tLoss: 0.2506\tLR: 0.020000\n",
            "Training Epoch: 72 [22912/50000]\tLoss: 0.3171\tLR: 0.020000\n",
            "Training Epoch: 72 [23040/50000]\tLoss: 0.3876\tLR: 0.020000\n",
            "Training Epoch: 72 [23168/50000]\tLoss: 0.3023\tLR: 0.020000\n",
            "Training Epoch: 72 [23296/50000]\tLoss: 0.2854\tLR: 0.020000\n",
            "Training Epoch: 72 [23424/50000]\tLoss: 0.3692\tLR: 0.020000\n",
            "Training Epoch: 72 [23552/50000]\tLoss: 0.3989\tLR: 0.020000\n",
            "Training Epoch: 72 [23680/50000]\tLoss: 0.3372\tLR: 0.020000\n",
            "Training Epoch: 72 [23808/50000]\tLoss: 0.3721\tLR: 0.020000\n",
            "Training Epoch: 72 [23936/50000]\tLoss: 0.3679\tLR: 0.020000\n",
            "Training Epoch: 72 [24064/50000]\tLoss: 0.2782\tLR: 0.020000\n",
            "Training Epoch: 72 [24192/50000]\tLoss: 0.4491\tLR: 0.020000\n",
            "Training Epoch: 72 [24320/50000]\tLoss: 0.2756\tLR: 0.020000\n",
            "Training Epoch: 72 [24448/50000]\tLoss: 0.3954\tLR: 0.020000\n",
            "Training Epoch: 72 [24576/50000]\tLoss: 0.3325\tLR: 0.020000\n",
            "Training Epoch: 72 [24704/50000]\tLoss: 0.4030\tLR: 0.020000\n",
            "Training Epoch: 72 [24832/50000]\tLoss: 0.2824\tLR: 0.020000\n",
            "Training Epoch: 72 [24960/50000]\tLoss: 0.3217\tLR: 0.020000\n",
            "Training Epoch: 72 [25088/50000]\tLoss: 0.3902\tLR: 0.020000\n",
            "Training Epoch: 72 [25216/50000]\tLoss: 0.3019\tLR: 0.020000\n",
            "Training Epoch: 72 [25344/50000]\tLoss: 0.3613\tLR: 0.020000\n",
            "Training Epoch: 72 [25472/50000]\tLoss: 0.3624\tLR: 0.020000\n",
            "Training Epoch: 72 [25600/50000]\tLoss: 0.4193\tLR: 0.020000\n",
            "Training Epoch: 72 [25728/50000]\tLoss: 0.3927\tLR: 0.020000\n",
            "Training Epoch: 72 [25856/50000]\tLoss: 0.3014\tLR: 0.020000\n",
            "Training Epoch: 72 [25984/50000]\tLoss: 0.3623\tLR: 0.020000\n",
            "Training Epoch: 72 [26112/50000]\tLoss: 0.3638\tLR: 0.020000\n",
            "Training Epoch: 72 [26240/50000]\tLoss: 0.3816\tLR: 0.020000\n",
            "Training Epoch: 72 [26368/50000]\tLoss: 0.3970\tLR: 0.020000\n",
            "Training Epoch: 72 [26496/50000]\tLoss: 0.3495\tLR: 0.020000\n",
            "Training Epoch: 72 [26624/50000]\tLoss: 0.3223\tLR: 0.020000\n",
            "Training Epoch: 72 [26752/50000]\tLoss: 0.3877\tLR: 0.020000\n",
            "Training Epoch: 72 [26880/50000]\tLoss: 0.3595\tLR: 0.020000\n",
            "Training Epoch: 72 [27008/50000]\tLoss: 0.3300\tLR: 0.020000\n",
            "Training Epoch: 72 [27136/50000]\tLoss: 0.5481\tLR: 0.020000\n",
            "Training Epoch: 72 [27264/50000]\tLoss: 0.4056\tLR: 0.020000\n",
            "Training Epoch: 72 [27392/50000]\tLoss: 0.4445\tLR: 0.020000\n",
            "Training Epoch: 72 [27520/50000]\tLoss: 0.3810\tLR: 0.020000\n",
            "Training Epoch: 72 [27648/50000]\tLoss: 0.2424\tLR: 0.020000\n",
            "Training Epoch: 72 [27776/50000]\tLoss: 0.3581\tLR: 0.020000\n",
            "Training Epoch: 72 [27904/50000]\tLoss: 0.3638\tLR: 0.020000\n",
            "Training Epoch: 72 [28032/50000]\tLoss: 0.3156\tLR: 0.020000\n",
            "Training Epoch: 72 [28160/50000]\tLoss: 0.3237\tLR: 0.020000\n",
            "Training Epoch: 72 [28288/50000]\tLoss: 0.2706\tLR: 0.020000\n",
            "Training Epoch: 72 [28416/50000]\tLoss: 0.5537\tLR: 0.020000\n",
            "Training Epoch: 72 [28544/50000]\tLoss: 0.5534\tLR: 0.020000\n",
            "Training Epoch: 72 [28672/50000]\tLoss: 0.4626\tLR: 0.020000\n",
            "Training Epoch: 72 [28800/50000]\tLoss: 0.3861\tLR: 0.020000\n",
            "Training Epoch: 72 [28928/50000]\tLoss: 0.4257\tLR: 0.020000\n",
            "Training Epoch: 72 [29056/50000]\tLoss: 0.3364\tLR: 0.020000\n",
            "Training Epoch: 72 [29184/50000]\tLoss: 0.3677\tLR: 0.020000\n",
            "Training Epoch: 72 [29312/50000]\tLoss: 0.2769\tLR: 0.020000\n",
            "Training Epoch: 72 [29440/50000]\tLoss: 0.4002\tLR: 0.020000\n",
            "Training Epoch: 72 [29568/50000]\tLoss: 0.4069\tLR: 0.020000\n",
            "Training Epoch: 72 [29696/50000]\tLoss: 0.3448\tLR: 0.020000\n",
            "Training Epoch: 72 [29824/50000]\tLoss: 0.3434\tLR: 0.020000\n",
            "Training Epoch: 72 [29952/50000]\tLoss: 0.4434\tLR: 0.020000\n",
            "Training Epoch: 72 [30080/50000]\tLoss: 0.4235\tLR: 0.020000\n",
            "Training Epoch: 72 [30208/50000]\tLoss: 0.4374\tLR: 0.020000\n",
            "Training Epoch: 72 [30336/50000]\tLoss: 0.4190\tLR: 0.020000\n",
            "Training Epoch: 72 [30464/50000]\tLoss: 0.3551\tLR: 0.020000\n",
            "Training Epoch: 72 [30592/50000]\tLoss: 0.3707\tLR: 0.020000\n",
            "Training Epoch: 72 [30720/50000]\tLoss: 0.3733\tLR: 0.020000\n",
            "Training Epoch: 72 [30848/50000]\tLoss: 0.3566\tLR: 0.020000\n",
            "Training Epoch: 72 [30976/50000]\tLoss: 0.3434\tLR: 0.020000\n",
            "Training Epoch: 72 [31104/50000]\tLoss: 0.3858\tLR: 0.020000\n",
            "Training Epoch: 72 [31232/50000]\tLoss: 0.4332\tLR: 0.020000\n",
            "Training Epoch: 72 [31360/50000]\tLoss: 0.2845\tLR: 0.020000\n",
            "Training Epoch: 72 [31488/50000]\tLoss: 0.4220\tLR: 0.020000\n",
            "Training Epoch: 72 [31616/50000]\tLoss: 0.3714\tLR: 0.020000\n",
            "Training Epoch: 72 [31744/50000]\tLoss: 0.3632\tLR: 0.020000\n",
            "Training Epoch: 72 [31872/50000]\tLoss: 0.3565\tLR: 0.020000\n",
            "Training Epoch: 72 [32000/50000]\tLoss: 0.2648\tLR: 0.020000\n",
            "Training Epoch: 72 [32128/50000]\tLoss: 0.3203\tLR: 0.020000\n",
            "Training Epoch: 72 [32256/50000]\tLoss: 0.4411\tLR: 0.020000\n",
            "Training Epoch: 72 [32384/50000]\tLoss: 0.3022\tLR: 0.020000\n",
            "Training Epoch: 72 [32512/50000]\tLoss: 0.3133\tLR: 0.020000\n",
            "Training Epoch: 72 [32640/50000]\tLoss: 0.4644\tLR: 0.020000\n",
            "Training Epoch: 72 [32768/50000]\tLoss: 0.5797\tLR: 0.020000\n",
            "Training Epoch: 72 [32896/50000]\tLoss: 0.5674\tLR: 0.020000\n",
            "Training Epoch: 72 [33024/50000]\tLoss: 0.3050\tLR: 0.020000\n",
            "Training Epoch: 72 [33152/50000]\tLoss: 0.3100\tLR: 0.020000\n",
            "Training Epoch: 72 [33280/50000]\tLoss: 0.3602\tLR: 0.020000\n",
            "Training Epoch: 72 [33408/50000]\tLoss: 0.3995\tLR: 0.020000\n",
            "Training Epoch: 72 [33536/50000]\tLoss: 0.3827\tLR: 0.020000\n",
            "Training Epoch: 72 [33664/50000]\tLoss: 0.3926\tLR: 0.020000\n",
            "Training Epoch: 72 [33792/50000]\tLoss: 0.2256\tLR: 0.020000\n",
            "Training Epoch: 72 [33920/50000]\tLoss: 0.3766\tLR: 0.020000\n",
            "Training Epoch: 72 [34048/50000]\tLoss: 0.3785\tLR: 0.020000\n",
            "Training Epoch: 72 [34176/50000]\tLoss: 0.3331\tLR: 0.020000\n",
            "Training Epoch: 72 [34304/50000]\tLoss: 0.2190\tLR: 0.020000\n",
            "Training Epoch: 72 [34432/50000]\tLoss: 0.4001\tLR: 0.020000\n",
            "Training Epoch: 72 [34560/50000]\tLoss: 0.3358\tLR: 0.020000\n",
            "Training Epoch: 72 [34688/50000]\tLoss: 0.5140\tLR: 0.020000\n",
            "Training Epoch: 72 [34816/50000]\tLoss: 0.3268\tLR: 0.020000\n",
            "Training Epoch: 72 [34944/50000]\tLoss: 0.3789\tLR: 0.020000\n",
            "Training Epoch: 72 [35072/50000]\tLoss: 0.4886\tLR: 0.020000\n",
            "Training Epoch: 72 [35200/50000]\tLoss: 0.4778\tLR: 0.020000\n",
            "Training Epoch: 72 [35328/50000]\tLoss: 0.3255\tLR: 0.020000\n",
            "Training Epoch: 72 [35456/50000]\tLoss: 0.4346\tLR: 0.020000\n",
            "Training Epoch: 72 [35584/50000]\tLoss: 0.3046\tLR: 0.020000\n",
            "Training Epoch: 72 [35712/50000]\tLoss: 0.2587\tLR: 0.020000\n",
            "Training Epoch: 72 [35840/50000]\tLoss: 0.3573\tLR: 0.020000\n",
            "Training Epoch: 72 [35968/50000]\tLoss: 0.4156\tLR: 0.020000\n",
            "Training Epoch: 72 [36096/50000]\tLoss: 0.3188\tLR: 0.020000\n",
            "Training Epoch: 72 [36224/50000]\tLoss: 0.5293\tLR: 0.020000\n",
            "Training Epoch: 72 [36352/50000]\tLoss: 0.5556\tLR: 0.020000\n",
            "Training Epoch: 72 [36480/50000]\tLoss: 0.4797\tLR: 0.020000\n",
            "Training Epoch: 72 [36608/50000]\tLoss: 0.3976\tLR: 0.020000\n",
            "Training Epoch: 72 [36736/50000]\tLoss: 0.4092\tLR: 0.020000\n",
            "Training Epoch: 72 [36864/50000]\tLoss: 0.3462\tLR: 0.020000\n",
            "Training Epoch: 72 [36992/50000]\tLoss: 0.4664\tLR: 0.020000\n",
            "Training Epoch: 72 [37120/50000]\tLoss: 0.3348\tLR: 0.020000\n",
            "Training Epoch: 72 [37248/50000]\tLoss: 0.3885\tLR: 0.020000\n",
            "Training Epoch: 72 [37376/50000]\tLoss: 0.2825\tLR: 0.020000\n",
            "Training Epoch: 72 [37504/50000]\tLoss: 0.3972\tLR: 0.020000\n",
            "Training Epoch: 72 [37632/50000]\tLoss: 0.4142\tLR: 0.020000\n",
            "Training Epoch: 72 [37760/50000]\tLoss: 0.2867\tLR: 0.020000\n",
            "Training Epoch: 72 [37888/50000]\tLoss: 0.3592\tLR: 0.020000\n",
            "Training Epoch: 72 [38016/50000]\tLoss: 0.3803\tLR: 0.020000\n",
            "Training Epoch: 72 [38144/50000]\tLoss: 0.4776\tLR: 0.020000\n",
            "Training Epoch: 72 [38272/50000]\tLoss: 0.3749\tLR: 0.020000\n",
            "Training Epoch: 72 [38400/50000]\tLoss: 0.2924\tLR: 0.020000\n",
            "Training Epoch: 72 [38528/50000]\tLoss: 0.3894\tLR: 0.020000\n",
            "Training Epoch: 72 [38656/50000]\tLoss: 0.3372\tLR: 0.020000\n",
            "Training Epoch: 72 [38784/50000]\tLoss: 0.4627\tLR: 0.020000\n",
            "Training Epoch: 72 [38912/50000]\tLoss: 0.3748\tLR: 0.020000\n",
            "Training Epoch: 72 [39040/50000]\tLoss: 0.4060\tLR: 0.020000\n",
            "Training Epoch: 72 [39168/50000]\tLoss: 0.4880\tLR: 0.020000\n",
            "Training Epoch: 72 [39296/50000]\tLoss: 0.4009\tLR: 0.020000\n",
            "Training Epoch: 72 [39424/50000]\tLoss: 0.4571\tLR: 0.020000\n",
            "Training Epoch: 72 [39552/50000]\tLoss: 0.3643\tLR: 0.020000\n",
            "Training Epoch: 72 [39680/50000]\tLoss: 0.3184\tLR: 0.020000\n",
            "Training Epoch: 72 [39808/50000]\tLoss: 0.3094\tLR: 0.020000\n",
            "Training Epoch: 72 [39936/50000]\tLoss: 0.3662\tLR: 0.020000\n",
            "Training Epoch: 72 [40064/50000]\tLoss: 0.5660\tLR: 0.020000\n",
            "Training Epoch: 72 [40192/50000]\tLoss: 0.4396\tLR: 0.020000\n",
            "Training Epoch: 72 [40320/50000]\tLoss: 0.3396\tLR: 0.020000\n",
            "Training Epoch: 72 [40448/50000]\tLoss: 0.3726\tLR: 0.020000\n",
            "Training Epoch: 72 [40576/50000]\tLoss: 0.3199\tLR: 0.020000\n",
            "Training Epoch: 72 [40704/50000]\tLoss: 0.4241\tLR: 0.020000\n",
            "Training Epoch: 72 [40832/50000]\tLoss: 0.4292\tLR: 0.020000\n",
            "Training Epoch: 72 [40960/50000]\tLoss: 0.5546\tLR: 0.020000\n",
            "Training Epoch: 72 [41088/50000]\tLoss: 0.2853\tLR: 0.020000\n",
            "Training Epoch: 72 [41216/50000]\tLoss: 0.2768\tLR: 0.020000\n",
            "Training Epoch: 72 [41344/50000]\tLoss: 0.4058\tLR: 0.020000\n",
            "Training Epoch: 72 [41472/50000]\tLoss: 0.2951\tLR: 0.020000\n",
            "Training Epoch: 72 [41600/50000]\tLoss: 0.4623\tLR: 0.020000\n",
            "Training Epoch: 72 [41728/50000]\tLoss: 0.2986\tLR: 0.020000\n",
            "Training Epoch: 72 [41856/50000]\tLoss: 0.4167\tLR: 0.020000\n",
            "Training Epoch: 72 [41984/50000]\tLoss: 0.4202\tLR: 0.020000\n",
            "Training Epoch: 72 [42112/50000]\tLoss: 0.3853\tLR: 0.020000\n",
            "Training Epoch: 72 [42240/50000]\tLoss: 0.5307\tLR: 0.020000\n",
            "Training Epoch: 72 [42368/50000]\tLoss: 0.4473\tLR: 0.020000\n",
            "Training Epoch: 72 [42496/50000]\tLoss: 0.4164\tLR: 0.020000\n",
            "Training Epoch: 72 [42624/50000]\tLoss: 0.4199\tLR: 0.020000\n",
            "Training Epoch: 72 [42752/50000]\tLoss: 0.4458\tLR: 0.020000\n",
            "Training Epoch: 72 [42880/50000]\tLoss: 0.3845\tLR: 0.020000\n",
            "Training Epoch: 72 [43008/50000]\tLoss: 0.5354\tLR: 0.020000\n",
            "Training Epoch: 72 [43136/50000]\tLoss: 0.3265\tLR: 0.020000\n",
            "Training Epoch: 72 [43264/50000]\tLoss: 0.3609\tLR: 0.020000\n",
            "Training Epoch: 72 [43392/50000]\tLoss: 0.5362\tLR: 0.020000\n",
            "Training Epoch: 72 [43520/50000]\tLoss: 0.4522\tLR: 0.020000\n",
            "Training Epoch: 72 [43648/50000]\tLoss: 0.3835\tLR: 0.020000\n",
            "Training Epoch: 72 [43776/50000]\tLoss: 0.4114\tLR: 0.020000\n",
            "Training Epoch: 72 [43904/50000]\tLoss: 0.3135\tLR: 0.020000\n",
            "Training Epoch: 72 [44032/50000]\tLoss: 0.3083\tLR: 0.020000\n",
            "Training Epoch: 72 [44160/50000]\tLoss: 0.3347\tLR: 0.020000\n",
            "Training Epoch: 72 [44288/50000]\tLoss: 0.4601\tLR: 0.020000\n",
            "Training Epoch: 72 [44416/50000]\tLoss: 0.3794\tLR: 0.020000\n",
            "Training Epoch: 72 [44544/50000]\tLoss: 0.3354\tLR: 0.020000\n",
            "Training Epoch: 72 [44672/50000]\tLoss: 0.3523\tLR: 0.020000\n",
            "Training Epoch: 72 [44800/50000]\tLoss: 0.3038\tLR: 0.020000\n",
            "Training Epoch: 72 [44928/50000]\tLoss: 0.3538\tLR: 0.020000\n",
            "Training Epoch: 72 [45056/50000]\tLoss: 0.6502\tLR: 0.020000\n",
            "Training Epoch: 72 [45184/50000]\tLoss: 0.4289\tLR: 0.020000\n",
            "Training Epoch: 72 [45312/50000]\tLoss: 0.3416\tLR: 0.020000\n",
            "Training Epoch: 72 [45440/50000]\tLoss: 0.4961\tLR: 0.020000\n",
            "Training Epoch: 72 [45568/50000]\tLoss: 0.3552\tLR: 0.020000\n",
            "Training Epoch: 72 [45696/50000]\tLoss: 0.4874\tLR: 0.020000\n",
            "Training Epoch: 72 [45824/50000]\tLoss: 0.4378\tLR: 0.020000\n",
            "Training Epoch: 72 [45952/50000]\tLoss: 0.4731\tLR: 0.020000\n",
            "Training Epoch: 72 [46080/50000]\tLoss: 0.4783\tLR: 0.020000\n",
            "Training Epoch: 72 [46208/50000]\tLoss: 0.3821\tLR: 0.020000\n",
            "Training Epoch: 72 [46336/50000]\tLoss: 0.4299\tLR: 0.020000\n",
            "Training Epoch: 72 [46464/50000]\tLoss: 0.4663\tLR: 0.020000\n",
            "Training Epoch: 72 [46592/50000]\tLoss: 0.3919\tLR: 0.020000\n",
            "Training Epoch: 72 [46720/50000]\tLoss: 0.3049\tLR: 0.020000\n",
            "Training Epoch: 72 [46848/50000]\tLoss: 0.4140\tLR: 0.020000\n",
            "Training Epoch: 72 [46976/50000]\tLoss: 0.5952\tLR: 0.020000\n",
            "Training Epoch: 72 [47104/50000]\tLoss: 0.3533\tLR: 0.020000\n",
            "Training Epoch: 72 [47232/50000]\tLoss: 0.4539\tLR: 0.020000\n",
            "Training Epoch: 72 [47360/50000]\tLoss: 0.4955\tLR: 0.020000\n",
            "Training Epoch: 72 [47488/50000]\tLoss: 0.3546\tLR: 0.020000\n",
            "Training Epoch: 72 [47616/50000]\tLoss: 0.3918\tLR: 0.020000\n",
            "Training Epoch: 72 [47744/50000]\tLoss: 0.3141\tLR: 0.020000\n",
            "Training Epoch: 72 [47872/50000]\tLoss: 0.4506\tLR: 0.020000\n",
            "Training Epoch: 72 [48000/50000]\tLoss: 0.4587\tLR: 0.020000\n",
            "Training Epoch: 72 [48128/50000]\tLoss: 0.4691\tLR: 0.020000\n",
            "Training Epoch: 72 [48256/50000]\tLoss: 0.3712\tLR: 0.020000\n",
            "Training Epoch: 72 [48384/50000]\tLoss: 0.5401\tLR: 0.020000\n",
            "Training Epoch: 72 [48512/50000]\tLoss: 0.4435\tLR: 0.020000\n",
            "Training Epoch: 72 [48640/50000]\tLoss: 0.3860\tLR: 0.020000\n",
            "Training Epoch: 72 [48768/50000]\tLoss: 0.2922\tLR: 0.020000\n",
            "Training Epoch: 72 [48896/50000]\tLoss: 0.4064\tLR: 0.020000\n",
            "Training Epoch: 72 [49024/50000]\tLoss: 0.5456\tLR: 0.020000\n",
            "Training Epoch: 72 [49152/50000]\tLoss: 0.2826\tLR: 0.020000\n",
            "Training Epoch: 72 [49280/50000]\tLoss: 0.4022\tLR: 0.020000\n",
            "Training Epoch: 72 [49408/50000]\tLoss: 0.3835\tLR: 0.020000\n",
            "Training Epoch: 72 [49536/50000]\tLoss: 0.3906\tLR: 0.020000\n",
            "Training Epoch: 72 [49664/50000]\tLoss: 0.4337\tLR: 0.020000\n",
            "Training Epoch: 72 [49792/50000]\tLoss: 0.4187\tLR: 0.020000\n",
            "Training Epoch: 72 [49920/50000]\tLoss: 0.3889\tLR: 0.020000\n",
            "Training Epoch: 72 [50000/50000]\tLoss: 0.4245\tLR: 0.020000\n",
            "epoch 72 training time consumed: 144.70s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  133678 KB |    1044 MB |  263853 GB |  263853 GB |\n",
            "|       from large pool |  122880 KB |    1034 MB |  263640 GB |  263640 GB |\n",
            "|       from small pool |   10798 KB |      13 MB |     213 GB |     213 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  133678 KB |    1044 MB |  263853 GB |  263853 GB |\n",
            "|       from large pool |  122880 KB |    1034 MB |  263640 GB |  263640 GB |\n",
            "|       from small pool |   10798 KB |      13 MB |     213 GB |     213 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    1838 MB |    1838 MB |    1838 MB |       0 B  |\n",
            "|       from large pool |    1822 MB |    1822 MB |    1822 MB |       0 B  |\n",
            "|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  433618 KB |  778793 KB |  122682 GB |  122681 GB |\n",
            "|       from large pool |  432128 KB |  776192 KB |  122469 GB |  122468 GB |\n",
            "|       from small pool |    1490 KB |    3494 KB |     213 GB |     213 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     255    |     335    |   10674 K  |   10674 K  |\n",
            "|       from large pool |      24    |      65    |    5232 K  |    5232 K  |\n",
            "|       from small pool |     231    |     274    |    5442 K  |    5442 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     255    |     335    |   10674 K  |   10674 K  |\n",
            "|       from large pool |      24    |      65    |    5232 K  |    5232 K  |\n",
            "|       from small pool |     231    |     274    |    5442 K  |    5442 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      20    |      20    |      20    |       0    |\n",
            "|       from large pool |      12    |      12    |      12    |       0    |\n",
            "|       from small pool |       8    |       8    |       8    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      35    |      45    |    6179 K  |    6179 K  |\n",
            "|       from large pool |      10    |      15    |    2368 K  |    2368 K  |\n",
            "|       from small pool |      25    |      35    |    3811 K  |    3811 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 72, Average loss: 0.0096, Accuracy: 0.6942, Time consumed:9.01s\n",
            "\n",
            "Training Epoch: 73 [128/50000]\tLoss: 0.2620\tLR: 0.020000\n",
            "Training Epoch: 73 [256/50000]\tLoss: 0.2373\tLR: 0.020000\n",
            "Training Epoch: 73 [384/50000]\tLoss: 0.2781\tLR: 0.020000\n",
            "Training Epoch: 73 [512/50000]\tLoss: 0.2322\tLR: 0.020000\n",
            "Training Epoch: 73 [640/50000]\tLoss: 0.4811\tLR: 0.020000\n",
            "Training Epoch: 73 [768/50000]\tLoss: 0.4396\tLR: 0.020000\n",
            "Training Epoch: 73 [896/50000]\tLoss: 0.3054\tLR: 0.020000\n",
            "Training Epoch: 73 [1024/50000]\tLoss: 0.2982\tLR: 0.020000\n",
            "Training Epoch: 73 [1152/50000]\tLoss: 0.3832\tLR: 0.020000\n",
            "Training Epoch: 73 [1280/50000]\tLoss: 0.3778\tLR: 0.020000\n",
            "Training Epoch: 73 [1408/50000]\tLoss: 0.3268\tLR: 0.020000\n",
            "Training Epoch: 73 [1536/50000]\tLoss: 0.3463\tLR: 0.020000\n",
            "Training Epoch: 73 [1664/50000]\tLoss: 0.2283\tLR: 0.020000\n",
            "Training Epoch: 73 [1792/50000]\tLoss: 0.3898\tLR: 0.020000\n",
            "Training Epoch: 73 [1920/50000]\tLoss: 0.3474\tLR: 0.020000\n",
            "Training Epoch: 73 [2048/50000]\tLoss: 0.3459\tLR: 0.020000\n",
            "Training Epoch: 73 [2176/50000]\tLoss: 0.3018\tLR: 0.020000\n",
            "Training Epoch: 73 [2304/50000]\tLoss: 0.5284\tLR: 0.020000\n",
            "Training Epoch: 73 [2432/50000]\tLoss: 0.3433\tLR: 0.020000\n",
            "Training Epoch: 73 [2560/50000]\tLoss: 0.4025\tLR: 0.020000\n",
            "Training Epoch: 73 [2688/50000]\tLoss: 0.3587\tLR: 0.020000\n",
            "Training Epoch: 73 [2816/50000]\tLoss: 0.3210\tLR: 0.020000\n",
            "Training Epoch: 73 [2944/50000]\tLoss: 0.3266\tLR: 0.020000\n",
            "Training Epoch: 73 [3072/50000]\tLoss: 0.3259\tLR: 0.020000\n",
            "Training Epoch: 73 [3200/50000]\tLoss: 0.3234\tLR: 0.020000\n",
            "Training Epoch: 73 [3328/50000]\tLoss: 0.4070\tLR: 0.020000\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 212, in <module>\n",
            "    train(epoch)\n",
            "  File \"train.py\", line 50, in train\n",
            "    writer.add_scalar('LastLayerGradients/grad_norm2_weights', para.grad.norm(), n_iter)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/tensorboard/writer.py\", line 355, in add_scalar\n",
            "    tag, scalar_value, new_style=new_style, double_precision=double_precision\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/tensorboard/summary.py\", line 249, in scalar\n",
            "    scalar = make_np(scalar)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/tensorboard/_convert_np.py\", line 23, in make_np\n",
            "    return _prepare_pytorch(x)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/tensorboard/_convert_np.py\", line 31, in _prepare_pytorch\n",
            "    x = x.cpu().numpy()\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "s6ndQxUpSzDc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}